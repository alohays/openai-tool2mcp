{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"openai-tool2mcp","text":"<p>openai-tool2mcp is a lightweight, open-source bridge that wraps OpenAI's powerful built-in tools as Model Context Protocol (MCP) servers. It enables you to use high-quality OpenAI tools like web search and code interpreter with Claude and other MCP-compatible models.</p>"},{"location":"#why-openai-tool2mcp","title":"Why openai-tool2mcp?","text":"<p>AI developers today face a challenging choice:</p> <ol> <li>OpenAI's ecosystem: Offers powerful built-in tools like web search and code interpreter, but ties you to a closed platform</li> <li>MCP ecosystem: Provides an open standard for interoperability, but lacks the advanced tools available in OpenAI</li> </ol> <p>openai-tool2mcp bridges this gap by letting you use OpenAI's mature, high-quality tools within the open MCP ecosystem, giving you the best of both worlds.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd0d Use OpenAI's robust web search in Claude App</li> <li>\ud83d\udcbb Access code interpreter functionality in any MCP-compatible LLM</li> <li>\ud83d\udd04 Seamless protocol translation between OpenAI and MCP</li> <li>\ud83d\udee0\ufe0f Simple API for easy integration</li> </ul>"},{"location":"#supported-tools","title":"Supported Tools","text":"OpenAI Tool MCP Equivalent Status Web Search Web Search \u2705 Implemented Code Interpreter Code Execution \u2705 Implemented Web Browser Browser \u2705 Implemented File Management File I/O \u2705 Implemented"},{"location":"#technical-architecture","title":"Technical Architecture","text":"<p>openai-tool2mcp works by:</p> <ol> <li>Exposing an MCP-compatible server interface</li> <li>Translating MCP requests to OpenAI Assistant API calls</li> <li>Converting OpenAI tool responses back to MCP format</li> <li>Delivering results to MCP clients like Claude App</li> </ol> <p>For detailed technical information, see the Architecture Overview and Implementation Guide.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install from PyPI\npip install openai-tool2mcp\n\n# Start the server with your OpenAI API key\nOPENAI_API_KEY=\"your-api-key\" openai-tool2mcp start\n\n# Connect Claude App to http://localhost:8000\n</code></pre> <p>Ready to get started? Check out our Getting Started Guide for detailed instructions.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides comprehensive documentation for the openai-tool2mcp API, including classes, methods, and configuration options.</p>"},{"location":"api-reference/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/#mcpserver","title":"MCPServer","text":"<p>The main server class that implements the MCP protocol and manages tool execution.</p> <pre><code>class MCPServer:\n    def __init__(self, config=None):\n        \"\"\"\n        Initialize the MCP server.\n\n        Args:\n            config (ServerConfig, optional): Server configuration\n        \"\"\"\n        pass\n\n    def register_routes(self):\n        \"\"\"Register FastAPI routes for the MCP protocol\"\"\"\n        pass\n\n    def start(self, host=\"127.0.0.1\", port=8000):\n        \"\"\"\n        Start the MCP server.\n\n        Args:\n            host (str): Host address to bind to\n            port (int): Port to listen on\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#serverconfig","title":"ServerConfig","text":"<p>Configuration class for the MCP server.</p> <pre><code>class ServerConfig:\n    def __init__(\n        self,\n        openai_api_key=None,\n        tools=None,\n        request_timeout=30,\n        max_retries=3\n    ):\n        \"\"\"\n        Initialize server configuration.\n\n        Args:\n            openai_api_key (str, optional): OpenAI API key\n            tools (List[str], optional): List of enabled tools\n            request_timeout (int): Request timeout in seconds\n            max_retries (int): Maximum number of retries\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#openaiclient","title":"OpenAIClient","text":"<p>Client for interacting with the OpenAI API.</p> <pre><code>class OpenAIClient:\n    def __init__(self, api_key):\n        \"\"\"\n        Initialize the OpenAI client.\n\n        Args:\n            api_key (str): OpenAI API key\n        \"\"\"\n        pass\n\n    async def invoke_tool(self, request):\n        \"\"\"\n        Invoke an OpenAI tool.\n\n        Args:\n            request (OpenAIToolRequest): Tool request\n\n        Returns:\n            OpenAIToolResponse: Tool response\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#toolregistry","title":"ToolRegistry","text":"<p>Registry of available tools and their configurations.</p> <pre><code>class ToolRegistry:\n    def __init__(self, enabled_tools=None):\n        \"\"\"\n        Initialize the tool registry.\n\n        Args:\n            enabled_tools (List[str], optional): List of enabled tools\n        \"\"\"\n        pass\n\n    def has_tool(self, tool_id):\n        \"\"\"\n        Check if a tool is registered and enabled.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool is available\n        \"\"\"\n        pass\n\n    def get_openai_tool_type(self, tool_id):\n        \"\"\"\n        Get the OpenAI tool type for a given MCP tool ID.\n\n        Args:\n            tool_id (str): MCP tool ID\n\n        Returns:\n            str: OpenAI tool type\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#mcp-protocol-models","title":"MCP Protocol Models","text":""},{"location":"api-reference/#mcprequest","title":"MCPRequest","text":"<p>Model for MCP tool requests.</p> <pre><code>class MCPRequest(BaseModel):\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n    context: Optional[Dict[str, Any]] = Field(default=None)\n</code></pre> <p>Fields:</p> <ul> <li><code>parameters</code>: Dictionary of tool parameters</li> <li><code>context</code>: Dictionary of context information</li> </ul>"},{"location":"api-reference/#mcpresponse","title":"MCPResponse","text":"<p>Model for MCP tool responses.</p> <pre><code>class MCPResponse(BaseModel):\n    content: str\n    error: Optional[str] = None\n    context: Optional[Dict[str, Any]] = Field(default_factory=dict)\n</code></pre> <p>Fields:</p> <ul> <li><code>content</code>: Response content</li> <li><code>error</code>: Optional error message</li> <li><code>context</code>: Dictionary of context information</li> </ul>"},{"location":"api-reference/#openai-api-models","title":"OpenAI API Models","text":""},{"location":"api-reference/#openaitoolrequest","title":"OpenAIToolRequest","text":"<p>Model for OpenAI tool requests.</p> <pre><code>class OpenAIToolRequest(BaseModel):\n    tool_type: str\n    parameters: Dict[str, Any]\n    thread_id: Optional[str] = None\n    instructions: Optional[str] = None\n</code></pre> <p>Fields:</p> <ul> <li><code>tool_type</code>: OpenAI tool type</li> <li><code>parameters</code>: Dictionary of tool parameters</li> <li><code>thread_id</code>: Optional thread ID for continued conversations</li> <li><code>instructions</code>: Optional instructions for the assistant</li> </ul>"},{"location":"api-reference/#openaitoolresponse","title":"OpenAIToolResponse","text":"<p>Model for OpenAI tool responses.</p> <pre><code>class OpenAIToolResponse(BaseModel):\n    thread_id: str\n    tool_outputs: List[Any]\n</code></pre> <p>Fields:</p> <ul> <li><code>thread_id</code>: Thread ID for the conversation</li> <li><code>tool_outputs</code>: List of tool outputs</li> </ul>"},{"location":"api-reference/#built-in-tools","title":"Built-in Tools","text":""},{"location":"api-reference/#openaibuiltintools","title":"OpenAIBuiltInTools","text":"<p>Enum of built-in OpenAI tools.</p> <pre><code>class OpenAIBuiltInTools(Enum):\n    WEB_SEARCH = \"retrieval\"\n    CODE_INTERPRETER = \"code_interpreter\"\n    WEB_BROWSER = \"web_browser\"\n    FILE_SEARCH = \"file_search\"\n</code></pre>"},{"location":"api-reference/#tool-adapters","title":"Tool Adapters","text":""},{"location":"api-reference/#tooladapter","title":"ToolAdapter","text":"<p>Abstract base class for tool adapters.</p> <pre><code>class ToolAdapter(ABC):\n    @property\n    @abstractmethod\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"Translate MCP request to OpenAI parameters\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"Translate OpenAI response to MCP response\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#http-api-endpoints","title":"HTTP API Endpoints","text":"<p>The MCP server exposes the following HTTP endpoints:</p>"},{"location":"api-reference/#tool-invocation","title":"Tool Invocation","text":"<p>Endpoint: <code>POST /v1/tools/{tool_id}/invoke</code></p> <p>Invokes a tool with the specified ID.</p> <p>Path Parameters:</p> <ul> <li><code>tool_id</code> (string): ID of the tool to invoke</li> </ul> <p>Request Body:</p> <pre><code>{\n  \"parameters\": {\n    \"param1\": \"value1\",\n    \"param2\": \"value2\"\n  },\n  \"context\": {\n    \"thread_id\": \"optional-thread-id\",\n    \"instructions\": \"optional-instructions\"\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"content\": \"Tool response content\",\n  \"error\": null,\n  \"context\": {\n    \"thread_id\": \"thread-id\"\n  }\n}\n</code></pre> <p>Status Codes:</p> <ul> <li><code>200 OK</code>: Tool executed successfully</li> <li><code>404 Not Found</code>: Tool not found</li> <li><code>400 Bad Request</code>: Invalid request</li> <li><code>500 Internal Server Error</code>: Server error</li> </ul>"},{"location":"api-reference/#translation-functions","title":"Translation Functions","text":""},{"location":"api-reference/#mcp-to-openai-translation","title":"MCP to OpenAI Translation","text":"<pre><code>def translate_request(mcp_request: MCPRequest, tool_id: str) -&gt; OpenAIToolRequest:\n    \"\"\"\n    Translate an MCP request to an OpenAI request format.\n\n    Args:\n        mcp_request: The MCP request to translate\n        tool_id: The ID of the tool to invoke\n\n    Returns:\n        An OpenAI tool request object\n    \"\"\"\n    pass\n\ndef map_tool_id_to_openai_type(tool_id: str) -&gt; str:\n    \"\"\"\n    Map MCP tool IDs to OpenAI tool types.\n\n    Args:\n        tool_id: MCP tool ID\n\n    Returns:\n        OpenAI tool type\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#openai-to-mcp-translation","title":"OpenAI to MCP Translation","text":"<pre><code>def translate_response(openai_response: OpenAIToolResponse) -&gt; MCPResponse:\n    \"\"\"\n    Translate an OpenAI response to an MCP response format.\n\n    Args:\n        openai_response: The OpenAI response to translate\n\n    Returns:\n        An MCP response object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/#mcperror","title":"MCPError","text":"<p>Base class for all MCP errors.</p> <pre><code>class MCPError(Exception):\n    def __init__(self, message, status_code=500):\n        \"\"\"\n        Initialize MCP error.\n\n        Args:\n            message (str): Error message\n            status_code (int): HTTP status code\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#toolnotfounderror","title":"ToolNotFoundError","text":"<p>Error raised when a requested tool is not found.</p> <pre><code>class ToolNotFoundError(MCPError):\n    def __init__(self, tool_id):\n        \"\"\"\n        Initialize tool not found error.\n\n        Args:\n            tool_id (str): Tool ID\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#openaierror","title":"OpenAIError","text":"<p>Error raised when there's an issue with the OpenAI API.</p> <pre><code>class OpenAIError(MCPError):\n    def __init__(self, message, status_code=500):\n        \"\"\"\n        Initialize OpenAI error.\n\n        Args:\n            message (str): Error message\n            status_code (int): HTTP status code\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#configurationerror","title":"ConfigurationError","text":"<p>Error raised when there's an issue with configuration.</p> <pre><code>class ConfigurationError(MCPError):\n    def __init__(self, message):\n        \"\"\"\n        Initialize configuration error.\n\n        Args:\n            message (str): Error message\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#command-line-interface","title":"Command-Line Interface","text":""},{"location":"api-reference/#main-function","title":"Main Function","text":"<pre><code>def main():\n    \"\"\"Main function for CLI\"\"\"\n    pass\n</code></pre> <p>Command-Line Arguments:</p> <ul> <li><code>--host</code>: Host address to bind to (default: 127.0.0.1)</li> <li><code>--port</code>: Port to listen on (default: 8000)</li> <li><code>--api-key</code>: OpenAI API key (defaults to OPENAI_API_KEY env var)</li> <li><code>--tools</code>: List of enabled tools (defaults to all)</li> <li><code>--timeout</code>: Request timeout in seconds (default: 30)</li> <li><code>--retries</code>: Maximum number of retries for failed requests (default: 3)</li> <li><code>--log-level</code>: Logging level (default: info)</li> </ul>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#configuration-management","title":"Configuration Management","text":"<pre><code>def load_config(config_file=None):\n    \"\"\"\n    Load configuration from file.\n\n    Args:\n        config_file (str, optional): Path to configuration file\n\n    Returns:\n        dict: Configuration dictionary\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#logging-utilities","title":"Logging Utilities","text":"<pre><code>def setup_logging(level=\"info\"):\n    \"\"\"\n    Set up logging.\n\n    Args:\n        level (str): Logging level\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#security-utilities","title":"Security Utilities","text":"<pre><code>def validate_api_key(api_key):\n    \"\"\"\n    Validate OpenAI API key.\n\n    Args:\n        api_key (str): API key to validate\n\n    Returns:\n        bool: True if valid\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#examples","title":"Examples","text":""},{"location":"api-reference/#basic-server-example","title":"Basic Server Example","text":"<pre><code>from openai_tool2mcp import MCPServer, ServerConfig\n\n# Create server with default configuration\nserver = MCPServer()\nserver.start()\n</code></pre>"},{"location":"api-reference/#custom-configuration-example","title":"Custom Configuration Example","text":"<pre><code>from openai_tool2mcp import MCPServer, ServerConfig\nfrom openai_tool2mcp.tools import OpenAIBuiltInTools\n\n# Create server with custom configuration\nconfig = ServerConfig(\n    openai_api_key=\"your-api-key\",\n    tools=[\n        OpenAIBuiltInTools.WEB_SEARCH.value,\n        OpenAIBuiltInTools.CODE_INTERPRETER.value\n    ],\n    request_timeout=60,\n    max_retries=5\n)\n\nserver = MCPServer(config)\nserver.start(host=\"127.0.0.1\", port=8888)\n</code></pre>"},{"location":"api-reference/#custom-tool-adapter-example","title":"Custom Tool Adapter Example","text":"<pre><code>from openai_tool2mcp.models.mcp import MCPRequest, MCPResponse\nfrom openai_tool2mcp.tools import ToolAdapter\n\nclass CustomToolAdapter(ToolAdapter):\n    @property\n    def tool_id(self) -&gt; str:\n        return \"custom-tool\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        return \"retrieval\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        # Custom request translation logic\n        return {\"query\": request.parameters.get(\"query\", \"\")}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        # Custom response translation logic\n        return MCPResponse(\n            content=\"Custom response\",\n            context={\"custom_context\": \"value\"}\n        )\n</code></pre> <p>This API reference provides a comprehensive overview of the openai-tool2mcp library's classes, methods, and functionalities. For more detailed examples and guides, refer to the Implementation Guide and Getting Started documentation.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>This document provides a technical overview of how openai-tool2mcp bridges the OpenAI Assistant API with Model Context Protocol (MCP) servers.</p>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<p>openai-tool2mcp is designed as a protocol translation layer that sits between MCP-compatible clients and the OpenAI API:</p> <pre><code>sequenceDiagram\n    participant Client as \"MCP Client&lt;br&gt;(e.g., Claude App)\"\n    participant Server as \"openai-tool2mcp Server\"\n    participant OAIAPI as \"OpenAI Assistant API\"\n\n    Client-&gt;&gt;Server: MCP Tool Request\n    note over Server: Protocol Translation\n    Server-&gt;&gt;OAIAPI: OpenAI API Request\n    OAIAPI-&gt;&gt;Server: OpenAI Tool Response\n    note over Server: Response Translation\n    Server-&gt;&gt;Client: MCP Tool Response\n</code></pre>"},{"location":"architecture/#protocol-translation","title":"Protocol Translation","text":""},{"location":"architecture/#mcp-to-openai-translation","title":"MCP to OpenAI Translation","text":"<p>MCP requests follow the Model Context Protocol specification, which defines a standard for tool usage. openai-tool2mcp maps these requests to OpenAI's Assistant API format:</p> MCP Component OpenAI Equivalent Tool ID Tool type Tool parameters Tool parameters Tool context Thread context Instructions System prompt"},{"location":"architecture/#openai-to-mcp-translation","title":"OpenAI to MCP Translation","text":"<p>Responses from OpenAI tools are translated back to MCP format:</p> OpenAI Component MCP Equivalent Tool output Tool response content Error information Error messages Metadata Tool context updates"},{"location":"architecture/#architectural-components","title":"Architectural Components","text":"<p>The system consists of the following main components:</p>"},{"location":"architecture/#1-mcp-server-interface","title":"1. MCP Server Interface","text":"<ul> <li>Implements the MCP protocol specification</li> <li>Provides HTTP endpoints for tool invocation</li> <li>Manages MCP contexts and sessions</li> </ul>"},{"location":"architecture/#2-protocol-translator","title":"2. Protocol Translator","text":"<ul> <li>Converts between MCP and OpenAI formats</li> <li>Maps tool identifiers and parameters</li> <li>Handles different serialization formats</li> </ul>"},{"location":"architecture/#3-openai-client","title":"3. OpenAI Client","text":"<ul> <li>Manages connections to OpenAI API</li> <li>Handles authentication and API key management</li> <li>Implements rate limiting and error handling</li> </ul>"},{"location":"architecture/#4-tool-registry","title":"4. Tool Registry","text":"<ul> <li>Maintains mappings between OpenAI tools and MCP tools</li> <li>Provides configuration for tool-specific parameters</li> <li>Handles tool capability discovery</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<ol> <li> <p>Request Phase:</p> </li> <li> <p>MCP client sends a tool request to the server</p> </li> <li>Server validates the MCP request format</li> <li>Protocol translator converts to OpenAI format</li> <li> <p>Request is sent to OpenAI API</p> </li> <li> <p>Processing Phase:</p> </li> <li> <p>OpenAI processes the tool request</p> </li> <li>Tool executes the requested operation</li> <li> <p>Results are returned to OpenAI</p> </li> <li> <p>Response Phase:</p> </li> <li>OpenAI API returns tool output</li> <li>Protocol translator converts to MCP format</li> <li>MCP-formatted response is sent to client</li> </ol>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":"<ul> <li>API Key Management: OpenAI API keys are securely stored and never exposed to clients</li> <li>Request Validation: All incoming requests are validated before processing</li> <li>Rate Limiting: Implements rate limiting to prevent API abuse</li> <li>Error Handling: Robust error handling prevents information leakage</li> </ul>"},{"location":"architecture/#tool-specific-implementations","title":"Tool-Specific Implementations","text":""},{"location":"architecture/#web-search-tool","title":"Web Search Tool","text":"<p>The Web Search tool maps MCP search requests to OpenAI's built-in web search capability:</p> <pre><code>flowchart TD\n    A[MCP Search Request] --&gt; B{Protocol Translator}\n    B --&gt; C[OpenAI Web Search]\n    C --&gt; D[Search Results]\n    D --&gt; B\n    B --&gt; E[MCP Search Response]\n</code></pre>"},{"location":"architecture/#code-interpreter-tool","title":"Code Interpreter Tool","text":"<p>The Code Interpreter tool maps MCP code execution requests to OpenAI's code interpreter:</p> <pre><code>flowchart TD\n    A[MCP Code Execution Request] --&gt; B{Protocol Translator}\n    B --&gt; C[OpenAI Code Interpreter]\n    C --&gt; D[Execution Results]\n    D --&gt; B\n    B --&gt; E[MCP Code Execution Response]\n</code></pre>"},{"location":"architecture/#state-management","title":"State Management","text":"<p>MCP and OpenAI have different approaches to maintaining state:</p> <ul> <li>MCP: Uses explicit contexts that are passed with each request</li> <li>OpenAI: Uses threads and assistant sessions</li> </ul> <p>openai-tool2mcp handles this difference by:</p> <ol> <li>Creating OpenAI threads for each MCP session</li> <li>Persisting thread IDs and mapping them to MCP contexts</li> <li>Maintaining a session store for consistent tool state</li> </ol>"},{"location":"architecture/#extension-points","title":"Extension Points","text":"<p>The architecture is designed for extensibility:</p> <ul> <li>Tool Adapters: New tool adapters can be added to support additional OpenAI tools</li> <li>Custom Mapping Rules: Rules for mapping between protocols can be customized</li> <li>Middleware Support: Processing pipelines can be extended with custom middleware</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you quickly set up and start using openai-tool2mcp to bring OpenAI's powerful built-in tools to your MCP-compatible models.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>Python 3.10 or higher</li> <li>An OpenAI API key with access to the Assistant API</li> <li>A MCP-compatible client (like Claude App)</li> <li>(Recommended) <code>uv</code> package manager for MCP compatibility</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#using-uv-recommended-for-mcp-compatibility","title":"Using uv (Recommended for MCP compatibility)","text":"<p>The Model Context Protocol (MCP) recommends using <code>uv</code> for package management and execution:</p> <pre><code># Install uv if you don't have it\npip install uv\n\n# Install openai-tool2mcp using uv\nuv pip install openai-tool2mcp\n</code></pre> <p>For more details on using <code>uv</code> with openai-tool2mcp, see our dedicated guide.</p>"},{"location":"getting-started/#from-pypi","title":"From PyPI","text":"<pre><code>pip install openai-tool2mcp\n</code></pre>"},{"location":"getting-started/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/alohays/openai-tool2mcp.git\ncd openai-tool2mcp\npip install -e .\n</code></pre>"},{"location":"getting-started/#basic-setup","title":"Basic Setup","text":""},{"location":"getting-started/#1-set-your-openai-api-key","title":"1. Set Your OpenAI API Key","text":"<p>You can set your API key in one of two ways:</p> <p>Option 1: Environment Variable</p> <pre><code># Linux/macOS\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# Windows (Command Prompt)\nset OPENAI_API_KEY=your-api-key-here\n\n# Windows (PowerShell)\n$env:OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Option 2: Configuration File</p> <p>Create a file named <code>.env</code> in your project directory:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"getting-started/#2-start-the-mcp-server","title":"2. Start the MCP Server","text":""},{"location":"getting-started/#using-uv-recommended-for-mcp-compatibility_1","title":"Using uv (Recommended for MCP compatibility)","text":"<pre><code># Start the server using the standalone entry script\nuv run openai_tool2mcp/server_entry.py --transport stdio\n</code></pre>"},{"location":"getting-started/#using-the-cli","title":"Using the CLI","text":"<pre><code># Alternative: Using the CLI\nopenai-tool2mcp start --transport stdio\n</code></pre>"},{"location":"getting-started/#3-connect-your-mcp-client","title":"3. Connect Your MCP Client","text":"<p>Configure your MCP-compatible client to connect to your local server:</p> <ul> <li>Server URL: <code>http://localhost:8000</code></li> </ul>"},{"location":"getting-started/#using-with-claude-app","title":"Using with Claude App","text":"<p>Claude App supports the Model Context Protocol, making it a perfect client for openai-tool2mcp.</p>"},{"location":"getting-started/#setting-up-claude-app","title":"Setting Up Claude App","text":"<ol> <li>Open Claude App</li> <li>Go to Settings &gt; API &amp; Integrations</li> <li>Add a new MCP server with the URL <code>http://localhost:8000</code></li> <li>Save your settings</li> </ol>"},{"location":"getting-started/#available-tools-in-claude","title":"Available Tools in Claude","text":"<p>Once configured, you'll see new tools available in Claude:</p> <ul> <li>Web Search: Access OpenAI's powerful web search capability</li> <li>Code Execution: Run code using OpenAI's code interpreter</li> <li>Web Browser: Browse the web using OpenAI's web browser</li> <li>File Management: Manage files using OpenAI's file tools</li> </ul>"},{"location":"getting-started/#example-usage-in-claude","title":"Example Usage in Claude","text":"<p>Here's how to use the tools in Claude:</p> <pre><code>Claude, can you search the web for the latest news about AI regulations?\n</code></pre> <p>Claude will use the OpenAI web search tool through your local MCP server to fetch the latest news.</p>"},{"location":"getting-started/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use openai-tool2mcp programmatically in your Python applications:</p> <pre><code>from openai_tool2mcp import MCPServer, ServerConfig\nfrom openai_tool2mcp.tools import OpenAIBuiltInTools\n\n# Configure the server\nconfig = ServerConfig(\n    openai_api_key=\"your-api-key-here\",  # Optional if set in environment\n    tools=[\n        OpenAIBuiltInTools.WEB_SEARCH.value,\n        OpenAIBuiltInTools.CODE_INTERPRETER.value\n    ]\n)\n\n# Create and start the server\nserver = MCPServer(config)\nserver.start(host=\"127.0.0.1\", port=8000)\n</code></pre>"},{"location":"getting-started/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/#server-configuration-options","title":"Server Configuration Options","text":"<p>You can customize your server with these options:</p> <pre><code>config = ServerConfig(\n    openai_api_key=\"your-api-key-here\",\n    tools=[\"retrieval\", \"code_interpreter\"],  # Enable specific tools\n    request_timeout=60,                       # Timeout in seconds\n    max_retries=5                            # Max retries for failed requests\n)\n</code></pre>"},{"location":"getting-started/#command-line-options","title":"Command-Line Options","text":"<p>The CLI provides several configuration options:</p> <pre><code>openai-tool2mcp start --help\n</code></pre> <p>Available options:</p> <ul> <li><code>--host</code>: Host address to bind to (default: 127.0.0.1)</li> <li><code>--port</code>: Port to listen on (default: 8000)</li> <li><code>--api-key</code>: OpenAI API key (alternative to environment variable)</li> <li><code>--tools</code>: Space-separated list of tools to enable</li> <li><code>--timeout</code>: Request timeout in seconds</li> <li><code>--retries</code>: Maximum number of retries for failed requests</li> </ul>"},{"location":"getting-started/#docker-deployment","title":"Docker Deployment","text":"<p>You can also run openai-tool2mcp in Docker:</p> <pre><code># Build the Docker image\ndocker build -t openai-tool2mcp .\n\n# Run the container\ndocker run -p 8000:8000 -e OPENAI_API_KEY=\"your-api-key-here\" openai-tool2mcp\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/#1-server-wont-start","title":"1. Server Won't Start","text":"<p>Make sure:</p> <ul> <li>Your OpenAI API key is valid</li> <li>You have proper network permissions</li> <li>The port (default 8000) is not already in use</li> </ul>"},{"location":"getting-started/#2-tool-calls-fail","title":"2. Tool Calls Fail","text":"<p>Check:</p> <ul> <li>Your OpenAI account has access to the Assistant API</li> <li>Your API key has the necessary permissions</li> <li>You have sufficient API credits/quota</li> </ul>"},{"location":"getting-started/#3-connection-issues","title":"3. Connection Issues","text":"<p>Verify:</p> <ul> <li>The server is running (you should see log messages)</li> <li>Your client is correctly configured with the server URL</li> <li>Your network allows the connection</li> </ul>"},{"location":"getting-started/#logs","title":"Logs","text":"<p>For more detailed troubleshooting, enable debug logs:</p> <pre><code>openai-tool2mcp start --log-level debug\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have openai-tool2mcp up and running, consider:</p> <ul> <li>Reading the Architecture Overview to understand how it works</li> <li>Exploring the Implementation Guide for technical details</li> <li>Checking the API Reference for complete documentation</li> </ul> <p>For any issues or contributions, visit our GitHub repository.</p>"},{"location":"implementation/","title":"Implementation Guide","text":"<p>This guide provides detailed technical information on implementing the openai-tool2mcp bridge, focusing on code structure, key components, and protocol details.</p>"},{"location":"implementation/#project-structure","title":"Project Structure","text":"<p>The project follows a modular architecture with the following structure:</p> <pre><code>openai_tool2mcp/\n\u251c\u2500\u2500 __init__.py             # Package exports\n\u251c\u2500\u2500 server.py               # MCP server implementation\n\u251c\u2500\u2500 translator/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 mcp_to_openai.py    # MCP to OpenAI translation\n\u2502   \u2514\u2500\u2500 openai_to_mcp.py    # OpenAI to MCP translation\n\u251c\u2500\u2500 openai_client/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 client.py           # OpenAI API client\n\u2502   \u2514\u2500\u2500 assistants.py       # Assistants API wrapper\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 registry.py         # Tool registry\n\u2502   \u251c\u2500\u2500 web_search.py       # Web search implementation\n\u2502   \u251c\u2500\u2500 code_interpreter.py # Code interpreter implementation\n\u2502   \u251c\u2500\u2500 browser.py          # Web browser implementation\n\u2502   \u2514\u2500\u2500 file_manager.py     # File management implementation\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 mcp.py              # MCP protocol models\n\u2502   \u2514\u2500\u2500 openai.py           # OpenAI API models\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py           # Configuration management\n    \u251c\u2500\u2500 logging.py          # Logging utilities\n    \u2514\u2500\u2500 security.py         # Security utilities\n</code></pre>"},{"location":"implementation/#core-components","title":"Core Components","text":""},{"location":"implementation/#1-mcp-server-implementation","title":"1. MCP Server Implementation","text":"<p>The MCP server is implemented using FastAPI for high performance and compatibility with async operations:</p> <pre><code>from fastapi import FastAPI, HTTPException, Depends\nfrom .models.mcp import MCPRequest, MCPResponse\nfrom .translator import mcp_to_openai, openai_to_mcp\nfrom .openai_client import OpenAIClient\nfrom .tools import ToolRegistry\n\nclass MCPServer:\n    def __init__(self, config=None):\n        self.app = FastAPI()\n        self.config = config or ServerConfig()\n        self.openai_client = OpenAIClient(self.config.openai_api_key)\n        self.tool_registry = ToolRegistry(self.config.tools)\n\n        # Register routes\n        self.register_routes()\n\n    def register_routes(self):\n        @self.app.post(\"/v1/tools/{tool_id}/invoke\")\n        async def invoke_tool(tool_id: str, request: MCPRequest):\n            # Validate tool exists\n            if not self.tool_registry.has_tool(tool_id):\n                raise HTTPException(status_code=404, detail=f\"Tool {tool_id} not found\")\n\n            # Translate MCP request to OpenAI format\n            openai_request = mcp_to_openai.translate_request(request, tool_id)\n\n            # Call OpenAI API\n            openai_response = await self.openai_client.invoke_tool(openai_request)\n\n            # Translate OpenAI response to MCP format\n            mcp_response = openai_to_mcp.translate_response(openai_response)\n\n            return mcp_response\n\n    def start(self, host=\"127.0.0.1\", port=8000):\n        import uvicorn\n        uvicorn.run(self.app, host=host, port=port)\n</code></pre>"},{"location":"implementation/#2-protocol-translation","title":"2. Protocol Translation","text":"<p>The translation layer handles the mapping between MCP and OpenAI formats:</p>"},{"location":"implementation/#mcp-to-openai-translation","title":"MCP to OpenAI Translation","text":"<pre><code>from ..models.mcp import MCPRequest\nfrom ..models.openai import OpenAIToolRequest\n\ndef translate_request(mcp_request: MCPRequest, tool_id: str) -&gt; OpenAIToolRequest:\n    \"\"\"\n    Translate an MCP request to an OpenAI request format.\n\n    Args:\n        mcp_request: The MCP request to translate\n        tool_id: The ID of the tool to invoke\n\n    Returns:\n        An OpenAI tool request object\n    \"\"\"\n    # Extract tool parameters\n    parameters = mcp_request.parameters\n\n    # Extract context information\n    context = mcp_request.context or {}\n\n    # Determine if this is a new or existing conversation\n    thread_id = context.get(\"thread_id\")\n\n    # Create OpenAI request\n    openai_request = OpenAIToolRequest(\n        tool_type=map_tool_id_to_openai_type(tool_id),\n        parameters=parameters,\n        thread_id=thread_id,\n        instructions=context.get(\"instructions\", \"\")\n    )\n\n    return openai_request\n\ndef map_tool_id_to_openai_type(tool_id: str) -&gt; str:\n    \"\"\"Map MCP tool IDs to OpenAI tool types\"\"\"\n    mapping = {\n        \"web-search\": \"retrieval\",\n        \"code-execution\": \"code_interpreter\",\n        \"browser\": \"web_browser\",\n        \"file-io\": \"file_search\"\n    }\n    return mapping.get(tool_id, tool_id)\n</code></pre>"},{"location":"implementation/#openai-to-mcp-translation","title":"OpenAI to MCP Translation","text":"<pre><code>from ..models.mcp import MCPResponse\nfrom ..models.openai import OpenAIToolResponse\n\ndef translate_response(openai_response: OpenAIToolResponse) -&gt; MCPResponse:\n    \"\"\"\n    Translate an OpenAI response to an MCP response format.\n\n    Args:\n        openai_response: The OpenAI response to translate\n\n    Returns:\n        An MCP response object\n    \"\"\"\n    # Extract tool output\n    tool_output = openai_response.tool_outputs[0] if openai_response.tool_outputs else None\n\n    if not tool_output:\n        return MCPResponse(\n            content=\"No result\",\n            error=\"Tool returned no output\",\n            context={\"thread_id\": openai_response.thread_id}\n        )\n\n    # Create MCP response\n    mcp_response = MCPResponse(\n        content=tool_output.output,\n        error=tool_output.error if hasattr(tool_output, \"error\") else None,\n        context={\"thread_id\": openai_response.thread_id}\n    )\n\n    return mcp_response\n</code></pre>"},{"location":"implementation/#3-openai-client","title":"3. OpenAI Client","text":"<p>The OpenAI client manages interactions with the OpenAI API:</p> <pre><code>import openai\nfrom ..models.openai import OpenAIToolRequest, OpenAIToolResponse\n\nclass OpenAIClient:\n    def __init__(self, api_key):\n        self.client = openai.Client(api_key=api_key)\n\n    async def invoke_tool(self, request: OpenAIToolRequest) -&gt; OpenAIToolResponse:\n        \"\"\"\n        Invoke an OpenAI tool.\n\n        Args:\n            request: The tool request\n\n        Returns:\n            The tool response\n        \"\"\"\n        # Create or get thread\n        thread_id = request.thread_id\n        if not thread_id:\n            thread = await self.client.beta.threads.create()\n            thread_id = thread.id\n\n        # Create message with tool call\n        message = await self.client.beta.threads.messages.create(\n            thread_id=thread_id,\n            role=\"user\",\n            content=f\"Please use the {request.tool_type} tool with these parameters: {request.parameters}\",\n        )\n\n        # Create assistant with the appropriate tool\n        assistant = await self.client.beta.assistants.create(\n            name=\"Tool Executor\",\n            instructions=request.instructions or \"Execute the requested tool function.\",\n            tools=[{\"type\": request.tool_type}],\n            model=\"gpt-4o-mini-search-preview\"\n        )\n\n        # Run the assistant\n        run = await self.client.beta.threads.runs.create(\n            thread_id=thread_id,\n            assistant_id=assistant.id\n        )\n\n        # Wait for completion\n        run = await self._wait_for_run(thread_id, run.id)\n\n        # Get tool outputs\n        tool_outputs = run.required_action.submit_tool_outputs.tool_calls if hasattr(run, \"required_action\") else []\n\n        # Create response\n        response = OpenAIToolResponse(\n            thread_id=thread_id,\n            tool_outputs=tool_outputs\n        )\n\n        return response\n\n    async def _wait_for_run(self, thread_id, run_id):\n        \"\"\"Wait for a run to complete\"\"\"\n        import time\n\n        while True:\n            run = await self.client.beta.threads.runs.retrieve(\n                thread_id=thread_id,\n                run_id=run_id\n            )\n\n            if run.status in [\"completed\", \"failed\", \"requires_action\"]:\n                return run\n\n            time.sleep(1)\n</code></pre>"},{"location":"implementation/#4-tool-registry","title":"4. Tool Registry","text":"<p>The tool registry manages available tools and their configurations:</p> <pre><code>from enum import Enum, auto\nfrom typing import List, Optional\n\nclass OpenAIBuiltInTools(Enum):\n    WEB_SEARCH = \"retrieval\"\n    CODE_INTERPRETER = \"code_interpreter\"\n    WEB_BROWSER = \"web_browser\"\n    FILE_SEARCH = \"file_search\"\n\nclass ToolRegistry:\n    def __init__(self, enabled_tools=None):\n        self.tools = {}\n        self.enabled_tools = enabled_tools or [t.value for t in OpenAIBuiltInTools]\n        self._register_default_tools()\n\n    def _register_default_tools(self):\n        \"\"\"Register the default tool mappings\"\"\"\n        self.tools = {\n            \"web-search\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_SEARCH.value in self.enabled_tools\n            },\n            \"code-execution\": {\n                \"openai_tool\": OpenAIBuiltInTools.CODE_INTERPRETER.value,\n                \"enabled\": OpenAIBuiltInTools.CODE_INTERPRETER.value in self.enabled_tools\n            },\n            \"browser\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_BROWSER.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_BROWSER.value in self.enabled_tools\n            },\n            \"file-io\": {\n                \"openai_tool\": OpenAIBuiltInTools.FILE_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.FILE_SEARCH.value in self.enabled_tools\n            }\n        }\n\n    def has_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"Check if a tool is registered and enabled\"\"\"\n        return tool_id in self.tools and self.tools[tool_id][\"enabled\"]\n\n    def get_openai_tool_type(self, tool_id: str) -&gt; Optional[str]:\n        \"\"\"Get the OpenAI tool type for a given MCP tool ID\"\"\"\n        if self.has_tool(tool_id):\n            return self.tools[tool_id][\"openai_tool\"]\n        return None\n</code></pre>"},{"location":"implementation/#configuration","title":"Configuration","text":"<p>Configuration is managed through a dedicated ServerConfig class:</p> <pre><code>from typing import List, Optional\nfrom .tools import OpenAIBuiltInTools\n\nclass ServerConfig:\n    def __init__(\n        self,\n        openai_api_key: Optional[str] = None,\n        tools: Optional[List[str]] = None,\n        request_timeout: int = 30,\n        max_retries: int = 3\n    ):\n        \"\"\"\n        Initialize server configuration.\n\n        Args:\n            openai_api_key: OpenAI API key (defaults to environment variable)\n            tools: List of enabled tools (defaults to all)\n            request_timeout: Timeout for API requests in seconds\n            max_retries: Maximum number of retries for failed requests\n        \"\"\"\n        import os\n\n        self.openai_api_key = openai_api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self.openai_api_key:\n            raise ValueError(\"OpenAI API key is required\")\n\n        self.tools = tools or [t.value for t in OpenAIBuiltInTools]\n        self.request_timeout = request_timeout\n        self.max_retries = max_retries\n</code></pre>"},{"location":"implementation/#mcp-protocol-implementation","title":"MCP Protocol Implementation","text":"<p>The implementation follows the MCP protocol specification:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Dict, Any, Optional\n\nclass MCPRequest(BaseModel):\n    \"\"\"Model for MCP tool request\"\"\"\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n    context: Optional[Dict[str, Any]] = Field(default=None)\n\nclass MCPResponse(BaseModel):\n    \"\"\"Model for MCP tool response\"\"\"\n    content: str\n    error: Optional[str] = None\n    context: Optional[Dict[str, Any]] = Field(default_factory=dict)\n</code></pre>"},{"location":"implementation/#running-the-server","title":"Running the Server","text":"<p>To run the server with default configuration:</p> <pre><code>from openai_tool2mcp import MCPServer, ServerConfig, OpenAIBuiltInTools\n\n# Create server with all tools enabled\nconfig = ServerConfig(\n    tools=[t.value for t in OpenAIBuiltInTools]\n)\n\n# Start server\nserver = MCPServer(config)\nserver.start(host=\"127.0.0.1\", port=8000)\n</code></pre>"},{"location":"implementation/#cli-interface","title":"CLI Interface","text":"<p>The package includes a CLI interface for easy server startup:</p> <pre><code>import argparse\nimport os\nfrom .server import MCPServer, ServerConfig\nfrom .tools import OpenAIBuiltInTools\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Start an MCP server for OpenAI tools\")\n    parser.add_argument(\"--host\", default=\"127.0.0.1\", help=\"Host to listen on\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to listen on\")\n    parser.add_argument(\"--api-key\", help=\"OpenAI API key (defaults to OPENAI_API_KEY env var)\")\n    parser.add_argument(\"--tools\", nargs=\"+\", choices=[t.value for t in OpenAIBuiltInTools],\n                        default=[t.value for t in OpenAIBuiltInTools],\n                        help=\"Enabled tools\")\n\n    args = parser.parse_args()\n\n    # Create server config\n    config = ServerConfig(\n        openai_api_key=args.api_key or os.environ.get(\"OPENAI_API_KEY\"),\n        tools=args.tools\n    )\n\n    # Start server\n    server = MCPServer(config)\n    print(f\"Starting MCP server on {args.host}:{args.port}\")\n    server.start(host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"implementation/#error-handling","title":"Error Handling","text":"<p>Robust error handling is implemented across the system:</p> <pre><code>class MCPError(Exception):\n    \"\"\"Base class for all MCP errors\"\"\"\n    def __init__(self, message, status_code=500):\n        self.message = message\n        self.status_code = status_code\n        super().__init__(self.message)\n\nclass ToolNotFoundError(MCPError):\n    \"\"\"Error raised when a requested tool is not found\"\"\"\n    def __init__(self, tool_id):\n        super().__init__(f\"Tool {tool_id} not found\", 404)\n\nclass OpenAIError(MCPError):\n    \"\"\"Error raised when there's an issue with the OpenAI API\"\"\"\n    def __init__(self, message, status_code=500):\n        super().__init__(f\"OpenAI API error: {message}\", status_code)\n\nclass ConfigurationError(MCPError):\n    \"\"\"Error raised when there's an issue with configuration\"\"\"\n    def __init__(self, message):\n        super().__init__(f\"Configuration error: {message}\", 500)\n</code></pre>"},{"location":"implementation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"implementation/#custom-tool-implementation","title":"Custom Tool Implementation","text":"<p>To implement a custom tool adapter:</p> <pre><code>from ..models.mcp import MCPRequest, MCPResponse\nfrom abc import ABC, abstractmethod\n\nclass ToolAdapter(ABC):\n    \"\"\"Base class for tool adapters\"\"\"\n\n    @property\n    @abstractmethod\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"Translate MCP request to OpenAI parameters\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"Translate OpenAI response to MCP response\"\"\"\n        pass\n\n# Example implementation for web search\nclass WebSearchAdapter(ToolAdapter):\n    @property\n    def tool_id(self) -&gt; str:\n        return \"web-search\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        return \"retrieval\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        # Extract search query\n        query = request.parameters.get(\"query\", \"\")\n\n        # Return OpenAI parameters\n        return {\"query\": query}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        # Extract search results\n        results = response.get(\"results\", [])\n\n        # Format results as markdown\n        content = \"# Search Results\\n\\n\"\n        for i, result in enumerate(results):\n            content += f\"## {i+1}. {result.get('title', 'No title')}\\n\"\n            content += f\"**URL**: {result.get('url', 'No URL')}\\n\"\n            content += f\"{result.get('snippet', 'No snippet')}\\n\\n\"\n\n        # Return MCP response\n        return MCPResponse(\n            content=content,\n            context={\"search_query\": query}\n        )\n</code></pre>"},{"location":"implementation/#middleware-support","title":"Middleware Support","text":"<p>Middleware can be used to add cross-cutting functionality:</p> <pre><code>from starlette.middleware.base import BaseHTTPMiddleware\nfrom fastapi import FastAPI\n\nclass LoggingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        # Log request\n        print(f\"Request: {request.method} {request.url}\")\n\n        # Call next middleware\n        response = await call_next(request)\n\n        # Log response\n        print(f\"Response: {response.status_code}\")\n\n        return response\n\ndef add_middleware(app: FastAPI):\n    \"\"\"Add middleware to FastAPI app\"\"\"\n    app.add_middleware(LoggingMiddleware)\n</code></pre> <p>This implementation guide provides a comprehensive blueprint for building the openai-tool2mcp bridge. By following this architecture and implementation details, you can create a fully functional MCP server that leverages OpenAI's powerful built-in tools.</p>"},{"location":"modules/","title":"Module Reference","text":"<p>This page provides auto-generated documentation for the openai-tool2mcp Python modules.</p>"},{"location":"modules/#core-modules","title":"Core Modules","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.server.MCPServer","title":"<code>MCPServer</code>","text":"<p>MCP server that wraps OpenAI tools</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>class MCPServer:\n    \"\"\"MCP server that wraps OpenAI tools\"\"\"\n\n    def __init__(self, config=None):\n        \"\"\"\n        Initialize the MCP server.\n\n        Args:\n            config (ServerConfig, optional): Server configuration\n        \"\"\"\n        self.config = config or ServerConfig()\n\n        # Ensure we have an API key\n        if not self.config.openai_api_key:\n            raise APIKeyMissingError()\n\n        # Initialize the FastMCP server\n        self.mcp = FastMCP(\"openai-tool2mcp\")\n\n        self.openai_client = OpenAIClient(\n            api_key=self.config.openai_api_key,\n            request_timeout=self.config.request_timeout,\n            max_retries=self.config.max_retries,\n        )\n        self.tool_registry = ToolRegistry(self.config.tools)\n        self.tools_map = self._build_tools_map()\n\n        # Register tools with MCP SDK\n        self._register_mcp_tools()\n\n    def _build_tools_map(self):\n        \"\"\"Build a map of tool adapters\"\"\"\n        tools_map = {}\n\n        # Register default tool adapters\n        adapters = [WebSearchAdapter(), CodeInterpreterAdapter(), BrowserAdapter(), FileManagerAdapter()]\n\n        for adapter in adapters:\n            # Only register if the tool is enabled\n            if adapter.openai_tool_type in self.config.tools:\n                tools_map[adapter.tool_id] = adapter\n\n        return tools_map\n\n    def _register_mcp_tools(self):\n        \"\"\"Register tools with the MCP SDK\"\"\"\n        for tool_id, adapter in self.tools_map.items():\n            # Define a tool handler for each adapter\n            # Create a closure to properly capture the values\n            def create_tool_handler(tool_id=tool_id, adapter=adapter):\n                @self.mcp.tool(name=tool_id, description=adapter.description)\n                async def tool_handler(**parameters):\n                    \"\"\"\n                    MCP tool handler for OpenAI tools.\n                    \"\"\"\n                    # Create an MCP request from the parameters\n                    mcp_request = MCPRequest(parameters=parameters)\n\n                    # Translate the request parameters using the adapter\n                    translated_params = await adapter.translate_request(mcp_request)\n\n                    # Create an OpenAI tool request\n                    openai_request = mcp_to_openai.translate_request(mcp_request, tool_id)\n\n                    # Override the parameters with the adapter-specific ones\n                    openai_request.parameters = translated_params\n\n                    try:\n                        # Call OpenAI API to execute the tool\n                        openai_response = await self.openai_client.invoke_tool(openai_request)\n\n                        # Translate the OpenAI response to MCP format using the adapter\n                        if openai_response.tool_outputs:\n                            # Use the adapter to translate the tool-specific response\n                            mcp_response = await adapter.translate_response(openai_response.tool_outputs[0].output)\n\n                            # Add thread_id to context for state management\n                            if mcp_response.context is None:\n                                mcp_response.context = {}\n                            mcp_response.context[\"thread_id\"] = openai_response.thread_id\n\n                            # Return the response content which will be used by MCP SDK\n                            return mcp_response.content\n                        else:\n                            # Fallback to generic translation\n                            mcp_response = openai_to_mcp.translate_response(openai_response)\n                            return mcp_response.content\n                    except Exception as e:\n                        logger.error(f\"Error invoking tool {tool_id}: {e!s}\")\n                        # Using custom exception class to fix TRY003\n                        raise ToolInvocationError() from e\n\n                return tool_handler\n\n            # Create and register the tool handler\n            create_tool_handler()\n\n    def start(self, host=\"127.0.0.1\", port=8000, transport=None):\n        \"\"\"\n        Start the MCP server.\n\n        Args:\n            host (str): Host address to bind to (used if a custom HTTP server is started)\n            port (int): Port to listen on (used if a custom HTTP server is started)\n            transport (str, optional): Transport method ('stdio' or 'sse')\n        \"\"\"\n        logger.info(\"Starting MCP server\")\n        logger.info(f\"Available tools: {', '.join(self.tools_map.keys())}\")\n\n        # If stdio transport is specified, use it\n        if transport == \"stdio\":\n            logger.info(\"Using STDIO transport\")\n            self.mcp.run(transport=\"stdio\")\n        # If sse transport is specified, use it\n        elif transport == \"sse\":\n            logger.info(\"Using SSE transport\")\n            self.mcp.run(transport=\"sse\")\n        # Otherwise start a custom HTTP server\n        else:\n            logger.info(f\"Using custom HTTP transport on {host}:{port}\")\n            import uvicorn\n            from fastapi import FastAPI\n\n            app = FastAPI(\n                title=\"OpenAI Tool2MCP Server\",\n                description=\"MCP server that wraps OpenAI built-in tools\",\n                version=\"0.1.0\",\n            )\n\n            # Define the root endpoint\n            @app.get(\"/\")\n            async def root():\n                return {\n                    \"name\": \"OpenAI Tool2MCP Server\",\n                    \"version\": \"0.1.0\",\n                    \"tools\": [\n                        {\"id\": tool_id, \"description\": adapter.description}\n                        for tool_id, adapter in self.tools_map.items()\n                    ],\n                }\n\n            # Define the health check endpoint\n            @app.get(\"/health\")\n            async def health():\n                return {\"status\": \"ok\"}\n\n            # Start the custom HTTP server\n            uvicorn.run(app, host=host, port=port)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ServerConfig</code> <p>Server configuration</p> <code>None</code> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def __init__(self, config=None):\n    \"\"\"\n    Initialize the MCP server.\n\n    Args:\n        config (ServerConfig, optional): Server configuration\n    \"\"\"\n    self.config = config or ServerConfig()\n\n    # Ensure we have an API key\n    if not self.config.openai_api_key:\n        raise APIKeyMissingError()\n\n    # Initialize the FastMCP server\n    self.mcp = FastMCP(\"openai-tool2mcp\")\n\n    self.openai_client = OpenAIClient(\n        api_key=self.config.openai_api_key,\n        request_timeout=self.config.request_timeout,\n        max_retries=self.config.max_retries,\n    )\n    self.tool_registry = ToolRegistry(self.config.tools)\n    self.tools_map = self._build_tools_map()\n\n    # Register tools with MCP SDK\n    self._register_mcp_tools()\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer._build_tools_map","title":"<code>_build_tools_map()</code>","text":"<p>Build a map of tool adapters</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def _build_tools_map(self):\n    \"\"\"Build a map of tool adapters\"\"\"\n    tools_map = {}\n\n    # Register default tool adapters\n    adapters = [WebSearchAdapter(), CodeInterpreterAdapter(), BrowserAdapter(), FileManagerAdapter()]\n\n    for adapter in adapters:\n        # Only register if the tool is enabled\n        if adapter.openai_tool_type in self.config.tools:\n            tools_map[adapter.tool_id] = adapter\n\n    return tools_map\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer._register_mcp_tools","title":"<code>_register_mcp_tools()</code>","text":"<p>Register tools with the MCP SDK</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def _register_mcp_tools(self):\n    \"\"\"Register tools with the MCP SDK\"\"\"\n    for tool_id, adapter in self.tools_map.items():\n        # Define a tool handler for each adapter\n        # Create a closure to properly capture the values\n        def create_tool_handler(tool_id=tool_id, adapter=adapter):\n            @self.mcp.tool(name=tool_id, description=adapter.description)\n            async def tool_handler(**parameters):\n                \"\"\"\n                MCP tool handler for OpenAI tools.\n                \"\"\"\n                # Create an MCP request from the parameters\n                mcp_request = MCPRequest(parameters=parameters)\n\n                # Translate the request parameters using the adapter\n                translated_params = await adapter.translate_request(mcp_request)\n\n                # Create an OpenAI tool request\n                openai_request = mcp_to_openai.translate_request(mcp_request, tool_id)\n\n                # Override the parameters with the adapter-specific ones\n                openai_request.parameters = translated_params\n\n                try:\n                    # Call OpenAI API to execute the tool\n                    openai_response = await self.openai_client.invoke_tool(openai_request)\n\n                    # Translate the OpenAI response to MCP format using the adapter\n                    if openai_response.tool_outputs:\n                        # Use the adapter to translate the tool-specific response\n                        mcp_response = await adapter.translate_response(openai_response.tool_outputs[0].output)\n\n                        # Add thread_id to context for state management\n                        if mcp_response.context is None:\n                            mcp_response.context = {}\n                        mcp_response.context[\"thread_id\"] = openai_response.thread_id\n\n                        # Return the response content which will be used by MCP SDK\n                        return mcp_response.content\n                    else:\n                        # Fallback to generic translation\n                        mcp_response = openai_to_mcp.translate_response(openai_response)\n                        return mcp_response.content\n                except Exception as e:\n                    logger.error(f\"Error invoking tool {tool_id}: {e!s}\")\n                    # Using custom exception class to fix TRY003\n                    raise ToolInvocationError() from e\n\n            return tool_handler\n\n        # Create and register the tool handler\n        create_tool_handler()\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer.start","title":"<code>start(host='127.0.0.1', port=8000, transport=None)</code>","text":"<p>Start the MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Host address to bind to (used if a custom HTTP server is started)</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>Port to listen on (used if a custom HTTP server is started)</p> <code>8000</code> <code>transport</code> <code>str</code> <p>Transport method ('stdio' or 'sse')</p> <code>None</code> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def start(self, host=\"127.0.0.1\", port=8000, transport=None):\n    \"\"\"\n    Start the MCP server.\n\n    Args:\n        host (str): Host address to bind to (used if a custom HTTP server is started)\n        port (int): Port to listen on (used if a custom HTTP server is started)\n        transport (str, optional): Transport method ('stdio' or 'sse')\n    \"\"\"\n    logger.info(\"Starting MCP server\")\n    logger.info(f\"Available tools: {', '.join(self.tools_map.keys())}\")\n\n    # If stdio transport is specified, use it\n    if transport == \"stdio\":\n        logger.info(\"Using STDIO transport\")\n        self.mcp.run(transport=\"stdio\")\n    # If sse transport is specified, use it\n    elif transport == \"sse\":\n        logger.info(\"Using SSE transport\")\n        self.mcp.run(transport=\"sse\")\n    # Otherwise start a custom HTTP server\n    else:\n        logger.info(f\"Using custom HTTP transport on {host}:{port}\")\n        import uvicorn\n        from fastapi import FastAPI\n\n        app = FastAPI(\n            title=\"OpenAI Tool2MCP Server\",\n            description=\"MCP server that wraps OpenAI built-in tools\",\n            version=\"0.1.0\",\n        )\n\n        # Define the root endpoint\n        @app.get(\"/\")\n        async def root():\n            return {\n                \"name\": \"OpenAI Tool2MCP Server\",\n                \"version\": \"0.1.0\",\n                \"tools\": [\n                    {\"id\": tool_id, \"description\": adapter.description}\n                    for tool_id, adapter in self.tools_map.items()\n                ],\n            }\n\n        # Define the health check endpoint\n        @app.get(\"/health\")\n        async def health():\n            return {\"status\": \"ok\"}\n\n        # Start the custom HTTP server\n        uvicorn.run(app, host=host, port=port)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.ToolInvocationError","title":"<code>ToolInvocationError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Error raised when a tool invocation fails.</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>class ToolInvocationError(ValueError):\n    \"\"\"Error raised when a tool invocation fails.\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Error invoking tool\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.OpenAIBuiltInTools","title":"<code>OpenAIBuiltInTools</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for built-in OpenAI tools</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>class OpenAIBuiltInTools(Enum):\n    \"\"\"Enum for built-in OpenAI tools\"\"\"\n\n    WEB_SEARCH = \"web_search\"\n    CODE_INTERPRETER = \"code_interpreter\"\n    WEB_BROWSER = \"web_browser\"\n    FILE_SEARCH = \"file_search\"\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry","title":"<code>ToolRegistry</code>","text":"<p>Registry for MCP tools mapped to OpenAI tools</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"Registry for MCP tools mapped to OpenAI tools\"\"\"\n\n    def __init__(self, enabled_tools=None):\n        \"\"\"\n        Initialize the tool registry.\n\n        Args:\n            enabled_tools (List[str], optional): List of enabled tools\n        \"\"\"\n        self.tools = {}\n        self.enabled_tools = enabled_tools or [t.value for t in OpenAIBuiltInTools]\n        self._register_default_tools()\n\n    def _register_default_tools(self):\n        \"\"\"Register the default tool mappings\"\"\"\n        self.tools = {\n            \"web-search\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_SEARCH.value in self.enabled_tools,\n                \"description\": \"Search the web for information\",\n            },\n            \"code-execution\": {\n                \"openai_tool\": OpenAIBuiltInTools.CODE_INTERPRETER.value,\n                \"enabled\": OpenAIBuiltInTools.CODE_INTERPRETER.value in self.enabled_tools,\n                \"description\": \"Execute code in a sandbox environment\",\n            },\n            \"browser\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_BROWSER.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_BROWSER.value in self.enabled_tools,\n                \"description\": \"Browse websites and access web content\",\n            },\n            \"file-io\": {\n                \"openai_tool\": OpenAIBuiltInTools.FILE_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.FILE_SEARCH.value in self.enabled_tools,\n                \"description\": \"Search and access file content\",\n            },\n        }\n\n    def register_tool(self, tool_id: str, openai_tool: str, enabled: bool = True, description: str = \"\"):\n        \"\"\"\n        Register a new tool.\n\n        Args:\n            tool_id (str): MCP tool ID\n            openai_tool (str): OpenAI tool type\n            enabled (bool): Whether the tool is enabled\n            description (str): Tool description\n        \"\"\"\n        self.tools[tool_id] = {\"openai_tool\": openai_tool, \"enabled\": enabled, \"description\": description}\n\n    def has_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"\n        Check if a tool is registered and enabled.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool is available\n        \"\"\"\n        return tool_id in self.tools and self.tools[tool_id][\"enabled\"]\n\n    def get_openai_tool_type(self, tool_id: str) -&gt; str | None:\n        \"\"\"\n        Get the OpenAI tool type for a given MCP tool ID.\n\n        Args:\n            tool_id (str): MCP tool ID\n\n        Returns:\n            str: OpenAI tool type\n        \"\"\"\n        if self.has_tool(tool_id):\n            return self.tools[tool_id][\"openai_tool\"]\n        return None\n\n    def get_enabled_tools(self) -&gt; dict[str, dict]:\n        \"\"\"\n        Get all enabled tools.\n\n        Returns:\n            dict[str, dict]: Dictionary of enabled tools\n        \"\"\"\n        return {tool_id: tool_info for tool_id, tool_info in self.tools.items() if tool_info[\"enabled\"]}\n\n    def enable_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"\n        Enable a tool.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool was enabled\n        \"\"\"\n        if tool_id in self.tools:\n            self.tools[tool_id][\"enabled\"] = True\n            return True\n        return False\n\n    def disable_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"\n        Disable a tool.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool was disabled\n        \"\"\"\n        if tool_id in self.tools:\n            self.tools[tool_id][\"enabled\"] = False\n            return True\n        return False\n\n    def register(self, tool_type: ToolType, tool):\n        \"\"\"Register a tool\"\"\"\n        self.tools[tool_type.value] = tool\n\n    def get_tool(self, tool_type: ToolType) -&gt; object | None:\n        \"\"\"Get a tool by type\"\"\"\n        return self.tools.get(tool_type.value)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.__init__","title":"<code>__init__(enabled_tools=None)</code>","text":"<p>Initialize the tool registry.</p> <p>Parameters:</p> Name Type Description Default <code>enabled_tools</code> <code>List[str]</code> <p>List of enabled tools</p> <code>None</code> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def __init__(self, enabled_tools=None):\n    \"\"\"\n    Initialize the tool registry.\n\n    Args:\n        enabled_tools (List[str], optional): List of enabled tools\n    \"\"\"\n    self.tools = {}\n    self.enabled_tools = enabled_tools or [t.value for t in OpenAIBuiltInTools]\n    self._register_default_tools()\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry._register_default_tools","title":"<code>_register_default_tools()</code>","text":"<p>Register the default tool mappings</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def _register_default_tools(self):\n    \"\"\"Register the default tool mappings\"\"\"\n    self.tools = {\n        \"web-search\": {\n            \"openai_tool\": OpenAIBuiltInTools.WEB_SEARCH.value,\n            \"enabled\": OpenAIBuiltInTools.WEB_SEARCH.value in self.enabled_tools,\n            \"description\": \"Search the web for information\",\n        },\n        \"code-execution\": {\n            \"openai_tool\": OpenAIBuiltInTools.CODE_INTERPRETER.value,\n            \"enabled\": OpenAIBuiltInTools.CODE_INTERPRETER.value in self.enabled_tools,\n            \"description\": \"Execute code in a sandbox environment\",\n        },\n        \"browser\": {\n            \"openai_tool\": OpenAIBuiltInTools.WEB_BROWSER.value,\n            \"enabled\": OpenAIBuiltInTools.WEB_BROWSER.value in self.enabled_tools,\n            \"description\": \"Browse websites and access web content\",\n        },\n        \"file-io\": {\n            \"openai_tool\": OpenAIBuiltInTools.FILE_SEARCH.value,\n            \"enabled\": OpenAIBuiltInTools.FILE_SEARCH.value in self.enabled_tools,\n            \"description\": \"Search and access file content\",\n        },\n    }\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.disable_tool","title":"<code>disable_tool(tool_id)</code>","text":"<p>Disable a tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>Tool ID</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool was disabled</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def disable_tool(self, tool_id: str) -&gt; bool:\n    \"\"\"\n    Disable a tool.\n\n    Args:\n        tool_id (str): Tool ID\n\n    Returns:\n        bool: True if the tool was disabled\n    \"\"\"\n    if tool_id in self.tools:\n        self.tools[tool_id][\"enabled\"] = False\n        return True\n    return False\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.enable_tool","title":"<code>enable_tool(tool_id)</code>","text":"<p>Enable a tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>Tool ID</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool was enabled</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def enable_tool(self, tool_id: str) -&gt; bool:\n    \"\"\"\n    Enable a tool.\n\n    Args:\n        tool_id (str): Tool ID\n\n    Returns:\n        bool: True if the tool was enabled\n    \"\"\"\n    if tool_id in self.tools:\n        self.tools[tool_id][\"enabled\"] = True\n        return True\n    return False\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.get_enabled_tools","title":"<code>get_enabled_tools()</code>","text":"<p>Get all enabled tools.</p> <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: Dictionary of enabled tools</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def get_enabled_tools(self) -&gt; dict[str, dict]:\n    \"\"\"\n    Get all enabled tools.\n\n    Returns:\n        dict[str, dict]: Dictionary of enabled tools\n    \"\"\"\n    return {tool_id: tool_info for tool_id, tool_info in self.tools.items() if tool_info[\"enabled\"]}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.get_openai_tool_type","title":"<code>get_openai_tool_type(tool_id)</code>","text":"<p>Get the OpenAI tool type for a given MCP tool ID.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>MCP tool ID</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>OpenAI tool type</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def get_openai_tool_type(self, tool_id: str) -&gt; str | None:\n    \"\"\"\n    Get the OpenAI tool type for a given MCP tool ID.\n\n    Args:\n        tool_id (str): MCP tool ID\n\n    Returns:\n        str: OpenAI tool type\n    \"\"\"\n    if self.has_tool(tool_id):\n        return self.tools[tool_id][\"openai_tool\"]\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.get_tool","title":"<code>get_tool(tool_type)</code>","text":"<p>Get a tool by type</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def get_tool(self, tool_type: ToolType) -&gt; object | None:\n    \"\"\"Get a tool by type\"\"\"\n    return self.tools.get(tool_type.value)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.has_tool","title":"<code>has_tool(tool_id)</code>","text":"<p>Check if a tool is registered and enabled.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>Tool ID</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool is available</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def has_tool(self, tool_id: str) -&gt; bool:\n    \"\"\"\n    Check if a tool is registered and enabled.\n\n    Args:\n        tool_id (str): Tool ID\n\n    Returns:\n        bool: True if the tool is available\n    \"\"\"\n    return tool_id in self.tools and self.tools[tool_id][\"enabled\"]\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.register","title":"<code>register(tool_type, tool)</code>","text":"<p>Register a tool</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def register(self, tool_type: ToolType, tool):\n    \"\"\"Register a tool\"\"\"\n    self.tools[tool_type.value] = tool\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.register_tool","title":"<code>register_tool(tool_id, openai_tool, enabled=True, description='')</code>","text":"<p>Register a new tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>MCP tool ID</p> required <code>openai_tool</code> <code>str</code> <p>OpenAI tool type</p> required <code>enabled</code> <code>bool</code> <p>Whether the tool is enabled</p> <code>True</code> <code>description</code> <code>str</code> <p>Tool description</p> <code>''</code> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def register_tool(self, tool_id: str, openai_tool: str, enabled: bool = True, description: str = \"\"):\n    \"\"\"\n    Register a new tool.\n\n    Args:\n        tool_id (str): MCP tool ID\n        openai_tool (str): OpenAI tool type\n        enabled (bool): Whether the tool is enabled\n        description (str): Tool description\n    \"\"\"\n    self.tools[tool_id] = {\"openai_tool\": openai_tool, \"enabled\": enabled, \"description\": description}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolType","title":"<code>ToolType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for tool types</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>class ToolType(Enum):\n    \"\"\"Enum for tool types\"\"\"\n\n    BROWSER = \"browser\"\n    CODE = \"code\"\n    SEARCH = \"search\"\n</code></pre>"},{"location":"modules/#protocol-translation","title":"Protocol Translation","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.translator.mcp_to_openai.map_tool_id_to_openai_type","title":"<code>map_tool_id_to_openai_type(tool_id)</code>","text":"<p>Map MCP tool IDs to OpenAI tool types.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>MCP tool ID</p> required <p>Returns:</p> Type Description <code>str</code> <p>OpenAI tool type</p> Source code in <code>openai_tool2mcp/translator/mcp_to_openai.py</code> <pre><code>def map_tool_id_to_openai_type(tool_id: str) -&gt; str:\n    \"\"\"\n    Map MCP tool IDs to OpenAI tool types.\n\n    Args:\n        tool_id: MCP tool ID\n\n    Returns:\n        OpenAI tool type\n    \"\"\"\n    mapping = {\n        \"web-search\": \"retrieval\",\n        \"code-execution\": \"code_interpreter\",\n        \"browser\": \"web_browser\",\n        \"file-io\": \"file_search\",\n    }\n\n    openai_type = mapping.get(tool_id, tool_id)\n    logger.debug(f\"Mapped MCP tool ID {tool_id} to OpenAI tool type {openai_type}\")\n\n    return openai_type\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.mcp_to_openai.translate_request","title":"<code>translate_request(mcp_request, tool_id)</code>","text":"<p>Translate an MCP request to an OpenAI request format.</p> <p>Parameters:</p> Name Type Description Default <code>mcp_request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <code>tool_id</code> <code>str</code> <p>The ID of the tool to invoke</p> required <p>Returns:</p> Type Description <code>ToolRequest</code> <p>An OpenAI tool request object</p> Source code in <code>openai_tool2mcp/translator/mcp_to_openai.py</code> <pre><code>def translate_request(mcp_request: MCPRequest, tool_id: str) -&gt; ToolRequest:\n    \"\"\"\n    Translate an MCP request to an OpenAI request format.\n\n    Args:\n        mcp_request: The MCP request to translate\n        tool_id: The ID of the tool to invoke\n\n    Returns:\n        An OpenAI tool request object\n    \"\"\"\n    # Extract tool parameters and sanitize them\n    parameters = sanitize_parameters(mcp_request.parameters)\n\n    # Extract context information\n    context = mcp_request.context or {}\n\n    # Determine if this is a new or existing conversation\n    thread_id = context.get(\"thread_id\")\n\n    # Log the translation\n    logger.debug(f\"Translating MCP request for tool {tool_id} to OpenAI format\")\n\n    # Create OpenAI request\n    openai_request = ToolRequest(\n        tool_type=map_tool_id_to_openai_type(tool_id),\n        parameters=parameters,\n        thread_id=thread_id,\n        instructions=context.get(\"instructions\", \"\"),\n    )\n\n    return openai_request\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.openai_to_mcp.format_code_result","title":"<code>format_code_result(result)</code>","text":"<p>Format code execution result in a human-readable format</p> Source code in <code>openai_tool2mcp/translator/openai_to_mcp.py</code> <pre><code>def format_code_result(result: dict[str, Any] | str) -&gt; str:\n    \"\"\"\n    Format code execution result in a human-readable format\n    \"\"\"\n    if isinstance(result, str):\n        return result\n\n    output = result.get(\"output\", \"\")\n    error = result.get(\"error\")\n    if error:\n        return f\"Error: {error}\\n\\nOutput:\\n{output}\"\n    return output\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.openai_to_mcp.format_search_results","title":"<code>format_search_results(results)</code>","text":"<p>Format search results in a human-readable format</p> Source code in <code>openai_tool2mcp/translator/openai_to_mcp.py</code> <pre><code>def format_search_results(results: list[dict[str, Any]]) -&gt; str:\n    \"\"\"\n    Format search results in a human-readable format\n    \"\"\"\n    if not results:\n        return \"No results found.\"\n\n    formatted_results = []\n    for result in results:\n        title = result.get(\"title\", \"Untitled\")\n        content = result.get(\"content\", \"\")\n        formatted_results.append(f\"# {title}\\n\\n{content}\")\n\n    return \"\\n\\n\".join(formatted_results)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.openai_to_mcp.translate_response","title":"<code>translate_response(openai_response)</code>","text":"<p>Translate an OpenAI response to an MCP response format.</p> <p>Parameters:</p> Name Type Description Default <code>openai_response</code> <code>ToolResponse</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>An MCP response object</p> Source code in <code>openai_tool2mcp/translator/openai_to_mcp.py</code> <pre><code>def translate_response(openai_response: ToolResponse) -&gt; MCPResponse:\n    \"\"\"\n    Translate an OpenAI response to an MCP response format.\n\n    Args:\n        openai_response: The OpenAI response to translate\n\n    Returns:\n        An MCP response object\n    \"\"\"\n    # Extract tool output\n    tool_output = openai_response.tool_outputs[0] if openai_response.tool_outputs else None\n\n    # Log the translation\n    logger.debug(\"Translating OpenAI response to MCP format\")\n\n    if not tool_output:\n        # Handle case where there's no tool output\n        logger.warning(\"No tool output found in OpenAI response\")\n        return MCPResponse(\n            content=\"No result\", error=\"Tool returned no output\", context={\"thread_id\": openai_response.thread_id}\n        )\n\n    # Format the output content\n    try:\n        # Try to parse as JSON for structured output\n        output_content = tool_output.output\n        if isinstance(output_content, str):\n            try:\n                # If it's a JSON string, parse it\n                parsed_content = json.loads(output_content)\n                # Format it nicely if possible\n                if isinstance(parsed_content, dict) and \"result\" in parsed_content:\n                    content = str(parsed_content[\"result\"])\n                else:\n                    content = json.dumps(parsed_content, indent=2)\n            except json.JSONDecodeError:\n                # Not JSON, use as is\n                content = output_content\n        else:\n            # Not a string, convert to string\n            content = str(output_content)\n    except Exception as e:\n        # Fallback for any errors\n        logger.error(f\"Error formatting tool output: {e!s}\")\n        content = str(tool_output.output)\n\n    # Create MCP response\n    error = tool_output.error if hasattr(tool_output, \"error\") and tool_output.error else None\n\n    mcp_response = MCPResponse(content=content, error=error, context={\"thread_id\": openai_response.thread_id})\n\n    return mcp_response\n</code></pre>"},{"location":"modules/#api-clients","title":"API Clients","text":"<p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.openai_client.client.AssistantCreationError","title":"<code>AssistantCreationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception raised when assistant creation fails</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class AssistantCreationError(RuntimeError):\n    \"\"\"Exception raised when assistant creation fails\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Assistant creation failed\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.NoChoicesError","title":"<code>NoChoicesError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when no choices are found in response</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class NoChoicesError(ValueError):\n    \"\"\"Exception raised when no choices are found in response\"\"\"\n\n    def __init__(self):\n        super().__init__(\"No choices\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>Client for interacting with the OpenAI API</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"Client for interacting with the OpenAI API\"\"\"\n\n    def __init__(self, api_key: str | None, request_timeout: int = 30, max_retries: int = 3):\n        \"\"\"\n        Initialize the OpenAI client.\n\n        Args:\n            api_key (str): OpenAI API key\n            request_timeout (int): Request timeout in seconds\n            max_retries (int): Maximum number of retries\n        \"\"\"\n        self.client = openai.Client(api_key=api_key, timeout=request_timeout)\n        self.max_retries = max_retries\n        self.retry_delay = 1  # Assuming a default retry_delay\n\n    async def invoke_tool(self, request: ToolRequest) -&gt; ToolResponse:\n        \"\"\"\n        Invoke an OpenAI tool.\n\n        Args:\n            request (ToolRequest): Tool request\n\n        Returns:\n            ToolResponse: Tool response\n        \"\"\"\n        # Special handling for web search\n        if request.tool_type == \"web_search\":\n            return await self._handle_web_search(request)\n\n        # Create or get thread\n        thread_id = request.thread_id\n        if not thread_id:\n            thread = await self._create_thread()\n            if thread is None:\n                raise ThreadCreationError()\n            thread_id = thread.id\n\n        # Create message with tool call\n        await self._create_message(\n            thread_id=thread_id,\n            content=f\"Please use the {request.tool_type} tool with these parameters: {request.parameters}\",\n        )\n\n        # Create assistant with the appropriate tool\n        assistant = await self._create_assistant(\n            tools=[{\"type\": request.tool_type}],\n            instructions=request.instructions or \"Execute the requested tool function.\",\n        )\n        if assistant is None:\n            raise AssistantCreationError()\n\n        # Run the assistant\n        run = await self._create_run(thread_id=thread_id, assistant_id=assistant.id)\n        if run is None:\n            raise RunCreationError()\n\n        # Wait for completion\n        run = await self._wait_for_run(thread_id, run.id)\n\n        # Get tool outputs\n        tool_outputs = []\n        if hasattr(run, \"required_action\") and hasattr(run.required_action, \"submit_tool_outputs\"):\n            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n                # Process each tool call\n                tool_outputs.append(ToolOutput(output=tool_call.function.arguments, error=None))\n\n        # Create response\n        response = ToolResponse(thread_id=thread_id, tool_outputs=tool_outputs)\n\n        return response\n\n    async def _create_thread(self) -&gt; Any | None:\n        \"\"\"Create a new thread\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                thread = await asyncio.to_thread(self.client.beta.threads.create)\n            except Exception as e:\n                logger.error(f\"Error creating thread (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return thread\n        return None\n\n    async def _create_message(self, thread_id: str, content: str) -&gt; Any | None:\n        \"\"\"Create a new message in a thread\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                message = await asyncio.to_thread(\n                    self.client.beta.threads.messages.create, thread_id=thread_id, role=\"user\", content=content\n                )\n            except Exception as e:\n                logger.error(f\"Error creating message (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return message\n        return None\n\n    async def _create_assistant(\n        self, tools: list[dict], instructions: str = \"\", model: str = \"gpt-4o-mini\"\n    ) -&gt; Any | None:\n        \"\"\"Create a new assistant with the specified tools\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                # Prepare assistant parameters based on tool type\n                assistant_params = self._prepare_assistant_params(tools, instructions, model)\n\n                # Create the assistant with prepared parameters\n                assistant = await asyncio.to_thread(self.client.beta.assistants.create, **assistant_params)\n\n                logger.info(f\"Assistant created: {assistant.id}\")\n            except Exception as e:\n                logger.error(f\"Error creating assistant (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return assistant\n        return None\n\n    def _prepare_assistant_params(self, tools: list[dict], instructions: str, model: str) -&gt; dict:\n        \"\"\"Prepare parameters for assistant creation based on tool types\"\"\"\n        # Check for web search tool\n        if tools and tools[0].get(\"type\") == \"web_search\":\n            return self._prepare_web_search_assistant_params(instructions)\n        else:\n            return self._prepare_standard_assistant_params(tools, instructions, model)\n\n    def _prepare_web_search_assistant_params(self, instructions: str) -&gt; dict:\n        \"\"\"Prepare parameters specifically for web search assistant\"\"\"\n        model = \"gpt-4o-mini-search-preview\"\n        logger.info(f\"Creating assistant with {model} for web search\")\n\n        return {\"name\": \"Tool Assistant\", \"model\": model, \"instructions\": instructions}\n\n    def _prepare_standard_assistant_params(self, tools: list[dict], instructions: str, model: str) -&gt; dict:\n        \"\"\"Prepare parameters for standard assistant with various tools\"\"\"\n        # Convert tools to format expected by OpenAI\n        formatted_tools = self._format_tools(tools)\n\n        assistant_params = {\"name\": \"Tool Assistant\", \"model\": model, \"instructions\": instructions}\n\n        # Add tools only if present\n        if formatted_tools:\n            assistant_params[\"tools\"] = formatted_tools\n\n        logger.info(f\"Creating assistant with model {model} and tools: {formatted_tools}\")\n        return assistant_params\n\n    def _format_tools(self, tools: list[dict]) -&gt; list[dict]:\n        \"\"\"Format tools list for OpenAI API compatibility\"\"\"\n        formatted_tools = []\n        for tool in tools:\n            tool_type = tool.get(\"type\", \"\")\n            if tool_type == \"code_interpreter\":\n                formatted_tools.append({\"type\": \"code_interpreter\"})\n            elif tool_type == \"file_search\":\n                formatted_tools.append({\"type\": \"retrieval\"})\n            elif tool_type == \"web_browser\":\n                # This type is not directly supported, so we substitute with retrieval\n                formatted_tools.append({\"type\": \"retrieval\"})\n\n        return formatted_tools\n\n    async def _create_run(self, thread_id: str, assistant_id: str) -&gt; Any | None:\n        \"\"\"Create a new run\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                run = await asyncio.to_thread(\n                    self.client.beta.threads.runs.create, thread_id=thread_id, assistant_id=assistant_id\n                )\n            except Exception as e:\n                logger.error(f\"Error creating run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return run\n        return None\n\n    async def _wait_for_run(self, thread_id: str, run_id: str) -&gt; Any:\n        \"\"\"Wait for a run to complete\"\"\"\n        max_wait_time = 60  # Maximum wait time in seconds\n        start_time = time.time()\n\n        while True:\n            if time.time() - start_time &gt; max_wait_time:\n                raise RunTimeoutError()\n\n            for attempt in range(self.max_retries):\n                try:\n                    run = await asyncio.to_thread(\n                        self.client.beta.threads.runs.retrieve, thread_id=thread_id, run_id=run_id\n                    )\n                    break\n                except Exception as e:\n                    logger.error(f\"Error retrieving run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                    if attempt == self.max_retries - 1:\n                        raise\n                    await asyncio.sleep(self.retry_delay)\n\n            if run.status in [\"completed\", \"failed\", \"requires_action\"]:\n                return run\n\n            await asyncio.sleep(1)\n\n    async def _handle_web_search(self, request: ToolRequest) -&gt; ToolResponse:\n        \"\"\"\n        Handle web search request using the chat completions API.\n\n        Args:\n            request (ToolRequest): Tool request\n\n        Returns:\n            ToolResponse: Tool response\n        \"\"\"\n        logger.info(f\"[WEB SEARCH] Handling request: {request.parameters}\")\n\n        # Validate and extract query\n        query = request.parameters.get(\"query\", \"\")\n        if not query:\n            logger.warning(\"[WEB SEARCH] Empty query received\")\n            return self._create_error_response(request.thread_id, \"No query provided\")\n\n        logger.info(f\"[WEB SEARCH] Executing with query: '{query}'\")\n\n        # Try multiple attempts based on max_retries\n        for attempt in range(self.max_retries):\n            try:\n                # Create and send request to OpenAI\n                response = await self._execute_web_search_request(query, attempt)\n\n                # Process the response\n                output_json = await self._process_web_search_response(response)\n\n                # Return successful response\n                logger.info(\"[WEB SEARCH] Successfully completed request\")\n                return ToolResponse(\n                    thread_id=request.thread_id or f\"web_search_{int(time.time())}\",\n                    tool_outputs=[ToolOutput(output=output_json, error=None)],\n                )\n\n            except Exception as e:\n                if not await self._handle_web_search_error(e, attempt):\n                    # Return error response on final attempt\n                    error_msg = str(e)\n                    return self._create_error_response(request.thread_id, error_msg)\n\n        # Unexpected situation - all attempts failed without exception\n        logger.error(\"[WEB SEARCH] Unexpected end of method without result or error\")\n        return self._create_error_response(request.thread_id, \"Unknown error in web search process\")\n\n    def _create_error_response(self, thread_id: str | None, error_msg: str) -&gt; ToolResponse:\n        \"\"\"Create an error response for web search\"\"\"\n        return ToolResponse(\n            thread_id=thread_id or \"error\",\n            tool_outputs=[ToolOutput(output=json.dumps({\"error\": error_msg}), error=error_msg)],\n        )\n\n    async def _execute_web_search_request(self, query: str, attempt: int) -&gt; Any:\n        \"\"\"Execute the web search request to OpenAI API\"\"\"\n        logger.debug(f\"[WEB SEARCH] Request attempt {attempt + 1}/{self.max_retries}\")\n        logger.debug(\"[WEB SEARCH] Creating request with model: 'gpt-4o-mini-search-preview'\")\n\n        # Prepare request data\n        request_data = {\n            \"model\": \"gpt-4o-mini-search-preview\",\n            \"messages\": [{\"role\": \"user\", \"content\": query}],\n            \"web_search_options\": {},  # Empty object to enable web search\n            \"store\": True,  # Store the response (optional)\n        }\n\n        logger.debug(f\"[WEB SEARCH] Full request data: {json.dumps(request_data)}\")\n\n        # Execute API call\n        return await asyncio.to_thread(self.client.chat.completions.create, **request_data)\n\n    async def _handle_web_search_error(self, error: Exception, attempt: int) -&gt; bool:\n        \"\"\"Handle errors in web search. Returns True if should retry, False if should stop.\"\"\"\n        logger.error(f\"[WEB SEARCH] Error: {error!s}\")\n\n        if attempt == self.max_retries - 1:\n            logger.error(f\"[WEB SEARCH] All attempts failed. Last error: {error}\")\n            return False\n\n        # Delay before retrying\n        logger.info(f\"[WEB SEARCH] Retrying in {self.retry_delay}s...\")\n        await asyncio.sleep(self.retry_delay)\n        return True\n\n    async def _process_web_search_response(self, response: Any) -&gt; str:\n        \"\"\"Process the web search response and return the serialized JSON output\"\"\"\n        # Log response information for debugging\n        logger.debug(f\"[WEB SEARCH] Response type: {type(response)}\")\n        logger.debug(f\"[WEB SEARCH] Response attributes: {dir(response)}\")\n\n        # Check if response has choices\n        if not response.choices:\n            logger.warning(\"[WEB SEARCH] No choices found in response\")\n            raise NoChoicesError()\n\n        # Extract message content\n        message = response.choices[0].message\n        logger.debug(f\"[WEB SEARCH] Message role: {message.role}\")\n        logger.debug(f\"[WEB SEARCH] Message attributes: {dir(message)}\")\n\n        # Get content\n        content = message.content or \"\"\n        logger.info(f\"[WEB SEARCH] Response content length: {len(content)}\")\n        logger.debug(f\"[WEB SEARCH] Content preview: {content[:100]}...\")\n\n        # Check for annotations\n        annotations = self._extract_annotations(message)\n\n        # Prepare response data\n        response_data = {\"content\": content, \"annotations\": annotations}\n        logger.debug(f\"[WEB SEARCH] Constructed response data with keys: {list(response_data.keys())}\")\n\n        # Serialize to JSON\n        return self._serialize_response_data(response_data, content)\n\n    def _extract_annotations(self, message: Any) -&gt; list:\n        \"\"\"Extract annotations from the message if available\"\"\"\n        annotations = []\n        has_annotations = hasattr(message, \"annotations\")\n        logger.debug(f\"[WEB SEARCH] Has annotations attribute: {has_annotations}\")\n\n        if has_annotations and message.annotations is not None:\n            annotations = message.annotations\n            logger.info(f\"[WEB SEARCH] Found {len(annotations)} annotations\")\n\n            # Log annotation details\n            for i, annotation in enumerate(annotations):\n                logger.debug(f\"[WEB SEARCH] Annotation {i + 1}: {type(annotation)}\")\n                if hasattr(annotation, \"type\"):\n                    logger.debug(f\"[WEB SEARCH] Annotation type: {annotation.type}\")\n\n        return annotations\n\n    def _serialize_response_data(self, response_data: dict, content: str) -&gt; str:\n        \"\"\"Serialize the response data to JSON\"\"\"\n        try:\n            # Helper function for serializing complex objects\n            def serialize_openai_objects(obj):\n                if hasattr(obj, \"model_dump_json\"):\n                    # Use model_dump_json for Pydantic objects\n                    return json.loads(obj.model_dump_json())\n                elif hasattr(obj, \"to_dict\"):\n                    # Use to_dict method if available\n                    return obj.to_dict()\n                elif hasattr(obj, \"__dict__\"):\n                    # Use __dict__ attribute if available\n                    return obj.__dict__\n                else:\n                    # Convert to string for other objects\n                    return str(obj)\n\n            # Convert complex OpenAI objects to standard Python objects\n            output_json = json.dumps(response_data, default=serialize_openai_objects)\n            logger.debug(f\"[WEB SEARCH] Successfully serialized to JSON (length: {len(output_json)})\")\n        except (TypeError, Exception) as e:\n            logger.error(f\"[WEB SEARCH] JSON serialization error: {e}\")\n\n            # Fallback response\n            fallback_data = {\"content\": content, \"error\": f\"Failed to serialize annotations: {e!s}\"}\n            output_json = json.dumps(fallback_data)\n            logger.debug(f\"[WEB SEARCH] Using fallback JSON: {output_json[:100]}...\")\n\n        return output_json\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient.__init__","title":"<code>__init__(api_key, request_timeout=30, max_retries=3)</code>","text":"<p>Initialize the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>request_timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries</p> <code>3</code> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def __init__(self, api_key: str | None, request_timeout: int = 30, max_retries: int = 3):\n    \"\"\"\n    Initialize the OpenAI client.\n\n    Args:\n        api_key (str): OpenAI API key\n        request_timeout (int): Request timeout in seconds\n        max_retries (int): Maximum number of retries\n    \"\"\"\n    self.client = openai.Client(api_key=api_key, timeout=request_timeout)\n    self.max_retries = max_retries\n    self.retry_delay = 1  # Assuming a default retry_delay\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_assistant","title":"<code>_create_assistant(tools, instructions='', model='gpt-4o-mini')</code>  <code>async</code>","text":"<p>Create a new assistant with the specified tools</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_assistant(\n    self, tools: list[dict], instructions: str = \"\", model: str = \"gpt-4o-mini\"\n) -&gt; Any | None:\n    \"\"\"Create a new assistant with the specified tools\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            # Prepare assistant parameters based on tool type\n            assistant_params = self._prepare_assistant_params(tools, instructions, model)\n\n            # Create the assistant with prepared parameters\n            assistant = await asyncio.to_thread(self.client.beta.assistants.create, **assistant_params)\n\n            logger.info(f\"Assistant created: {assistant.id}\")\n        except Exception as e:\n            logger.error(f\"Error creating assistant (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return assistant\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_error_response","title":"<code>_create_error_response(thread_id, error_msg)</code>","text":"<p>Create an error response for web search</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _create_error_response(self, thread_id: str | None, error_msg: str) -&gt; ToolResponse:\n    \"\"\"Create an error response for web search\"\"\"\n    return ToolResponse(\n        thread_id=thread_id or \"error\",\n        tool_outputs=[ToolOutput(output=json.dumps({\"error\": error_msg}), error=error_msg)],\n    )\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_message","title":"<code>_create_message(thread_id, content)</code>  <code>async</code>","text":"<p>Create a new message in a thread</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_message(self, thread_id: str, content: str) -&gt; Any | None:\n    \"\"\"Create a new message in a thread\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            message = await asyncio.to_thread(\n                self.client.beta.threads.messages.create, thread_id=thread_id, role=\"user\", content=content\n            )\n        except Exception as e:\n            logger.error(f\"Error creating message (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return message\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_run","title":"<code>_create_run(thread_id, assistant_id)</code>  <code>async</code>","text":"<p>Create a new run</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_run(self, thread_id: str, assistant_id: str) -&gt; Any | None:\n    \"\"\"Create a new run\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            run = await asyncio.to_thread(\n                self.client.beta.threads.runs.create, thread_id=thread_id, assistant_id=assistant_id\n            )\n        except Exception as e:\n            logger.error(f\"Error creating run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return run\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_thread","title":"<code>_create_thread()</code>  <code>async</code>","text":"<p>Create a new thread</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_thread(self) -&gt; Any | None:\n    \"\"\"Create a new thread\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            thread = await asyncio.to_thread(self.client.beta.threads.create)\n        except Exception as e:\n            logger.error(f\"Error creating thread (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return thread\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._execute_web_search_request","title":"<code>_execute_web_search_request(query, attempt)</code>  <code>async</code>","text":"<p>Execute the web search request to OpenAI API</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _execute_web_search_request(self, query: str, attempt: int) -&gt; Any:\n    \"\"\"Execute the web search request to OpenAI API\"\"\"\n    logger.debug(f\"[WEB SEARCH] Request attempt {attempt + 1}/{self.max_retries}\")\n    logger.debug(\"[WEB SEARCH] Creating request with model: 'gpt-4o-mini-search-preview'\")\n\n    # Prepare request data\n    request_data = {\n        \"model\": \"gpt-4o-mini-search-preview\",\n        \"messages\": [{\"role\": \"user\", \"content\": query}],\n        \"web_search_options\": {},  # Empty object to enable web search\n        \"store\": True,  # Store the response (optional)\n    }\n\n    logger.debug(f\"[WEB SEARCH] Full request data: {json.dumps(request_data)}\")\n\n    # Execute API call\n    return await asyncio.to_thread(self.client.chat.completions.create, **request_data)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._extract_annotations","title":"<code>_extract_annotations(message)</code>","text":"<p>Extract annotations from the message if available</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _extract_annotations(self, message: Any) -&gt; list:\n    \"\"\"Extract annotations from the message if available\"\"\"\n    annotations = []\n    has_annotations = hasattr(message, \"annotations\")\n    logger.debug(f\"[WEB SEARCH] Has annotations attribute: {has_annotations}\")\n\n    if has_annotations and message.annotations is not None:\n        annotations = message.annotations\n        logger.info(f\"[WEB SEARCH] Found {len(annotations)} annotations\")\n\n        # Log annotation details\n        for i, annotation in enumerate(annotations):\n            logger.debug(f\"[WEB SEARCH] Annotation {i + 1}: {type(annotation)}\")\n            if hasattr(annotation, \"type\"):\n                logger.debug(f\"[WEB SEARCH] Annotation type: {annotation.type}\")\n\n    return annotations\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._format_tools","title":"<code>_format_tools(tools)</code>","text":"<p>Format tools list for OpenAI API compatibility</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _format_tools(self, tools: list[dict]) -&gt; list[dict]:\n    \"\"\"Format tools list for OpenAI API compatibility\"\"\"\n    formatted_tools = []\n    for tool in tools:\n        tool_type = tool.get(\"type\", \"\")\n        if tool_type == \"code_interpreter\":\n            formatted_tools.append({\"type\": \"code_interpreter\"})\n        elif tool_type == \"file_search\":\n            formatted_tools.append({\"type\": \"retrieval\"})\n        elif tool_type == \"web_browser\":\n            # This type is not directly supported, so we substitute with retrieval\n            formatted_tools.append({\"type\": \"retrieval\"})\n\n    return formatted_tools\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._handle_web_search","title":"<code>_handle_web_search(request)</code>  <code>async</code>","text":"<p>Handle web search request using the chat completions API.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ToolRequest</code> <p>Tool request</p> required <p>Returns:</p> Name Type Description <code>ToolResponse</code> <code>ToolResponse</code> <p>Tool response</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _handle_web_search(self, request: ToolRequest) -&gt; ToolResponse:\n    \"\"\"\n    Handle web search request using the chat completions API.\n\n    Args:\n        request (ToolRequest): Tool request\n\n    Returns:\n        ToolResponse: Tool response\n    \"\"\"\n    logger.info(f\"[WEB SEARCH] Handling request: {request.parameters}\")\n\n    # Validate and extract query\n    query = request.parameters.get(\"query\", \"\")\n    if not query:\n        logger.warning(\"[WEB SEARCH] Empty query received\")\n        return self._create_error_response(request.thread_id, \"No query provided\")\n\n    logger.info(f\"[WEB SEARCH] Executing with query: '{query}'\")\n\n    # Try multiple attempts based on max_retries\n    for attempt in range(self.max_retries):\n        try:\n            # Create and send request to OpenAI\n            response = await self._execute_web_search_request(query, attempt)\n\n            # Process the response\n            output_json = await self._process_web_search_response(response)\n\n            # Return successful response\n            logger.info(\"[WEB SEARCH] Successfully completed request\")\n            return ToolResponse(\n                thread_id=request.thread_id or f\"web_search_{int(time.time())}\",\n                tool_outputs=[ToolOutput(output=output_json, error=None)],\n            )\n\n        except Exception as e:\n            if not await self._handle_web_search_error(e, attempt):\n                # Return error response on final attempt\n                error_msg = str(e)\n                return self._create_error_response(request.thread_id, error_msg)\n\n    # Unexpected situation - all attempts failed without exception\n    logger.error(\"[WEB SEARCH] Unexpected end of method without result or error\")\n    return self._create_error_response(request.thread_id, \"Unknown error in web search process\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._handle_web_search_error","title":"<code>_handle_web_search_error(error, attempt)</code>  <code>async</code>","text":"<p>Handle errors in web search. Returns True if should retry, False if should stop.</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _handle_web_search_error(self, error: Exception, attempt: int) -&gt; bool:\n    \"\"\"Handle errors in web search. Returns True if should retry, False if should stop.\"\"\"\n    logger.error(f\"[WEB SEARCH] Error: {error!s}\")\n\n    if attempt == self.max_retries - 1:\n        logger.error(f\"[WEB SEARCH] All attempts failed. Last error: {error}\")\n        return False\n\n    # Delay before retrying\n    logger.info(f\"[WEB SEARCH] Retrying in {self.retry_delay}s...\")\n    await asyncio.sleep(self.retry_delay)\n    return True\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._prepare_assistant_params","title":"<code>_prepare_assistant_params(tools, instructions, model)</code>","text":"<p>Prepare parameters for assistant creation based on tool types</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _prepare_assistant_params(self, tools: list[dict], instructions: str, model: str) -&gt; dict:\n    \"\"\"Prepare parameters for assistant creation based on tool types\"\"\"\n    # Check for web search tool\n    if tools and tools[0].get(\"type\") == \"web_search\":\n        return self._prepare_web_search_assistant_params(instructions)\n    else:\n        return self._prepare_standard_assistant_params(tools, instructions, model)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._prepare_standard_assistant_params","title":"<code>_prepare_standard_assistant_params(tools, instructions, model)</code>","text":"<p>Prepare parameters for standard assistant with various tools</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _prepare_standard_assistant_params(self, tools: list[dict], instructions: str, model: str) -&gt; dict:\n    \"\"\"Prepare parameters for standard assistant with various tools\"\"\"\n    # Convert tools to format expected by OpenAI\n    formatted_tools = self._format_tools(tools)\n\n    assistant_params = {\"name\": \"Tool Assistant\", \"model\": model, \"instructions\": instructions}\n\n    # Add tools only if present\n    if formatted_tools:\n        assistant_params[\"tools\"] = formatted_tools\n\n    logger.info(f\"Creating assistant with model {model} and tools: {formatted_tools}\")\n    return assistant_params\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._prepare_web_search_assistant_params","title":"<code>_prepare_web_search_assistant_params(instructions)</code>","text":"<p>Prepare parameters specifically for web search assistant</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _prepare_web_search_assistant_params(self, instructions: str) -&gt; dict:\n    \"\"\"Prepare parameters specifically for web search assistant\"\"\"\n    model = \"gpt-4o-mini-search-preview\"\n    logger.info(f\"Creating assistant with {model} for web search\")\n\n    return {\"name\": \"Tool Assistant\", \"model\": model, \"instructions\": instructions}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._process_web_search_response","title":"<code>_process_web_search_response(response)</code>  <code>async</code>","text":"<p>Process the web search response and return the serialized JSON output</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _process_web_search_response(self, response: Any) -&gt; str:\n    \"\"\"Process the web search response and return the serialized JSON output\"\"\"\n    # Log response information for debugging\n    logger.debug(f\"[WEB SEARCH] Response type: {type(response)}\")\n    logger.debug(f\"[WEB SEARCH] Response attributes: {dir(response)}\")\n\n    # Check if response has choices\n    if not response.choices:\n        logger.warning(\"[WEB SEARCH] No choices found in response\")\n        raise NoChoicesError()\n\n    # Extract message content\n    message = response.choices[0].message\n    logger.debug(f\"[WEB SEARCH] Message role: {message.role}\")\n    logger.debug(f\"[WEB SEARCH] Message attributes: {dir(message)}\")\n\n    # Get content\n    content = message.content or \"\"\n    logger.info(f\"[WEB SEARCH] Response content length: {len(content)}\")\n    logger.debug(f\"[WEB SEARCH] Content preview: {content[:100]}...\")\n\n    # Check for annotations\n    annotations = self._extract_annotations(message)\n\n    # Prepare response data\n    response_data = {\"content\": content, \"annotations\": annotations}\n    logger.debug(f\"[WEB SEARCH] Constructed response data with keys: {list(response_data.keys())}\")\n\n    # Serialize to JSON\n    return self._serialize_response_data(response_data, content)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._serialize_response_data","title":"<code>_serialize_response_data(response_data, content)</code>","text":"<p>Serialize the response data to JSON</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def _serialize_response_data(self, response_data: dict, content: str) -&gt; str:\n    \"\"\"Serialize the response data to JSON\"\"\"\n    try:\n        # Helper function for serializing complex objects\n        def serialize_openai_objects(obj):\n            if hasattr(obj, \"model_dump_json\"):\n                # Use model_dump_json for Pydantic objects\n                return json.loads(obj.model_dump_json())\n            elif hasattr(obj, \"to_dict\"):\n                # Use to_dict method if available\n                return obj.to_dict()\n            elif hasattr(obj, \"__dict__\"):\n                # Use __dict__ attribute if available\n                return obj.__dict__\n            else:\n                # Convert to string for other objects\n                return str(obj)\n\n        # Convert complex OpenAI objects to standard Python objects\n        output_json = json.dumps(response_data, default=serialize_openai_objects)\n        logger.debug(f\"[WEB SEARCH] Successfully serialized to JSON (length: {len(output_json)})\")\n    except (TypeError, Exception) as e:\n        logger.error(f\"[WEB SEARCH] JSON serialization error: {e}\")\n\n        # Fallback response\n        fallback_data = {\"content\": content, \"error\": f\"Failed to serialize annotations: {e!s}\"}\n        output_json = json.dumps(fallback_data)\n        logger.debug(f\"[WEB SEARCH] Using fallback JSON: {output_json[:100]}...\")\n\n    return output_json\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._wait_for_run","title":"<code>_wait_for_run(thread_id, run_id)</code>  <code>async</code>","text":"<p>Wait for a run to complete</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _wait_for_run(self, thread_id: str, run_id: str) -&gt; Any:\n    \"\"\"Wait for a run to complete\"\"\"\n    max_wait_time = 60  # Maximum wait time in seconds\n    start_time = time.time()\n\n    while True:\n        if time.time() - start_time &gt; max_wait_time:\n            raise RunTimeoutError()\n\n        for attempt in range(self.max_retries):\n            try:\n                run = await asyncio.to_thread(\n                    self.client.beta.threads.runs.retrieve, thread_id=thread_id, run_id=run_id\n                )\n                break\n            except Exception as e:\n                logger.error(f\"Error retrieving run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n\n        if run.status in [\"completed\", \"failed\", \"requires_action\"]:\n            return run\n\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient.invoke_tool","title":"<code>invoke_tool(request)</code>  <code>async</code>","text":"<p>Invoke an OpenAI tool.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ToolRequest</code> <p>Tool request</p> required <p>Returns:</p> Name Type Description <code>ToolResponse</code> <code>ToolResponse</code> <p>Tool response</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def invoke_tool(self, request: ToolRequest) -&gt; ToolResponse:\n    \"\"\"\n    Invoke an OpenAI tool.\n\n    Args:\n        request (ToolRequest): Tool request\n\n    Returns:\n        ToolResponse: Tool response\n    \"\"\"\n    # Special handling for web search\n    if request.tool_type == \"web_search\":\n        return await self._handle_web_search(request)\n\n    # Create or get thread\n    thread_id = request.thread_id\n    if not thread_id:\n        thread = await self._create_thread()\n        if thread is None:\n            raise ThreadCreationError()\n        thread_id = thread.id\n\n    # Create message with tool call\n    await self._create_message(\n        thread_id=thread_id,\n        content=f\"Please use the {request.tool_type} tool with these parameters: {request.parameters}\",\n    )\n\n    # Create assistant with the appropriate tool\n    assistant = await self._create_assistant(\n        tools=[{\"type\": request.tool_type}],\n        instructions=request.instructions or \"Execute the requested tool function.\",\n    )\n    if assistant is None:\n        raise AssistantCreationError()\n\n    # Run the assistant\n    run = await self._create_run(thread_id=thread_id, assistant_id=assistant.id)\n    if run is None:\n        raise RunCreationError()\n\n    # Wait for completion\n    run = await self._wait_for_run(thread_id, run.id)\n\n    # Get tool outputs\n    tool_outputs = []\n    if hasattr(run, \"required_action\") and hasattr(run.required_action, \"submit_tool_outputs\"):\n        for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n            # Process each tool call\n            tool_outputs.append(ToolOutput(output=tool_call.function.arguments, error=None))\n\n    # Create response\n    response = ToolResponse(thread_id=thread_id, tool_outputs=tool_outputs)\n\n    return response\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.RunCreationError","title":"<code>RunCreationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception raised when run creation fails</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class RunCreationError(RuntimeError):\n    \"\"\"Exception raised when run creation fails\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Run creation failed\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.RunTimeoutError","title":"<code>RunTimeoutError</code>","text":"<p>               Bases: <code>TimeoutError</code></p> <p>Exception raised when run times out</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class RunTimeoutError(TimeoutError):\n    \"\"\"Exception raised when run times out\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Run timed out\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.ThreadCreationError","title":"<code>ThreadCreationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception raised when thread creation fails</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class ThreadCreationError(RuntimeError):\n    \"\"\"Exception raised when thread creation fails\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Thread creation failed\")\n</code></pre>"},{"location":"modules/#tool-implementations","title":"Tool Implementations","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter","title":"<code>WebSearchAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's web search tool</p> Source code in <code>openai_tool2mcp/tools/web_search.py</code> <pre><code>class WebSearchAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's web search tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"web-search\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"web_search\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Search the web for real-time information\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters.\n\n        Args:\n            request (MCPRequest): MCP request\n\n        Returns:\n            dict: Dictionary of OpenAI parameters\n        \"\"\"\n        parameters = {}\n        if \"search_term\" in request.parameters:\n            parameters[\"query\"] = request.parameters[\"search_term\"]\n        elif \"parameters\" in request.parameters:\n            # \uacbd\uc6b0\uc5d0 \ub530\ub77c parameters \ud544\ub4dc \ub0b4\uc5d0 \uc9c1\uc811 \ucffc\ub9ac\uac00 \ubb38\uc790\uc5f4\ub85c \uc804\ub2ec\ub420 \uc218 \uc788\uc74c\n            parameters[\"query\"] = request.parameters[\"parameters\"]\n\n        # \ud5a5\uc0c1\ub41c \ub85c\uae45 \ucd94\uac00\n        logger.info(f\"[WEB SEARCH] Translated MCP request to tool parameters: {parameters}\")\n\n        return parameters\n\n    async def translate_response(self, response: Any) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI tool response to MCP response.\n\n        Args:\n            response (Any): OpenAI tool response (JSON string or dictionary)\n\n        Returns:\n            MCPResponse: MCP response\n        \"\"\"\n        # \ud5a5\uc0c1\ub41c \ub514\ubc84\uadf8 \ub85c\uae45 \ucd94\uac00\n        logger.debug(f\"[WEB SEARCH] Translating response of type: {type(response)}\")\n\n        # \uc751\ub2f5\uc774 \uc774\ubbf8 JSON \ubb38\uc790\uc5f4\uc778\uc9c0 \ud655\uc778\n        if isinstance(response, str):\n            try:\n                # \uc774\ubbf8 JSON \ubb38\uc790\uc5f4\uc778 \uacbd\uc6b0 \ud30c\uc2f1\n                logger.debug(\"[WEB SEARCH] Response is a string, attempting to parse as JSON\")\n                parsed_response = json.loads(response)\n\n                # \uc751\ub2f5 \ub0b4\uc6a9 \ud655\uc778 \ubc0f \ucd94\ucd9c\n                logger.debug(\n                    f\"[WEB SEARCH] Parsed JSON with keys: {list(parsed_response.keys()) if isinstance(parsed_response, dict) else 'not a dict'}\"\n                )\n\n                # \ucf58\ud150\uce20 \ucd94\ucd9c (\ubb38\uc790\uc5f4\ub85c \ubcc0\ud658)\n                if isinstance(parsed_response, dict) and \"content\" in parsed_response:\n                    content = str(parsed_response[\"content\"])\n                    logger.info(f\"[WEB SEARCH] Extracted content from JSON response, length: {len(content)}\")\n                    return MCPResponse(content=content, context={\"raw_response\": response})\n                else:\n                    # \ucf58\ud150\uce20 \ud0a4\uac00 \uc5c6\uc73c\uba74 \uc804\uccb4 \uc751\ub2f5\uc744 \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n                    content_str = json.dumps(parsed_response)\n                    logger.info(\n                        f\"[WEB SEARCH] No content key found, using full response as content, length: {len(content_str)}\"\n                    )\n                    return MCPResponse(content=content_str, context={\"raw_response\": response})\n\n            except json.JSONDecodeError:\n                # JSON \ud30c\uc2f1 \uc2e4\ud328 \uc2dc \uc6d0\ubcf8 \ubb38\uc790\uc5f4 \uc0ac\uc6a9\n                logger.warning(\"[WEB SEARCH] Failed to parse response as JSON, using raw string\")\n                return MCPResponse(content=response, context={\"raw_response\": response})\n\n        # \ub515\uc154\ub108\ub9ac\uc778 \uacbd\uc6b0\n        elif isinstance(response, dict):\n            logger.debug(f\"[WEB SEARCH] Response is a dict with keys: {list(response.keys())}\")\n\n            # \ucf58\ud150\uce20 \ud0a4\uac00 \uc788\uc73c\uba74 \ud574\ub2f9 \uac12 \uc0ac\uc6a9\n            if \"content\" in response:\n                # \ubc18\ub4dc\uc2dc content\ub97c \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n                content = str(response[\"content\"])\n                logger.info(f\"[WEB SEARCH] Extracted content from dict response, length: {len(content)}\")\n                return MCPResponse(content=content, context={\"raw_response\": json.dumps(response)})\n\n            # \ucf58\ud150\uce20 \ud0a4\uac00 \uc5c6\uc73c\uba74 \uc804\uccb4 \ub515\uc154\ub108\ub9ac\ub97c JSON \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n            content_str = json.dumps(response)\n            logger.info(\n                f\"[WEB SEARCH] No content key in dict, using full dict as JSON string, length: {len(content_str)}\"\n            )\n            return MCPResponse(content=content_str, context={\"raw_response\": content_str})\n\n        # \ub2e4\ub978 \ubaa8\ub4e0 \ud0c0\uc785\uc758 \uacbd\uc6b0\n        else:\n            # \uc548\uc804\ud558\uac8c \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n            logger.warning(f\"[WEB SEARCH] Unexpected response type: {type(response)}, converting to string\")\n            content_str = str(response)\n            return MCPResponse(content=content_str, context={\"raw_response\": content_str})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>MCP request</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/web_search.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters.\n\n    Args:\n        request (MCPRequest): MCP request\n\n    Returns:\n        dict: Dictionary of OpenAI parameters\n    \"\"\"\n    parameters = {}\n    if \"search_term\" in request.parameters:\n        parameters[\"query\"] = request.parameters[\"search_term\"]\n    elif \"parameters\" in request.parameters:\n        # \uacbd\uc6b0\uc5d0 \ub530\ub77c parameters \ud544\ub4dc \ub0b4\uc5d0 \uc9c1\uc811 \ucffc\ub9ac\uac00 \ubb38\uc790\uc5f4\ub85c \uc804\ub2ec\ub420 \uc218 \uc788\uc74c\n        parameters[\"query\"] = request.parameters[\"parameters\"]\n\n    # \ud5a5\uc0c1\ub41c \ub85c\uae45 \ucd94\uac00\n    logger.info(f\"[WEB SEARCH] Translated MCP request to tool parameters: {parameters}\")\n\n    return parameters\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI tool response to MCP response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>OpenAI tool response (JSON string or dictionary)</p> required <p>Returns:</p> Name Type Description <code>MCPResponse</code> <code>MCPResponse</code> <p>MCP response</p> Source code in <code>openai_tool2mcp/tools/web_search.py</code> <pre><code>async def translate_response(self, response: Any) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI tool response to MCP response.\n\n    Args:\n        response (Any): OpenAI tool response (JSON string or dictionary)\n\n    Returns:\n        MCPResponse: MCP response\n    \"\"\"\n    # \ud5a5\uc0c1\ub41c \ub514\ubc84\uadf8 \ub85c\uae45 \ucd94\uac00\n    logger.debug(f\"[WEB SEARCH] Translating response of type: {type(response)}\")\n\n    # \uc751\ub2f5\uc774 \uc774\ubbf8 JSON \ubb38\uc790\uc5f4\uc778\uc9c0 \ud655\uc778\n    if isinstance(response, str):\n        try:\n            # \uc774\ubbf8 JSON \ubb38\uc790\uc5f4\uc778 \uacbd\uc6b0 \ud30c\uc2f1\n            logger.debug(\"[WEB SEARCH] Response is a string, attempting to parse as JSON\")\n            parsed_response = json.loads(response)\n\n            # \uc751\ub2f5 \ub0b4\uc6a9 \ud655\uc778 \ubc0f \ucd94\ucd9c\n            logger.debug(\n                f\"[WEB SEARCH] Parsed JSON with keys: {list(parsed_response.keys()) if isinstance(parsed_response, dict) else 'not a dict'}\"\n            )\n\n            # \ucf58\ud150\uce20 \ucd94\ucd9c (\ubb38\uc790\uc5f4\ub85c \ubcc0\ud658)\n            if isinstance(parsed_response, dict) and \"content\" in parsed_response:\n                content = str(parsed_response[\"content\"])\n                logger.info(f\"[WEB SEARCH] Extracted content from JSON response, length: {len(content)}\")\n                return MCPResponse(content=content, context={\"raw_response\": response})\n            else:\n                # \ucf58\ud150\uce20 \ud0a4\uac00 \uc5c6\uc73c\uba74 \uc804\uccb4 \uc751\ub2f5\uc744 \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n                content_str = json.dumps(parsed_response)\n                logger.info(\n                    f\"[WEB SEARCH] No content key found, using full response as content, length: {len(content_str)}\"\n                )\n                return MCPResponse(content=content_str, context={\"raw_response\": response})\n\n        except json.JSONDecodeError:\n            # JSON \ud30c\uc2f1 \uc2e4\ud328 \uc2dc \uc6d0\ubcf8 \ubb38\uc790\uc5f4 \uc0ac\uc6a9\n            logger.warning(\"[WEB SEARCH] Failed to parse response as JSON, using raw string\")\n            return MCPResponse(content=response, context={\"raw_response\": response})\n\n    # \ub515\uc154\ub108\ub9ac\uc778 \uacbd\uc6b0\n    elif isinstance(response, dict):\n        logger.debug(f\"[WEB SEARCH] Response is a dict with keys: {list(response.keys())}\")\n\n        # \ucf58\ud150\uce20 \ud0a4\uac00 \uc788\uc73c\uba74 \ud574\ub2f9 \uac12 \uc0ac\uc6a9\n        if \"content\" in response:\n            # \ubc18\ub4dc\uc2dc content\ub97c \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n            content = str(response[\"content\"])\n            logger.info(f\"[WEB SEARCH] Extracted content from dict response, length: {len(content)}\")\n            return MCPResponse(content=content, context={\"raw_response\": json.dumps(response)})\n\n        # \ucf58\ud150\uce20 \ud0a4\uac00 \uc5c6\uc73c\uba74 \uc804\uccb4 \ub515\uc154\ub108\ub9ac\ub97c JSON \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n        content_str = json.dumps(response)\n        logger.info(\n            f\"[WEB SEARCH] No content key in dict, using full dict as JSON string, length: {len(content_str)}\"\n        )\n        return MCPResponse(content=content_str, context={\"raw_response\": content_str})\n\n    # \ub2e4\ub978 \ubaa8\ub4e0 \ud0c0\uc785\uc758 \uacbd\uc6b0\n    else:\n        # \uc548\uc804\ud558\uac8c \ubb38\uc790\uc5f4\ub85c \ubcc0\ud658\n        logger.warning(f\"[WEB SEARCH] Unexpected response type: {type(response)}, converting to string\")\n        content_str = str(response)\n        return MCPResponse(content=content_str, context={\"raw_response\": content_str})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter","title":"<code>CodeInterpreterAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's code interpreter tool</p> Source code in <code>openai_tool2mcp/tools/code_interpreter.py</code> <pre><code>class CodeInterpreterAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's code interpreter tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"code-execution\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"code_interpreter\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Execute code and return the result\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract code to execute\n        code = request.parameters.get(\"code\", \"\")\n        language = request.parameters.get(\"language\", \"python\")\n\n        logger.debug(f\"Translating code execution request with language: {language}\")\n\n        # Return OpenAI parameters\n        return {\"code\": code, \"language\": language}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract execution result\n        result = response.get(\"result\", {})\n\n        logger.debug(\"Translating code execution response\")\n\n        # Format result as markdown\n        content = format_code_result(result)\n\n        # Check for errors\n        error = None\n        if isinstance(result, dict) and \"error\" in result:\n            error = result[\"error\"]\n\n        # Return MCP response\n        return MCPResponse(content=content, error=error, context={\"language\": response.get(\"language\", \"python\")})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/code_interpreter.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract code to execute\n    code = request.parameters.get(\"code\", \"\")\n    language = request.parameters.get(\"language\", \"python\")\n\n    logger.debug(f\"Translating code execution request with language: {language}\")\n\n    # Return OpenAI parameters\n    return {\"code\": code, \"language\": language}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/code_interpreter.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract execution result\n    result = response.get(\"result\", {})\n\n    logger.debug(\"Translating code execution response\")\n\n    # Format result as markdown\n    content = format_code_result(result)\n\n    # Check for errors\n    error = None\n    if isinstance(result, dict) and \"error\" in result:\n        error = result[\"error\"]\n\n    # Return MCP response\n    return MCPResponse(content=content, error=error, context={\"language\": response.get(\"language\", \"python\")})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter","title":"<code>BrowserAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's web browser tool</p> Source code in <code>openai_tool2mcp/tools/browser.py</code> <pre><code>class BrowserAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's web browser tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"browser\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"web_browser\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Browse websites and interact with web content\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract URL and action\n        url = request.parameters.get(\"url\", \"\")\n        action = request.parameters.get(\"action\", \"browse\")\n\n        logger.debug(f\"Translating browser request for URL: {url}, action: {action}\")\n\n        # Return OpenAI parameters\n        return {\"url\": url, \"action\": action}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract content\n        content = response.get(\"content\", \"\")\n        title = response.get(\"title\", \"\")\n        url = response.get(\"url\", \"\")\n\n        logger.debug(f\"Translating browser response for URL: {url}\")\n\n        # Format content as markdown\n        formatted_content = f\"# {title}\\n\\n{content}\" if title else content\n\n        # Check for errors\n        error = response.get(\"error\")\n\n        # Return MCP response\n        return MCPResponse(content=formatted_content, error=error, context={\"url\": url, \"title\": title})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/browser.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract URL and action\n    url = request.parameters.get(\"url\", \"\")\n    action = request.parameters.get(\"action\", \"browse\")\n\n    logger.debug(f\"Translating browser request for URL: {url}, action: {action}\")\n\n    # Return OpenAI parameters\n    return {\"url\": url, \"action\": action}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/browser.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract content\n    content = response.get(\"content\", \"\")\n    title = response.get(\"title\", \"\")\n    url = response.get(\"url\", \"\")\n\n    logger.debug(f\"Translating browser response for URL: {url}\")\n\n    # Format content as markdown\n    formatted_content = f\"# {title}\\n\\n{content}\" if title else content\n\n    # Check for errors\n    error = response.get(\"error\")\n\n    # Return MCP response\n    return MCPResponse(content=formatted_content, error=error, context={\"url\": url, \"title\": title})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter","title":"<code>FileManagerAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's file management tool</p> Source code in <code>openai_tool2mcp/tools/file_manager.py</code> <pre><code>class FileManagerAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's file management tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"file-io\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"file_search\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Search and access file content\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract file operation parameters\n        operation = request.parameters.get(\"operation\", \"read\")\n        path = request.parameters.get(\"path\", \"\")\n        content = request.parameters.get(\"content\", \"\")\n\n        logger.debug(f\"Translating file request for operation: {operation}, path: {path}\")\n\n        # Return OpenAI parameters\n        return {\"operation\": operation, \"path\": path, \"content\": content}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract result\n        path = response.get(\"path\", \"\")\n        operation = response.get(\"operation\", \"read\")\n        content = response.get(\"content\", \"\")\n\n        logger.debug(f\"Translating file response for operation: {operation}, path: {path}\")\n\n        # Format content based on operation\n        if operation == \"read\":\n            formatted_content = f\"# File: {path}\\n\\n```\\n{content}\\n```\"\n        elif operation == \"write\":\n            formatted_content = f\"File written to {path}\"\n        elif operation == \"delete\":\n            formatted_content = f\"File deleted: {path}\"\n        elif operation == \"list\":\n            formatted_content = f\"# Directory: {path}\\n\\n\"\n            for item in content.split(\"\\n\"):\n                if item.strip():\n                    formatted_content += f\"- {item.strip()}\\n\"\n        else:\n            formatted_content = f\"Operation '{operation}' completed on {path}\"\n\n        # Check for errors\n        error = response.get(\"error\")\n\n        # Return MCP response\n        return MCPResponse(content=formatted_content, error=error, context={\"path\": path, \"operation\": operation})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/file_manager.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract file operation parameters\n    operation = request.parameters.get(\"operation\", \"read\")\n    path = request.parameters.get(\"path\", \"\")\n    content = request.parameters.get(\"content\", \"\")\n\n    logger.debug(f\"Translating file request for operation: {operation}, path: {path}\")\n\n    # Return OpenAI parameters\n    return {\"operation\": operation, \"path\": path, \"content\": content}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/file_manager.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract result\n    path = response.get(\"path\", \"\")\n    operation = response.get(\"operation\", \"read\")\n    content = response.get(\"content\", \"\")\n\n    logger.debug(f\"Translating file response for operation: {operation}, path: {path}\")\n\n    # Format content based on operation\n    if operation == \"read\":\n        formatted_content = f\"# File: {path}\\n\\n```\\n{content}\\n```\"\n    elif operation == \"write\":\n        formatted_content = f\"File written to {path}\"\n    elif operation == \"delete\":\n        formatted_content = f\"File deleted: {path}\"\n    elif operation == \"list\":\n        formatted_content = f\"# Directory: {path}\\n\\n\"\n        for item in content.split(\"\\n\"):\n            if item.strip():\n                formatted_content += f\"- {item.strip()}\\n\"\n    else:\n        formatted_content = f\"Operation '{operation}' completed on {path}\"\n\n    # Check for errors\n    error = response.get(\"error\")\n\n    # Return MCP response\n    return MCPResponse(content=formatted_content, error=error, context={\"path\": path, \"operation\": operation})\n</code></pre>"},{"location":"modules/#data-models","title":"Data Models","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.models.mcp.MCPRequest","title":"<code>MCPRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for MCP tool request</p> Source code in <code>openai_tool2mcp/models/mcp.py</code> <pre><code>class MCPRequest(BaseModel):\n    \"\"\"Model for MCP tool request\"\"\"\n\n    parameters: dict[str, Any] = Field(default_factory=dict)\n    context: dict[str, Any] | None = Field(default=None)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.mcp.MCPResponse","title":"<code>MCPResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for MCP tool response</p> Source code in <code>openai_tool2mcp/models/mcp.py</code> <pre><code>class MCPResponse(BaseModel):\n    \"\"\"Model for MCP tool response\"\"\"\n\n    content: str\n    error: str | None = None\n    context: dict[str, Any] | None = Field(default_factory=dict)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.openai.ToolOutput","title":"<code>ToolOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for OpenAI tool output</p> Source code in <code>openai_tool2mcp/models/openai.py</code> <pre><code>class ToolOutput(BaseModel):\n    \"\"\"Model for OpenAI tool output\"\"\"\n\n    output: str\n    error: str | None = None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.openai.ToolRequest","title":"<code>ToolRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for OpenAI tool request</p> Source code in <code>openai_tool2mcp/models/openai.py</code> <pre><code>class ToolRequest(BaseModel):\n    \"\"\"Model for OpenAI tool request\"\"\"\n\n    tool_type: str\n    parameters: dict[str, Any]\n    thread_id: str | None = None\n    instructions: str | None = None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.openai.ToolResponse","title":"<code>ToolResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for OpenAI tool response</p> Source code in <code>openai_tool2mcp/models/openai.py</code> <pre><code>class ToolResponse(BaseModel):\n    \"\"\"Model for OpenAI tool response\"\"\"\n\n    thread_id: str\n    tool_outputs: list[Any] = Field(default_factory=list)\n</code></pre>"},{"location":"modules/#utilities","title":"Utilities","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.utils.config.APIKeyMissingError","title":"<code>APIKeyMissingError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when the API key is missing</p> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>class APIKeyMissingError(ValueError):\n    \"\"\"Exception raised when the API key is missing\"\"\"\n\n    def __init__(self):\n        super().__init__(\"No API key\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.config.ServerConfig","title":"<code>ServerConfig</code>","text":"<p>Configuration class for the MCP server</p> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>class ServerConfig:\n    \"\"\"Configuration class for the MCP server\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: str | None = None,\n        tools: list[str] | None = None,\n        request_timeout: int = 30,\n        max_retries: int = 3,\n    ):\n        \"\"\"\n        Initialize server configuration.\n\n        Args:\n            openai_api_key: OpenAI API key (defaults to environment variable)\n            tools: List of enabled tools (defaults to all)\n            request_timeout: Timeout for API requests in seconds\n            max_retries: Maximum number of retries for failed requests\n        \"\"\"\n        # Load environment variables\n        load_dotenv()\n\n        self.openai_api_key = openai_api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self.openai_api_key:\n            raise APIKeyMissingError()\n\n        self.tools = tools or []  # Will default to all tools in the ToolRegistry\n        self.request_timeout = request_timeout\n        self.max_retries = max_retries\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.config.ServerConfig.__init__","title":"<code>__init__(openai_api_key=None, tools=None, request_timeout=30, max_retries=3)</code>","text":"<p>Initialize server configuration.</p> <p>Parameters:</p> Name Type Description Default <code>openai_api_key</code> <code>str | None</code> <p>OpenAI API key (defaults to environment variable)</p> <code>None</code> <code>tools</code> <code>list[str] | None</code> <p>List of enabled tools (defaults to all)</p> <code>None</code> <code>request_timeout</code> <code>int</code> <p>Timeout for API requests in seconds</p> <code>30</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for failed requests</p> <code>3</code> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>def __init__(\n    self,\n    openai_api_key: str | None = None,\n    tools: list[str] | None = None,\n    request_timeout: int = 30,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize server configuration.\n\n    Args:\n        openai_api_key: OpenAI API key (defaults to environment variable)\n        tools: List of enabled tools (defaults to all)\n        request_timeout: Timeout for API requests in seconds\n        max_retries: Maximum number of retries for failed requests\n    \"\"\"\n    # Load environment variables\n    load_dotenv()\n\n    self.openai_api_key = openai_api_key or os.environ.get(\"OPENAI_API_KEY\")\n    if not self.openai_api_key:\n        raise APIKeyMissingError()\n\n    self.tools = tools or []  # Will default to all tools in the ToolRegistry\n    self.request_timeout = request_timeout\n    self.max_retries = max_retries\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.config.load_config","title":"<code>load_config(config_file=None)</code>","text":"<p>Load configuration from file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to configuration file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Configuration dictionary</p> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>def load_config(config_file=None) -&gt; dict:\n    \"\"\"\n    Load configuration from file.\n\n    Args:\n        config_file (str, optional): Path to configuration file\n\n    Returns:\n        dict: Configuration dictionary\n    \"\"\"\n    # If a specific config file is provided, load it\n    if config_file and os.path.exists(config_file):\n        # For now, just return a simple config dictionary\n        # In a real implementation, parse the file\n        return {\"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\")}\n\n    # Default configuration\n    return {\"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\")}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.logging.setup_logging","title":"<code>setup_logging(level='info')</code>","text":"<p>Set up logging configuration.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (debug, info, warning, error, critical)</p> <code>'info'</code> Source code in <code>openai_tool2mcp/utils/logging.py</code> <pre><code>def setup_logging(level=\"info\"):\n    \"\"\"\n    Set up logging configuration.\n\n    Args:\n        level (str): Logging level (debug, info, warning, error, critical)\n    \"\"\"\n    level_map = {\n        \"debug\": logging.DEBUG,\n        \"info\": logging.INFO,\n        \"warning\": logging.WARNING,\n        \"error\": logging.ERROR,\n        \"critical\": logging.CRITICAL,\n    }\n\n    log_level = level_map.get(level.lower(), logging.INFO)\n\n    # Configure root logger\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        handlers=[logging.StreamHandler(sys.stderr)],\n    )\n\n    # Create logger for this package\n    logger = logging.getLogger(\"openai_tool2mcp\")\n    logger.setLevel(log_level)\n\n    return logger\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.security.sanitize_parameters","title":"<code>sanitize_parameters(parameters)</code>","text":"<p>Sanitize parameters to prevent injection attacks.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict</code> <p>Parameters to sanitize</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Sanitized parameters</p> Source code in <code>openai_tool2mcp/utils/security.py</code> <pre><code>def sanitize_parameters(parameters: dict) -&gt; dict:\n    \"\"\"\n    Sanitize parameters to prevent injection attacks.\n\n    Args:\n        parameters (dict): Parameters to sanitize\n\n    Returns:\n        dict: Sanitized parameters\n    \"\"\"\n    sanitized = {}\n\n    for key, value in parameters.items():\n        if isinstance(value, str):\n            # Basic sanitization for strings\n            sanitized[key] = value.replace(\"&lt;script&gt;\", \"\").replace(\"&lt;/script&gt;\", \"\")\n        elif isinstance(value, dict | list):\n            # Recursively sanitize nested structures\n            sanitized[key] = (\n                sanitize_parameters(value)\n                if isinstance(value, dict)\n                else [sanitize_parameters(item) if isinstance(item, dict) else item for item in value]\n            )\n        else:\n            # Keep other types as is\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.security.validate_api_key","title":"<code>validate_api_key(api_key)</code>","text":"<p>Validate OpenAI API key format.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the key format is valid</p> Source code in <code>openai_tool2mcp/utils/security.py</code> <pre><code>def validate_api_key(api_key: str) -&gt; bool:\n    \"\"\"\n    Validate OpenAI API key format.\n\n    Args:\n        api_key (str): API key to validate\n\n    Returns:\n        bool: True if the key format is valid\n    \"\"\"\n    if not api_key:\n        return False\n\n    # Check if key follows OpenAI format\n    # This is a simple format check, not a real validation\n    return bool(api_key.startswith((\"sk-\", \"org-\")))\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.security.validate_api_key_with_openai","title":"<code>validate_api_key_with_openai(api_key)</code>","text":"<p>Validate OpenAI API key by making a test request.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the key is valid</p> Source code in <code>openai_tool2mcp/utils/security.py</code> <pre><code>def validate_api_key_with_openai(api_key: str) -&gt; bool:\n    \"\"\"\n    Validate OpenAI API key by making a test request.\n\n    Args:\n        api_key (str): OpenAI API key\n\n    Returns:\n        bool: True if the key is valid\n    \"\"\"\n    if not api_key:\n        return False\n\n    try:\n        # Make a minimal request to check key validity\n        response = requests.get(\n            \"https://api.openai.com/v1/models\", headers={\"Authorization\": f\"Bearer {api_key}\"}, timeout=10\n        )\n\n        if response.status_code == 200:\n            logger.info(\"OpenAI API key is valid\")\n            return True\n        else:\n            logger.error(f\"OpenAI API key validation failed with status {response.status_code}\")\n            return False\n    except requests.exceptions.RequestException:\n        logger.error(\"OpenAI API key validation error: RequestException\")\n        return False\n</code></pre>"},{"location":"uv-integration/","title":"Using uv with openai-tool2mcp","text":"<p>The Model Context Protocol (MCP) recommends using <code>uv</code> as the package manager and execution environment for MCP servers. This guide explains how to integrate <code>openai-tool2mcp</code> with <code>uv</code> for optimal MCP compatibility.</p>"},{"location":"uv-integration/#why-use-uv","title":"Why use uv?","text":"<p><code>uv</code> is a fast, reliable Python package manager and execution environment that provides:</p> <ul> <li>Reproducible installations with exact dependency resolution</li> <li>Isolated execution environments for each project</li> <li>Improved compatibility with the Claude Desktop app and other MCP-compatible clients</li> <li>Simplified configuration for MCP servers</li> </ul>"},{"location":"uv-integration/#installation","title":"Installation","text":"<p>First, install <code>uv</code> if you haven't already:</p> <pre><code>pip install uv\n</code></pre> <p>Then, install <code>openai-tool2mcp</code> using <code>uv</code>:</p> <pre><code>uv pip install openai-tool2mcp\n</code></pre>"},{"location":"uv-integration/#running-the-server-with-uv","title":"Running the Server with uv","text":"<p>There are two main ways to run <code>openai-tool2mcp</code> with <code>uv</code>:</p>"},{"location":"uv-integration/#1-using-the-standalone-entry-script","title":"1. Using the Standalone Entry Script","text":"<p>The package provides a standalone entry script that can be run directly with <code>uv</code>:</p> <pre><code>uv run openai_tool2mcp/server_entry.py --transport stdio\n</code></pre> <p>This approach is recommended for MCP compatibility as it matches the execution model expected by the MCP protocol.</p>"},{"location":"uv-integration/#2-creating-your-own-script","title":"2. Creating Your Own Script","text":"<p>You can also create your own script and run it with <code>uv</code>:</p> <pre><code># my_openai_tools_server.py\nimport os\nfrom dotenv import load_dotenv\nfrom openai_tool2mcp import MCPServer, ServerConfig, OpenAIBuiltInTools\n\n# Load environment variables\nload_dotenv()\n\n# Create a server config with desired tools\nconfig = ServerConfig(\n    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    tools=[\n        OpenAIBuiltInTools.WEB_SEARCH.value,\n        OpenAIBuiltInTools.CODE_INTERPRETER.value,\n    ]\n)\n\n# Create and start the server\nserver = MCPServer(config)\nserver.start(transport=\"stdio\")\n</code></pre> <p>Run this script with:</p> <pre><code>uv run my_openai_tools_server.py\n</code></pre>"},{"location":"uv-integration/#configuring-claude-for-desktop","title":"Configuring Claude for Desktop","text":"<p>To configure Claude for Desktop to use <code>openai-tool2mcp</code> with <code>uv</code>, edit your <code>claude_desktop_config.json</code> file:</p> <pre><code>{\n  \"mcpServers\": {\n    \"openai-tools\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/absolute/path/to/your/openai-tool2mcp\",\n        \"run\",\n        \"openai_tool2mcp/server_entry.py\"\n      ]\n    }\n  }\n}\n</code></pre> <p>The config file is located at:</p> <ul> <li>MacOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%AppData%\\Claude\\claude_desktop_config.json</code></li> </ul>"},{"location":"uv-integration/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can pass additional arguments to the server by adding them to the <code>args</code> array:</p> <pre><code>{\n  \"mcpServers\": {\n    \"openai-tools\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/absolute/path/to/your/openai-tool2mcp\",\n        \"run\",\n        \"openai_tool2mcp/server_entry.py\",\n        \"--tools\",\n        \"web_search\",\n        \"code_interpreter\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"uv-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"uv-integration/#common-issues","title":"Common Issues","text":"<ol> <li>Path Not Found: Ensure you're using the absolute path to your openai-tool2mcp installation</li> <li>Missing Dependencies: Make sure all dependencies are installed with <code>uv pip install -e .</code></li> <li>API Key Issues: Verify that your OpenAI API key is properly set in the environment</li> </ol>"},{"location":"uv-integration/#debugging","title":"Debugging","text":"<p>To enable debugging output, add the <code>--log-level debug</code> argument:</p> <pre><code>{\n  \"mcpServers\": {\n    \"openai-tools\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/absolute/path/to/your/openai-tool2mcp\",\n        \"run\",\n        \"openai_tool2mcp/server_entry.py\",\n        \"--log-level\",\n        \"debug\"\n      ]\n    }\n  }\n}\n</code></pre>"}]}