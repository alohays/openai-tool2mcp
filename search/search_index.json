{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"openai-tool2mcp","text":"<p>openai-tool2mcp is a lightweight, open-source bridge that wraps OpenAI's powerful built-in tools as Model Context Protocol (MCP) servers. It enables you to use high-quality OpenAI tools like web search and code interpreter with Claude and other MCP-compatible models.</p>"},{"location":"#why-openai-tool2mcp","title":"Why openai-tool2mcp?","text":"<p>AI developers today face a challenging choice:</p> <ol> <li>OpenAI's ecosystem: Offers powerful built-in tools like web search and code interpreter, but ties you to a closed platform</li> <li>MCP ecosystem: Provides an open standard for interoperability, but lacks the advanced tools available in OpenAI</li> </ol> <p>openai-tool2mcp bridges this gap by letting you use OpenAI's mature, high-quality tools within the open MCP ecosystem, giving you the best of both worlds.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd0d Use OpenAI's robust web search in Claude App</li> <li>\ud83d\udcbb Access code interpreter functionality in any MCP-compatible LLM</li> <li>\ud83d\udd04 Seamless protocol translation between OpenAI and MCP</li> <li>\ud83d\udee0\ufe0f Simple API for easy integration</li> </ul>"},{"location":"#supported-tools","title":"Supported Tools","text":"OpenAI Tool MCP Equivalent Status Web Search Web Search \u2705 Implemented Code Interpreter Code Execution \u2705 Implemented Web Browser Browser \u2705 Implemented File Management File I/O \u2705 Implemented"},{"location":"#technical-architecture","title":"Technical Architecture","text":"<p>openai-tool2mcp works by:</p> <ol> <li>Exposing an MCP-compatible server interface</li> <li>Translating MCP requests to OpenAI Assistant API calls</li> <li>Converting OpenAI tool responses back to MCP format</li> <li>Delivering results to MCP clients like Claude App</li> </ol> <p>For detailed technical information, see the Architecture Overview and Implementation Guide.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install from PyPI\npip install openai-tool2mcp\n\n# Start the server with your OpenAI API key\nOPENAI_API_KEY=\"your-api-key\" openai-tool2mcp start\n\n# Connect Claude App to http://localhost:8000\n</code></pre> <p>Ready to get started? Check out our Getting Started Guide for detailed instructions.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides comprehensive documentation for the openai-tool2mcp API, including classes, methods, and configuration options.</p>"},{"location":"api-reference/#core-classes","title":"Core Classes","text":""},{"location":"api-reference/#mcpserver","title":"MCPServer","text":"<p>The main server class that implements the MCP protocol and manages tool execution.</p> <pre><code>class MCPServer:\n    def __init__(self, config=None):\n        \"\"\"\n        Initialize the MCP server.\n\n        Args:\n            config (ServerConfig, optional): Server configuration\n        \"\"\"\n        pass\n\n    def register_routes(self):\n        \"\"\"Register FastAPI routes for the MCP protocol\"\"\"\n        pass\n\n    def start(self, host=\"127.0.0.1\", port=8000):\n        \"\"\"\n        Start the MCP server.\n\n        Args:\n            host (str): Host address to bind to\n            port (int): Port to listen on\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#serverconfig","title":"ServerConfig","text":"<p>Configuration class for the MCP server.</p> <pre><code>class ServerConfig:\n    def __init__(\n        self,\n        openai_api_key=None,\n        tools=None,\n        request_timeout=30,\n        max_retries=3\n    ):\n        \"\"\"\n        Initialize server configuration.\n\n        Args:\n            openai_api_key (str, optional): OpenAI API key\n            tools (List[str], optional): List of enabled tools\n            request_timeout (int): Request timeout in seconds\n            max_retries (int): Maximum number of retries\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#openaiclient","title":"OpenAIClient","text":"<p>Client for interacting with the OpenAI API.</p> <pre><code>class OpenAIClient:\n    def __init__(self, api_key):\n        \"\"\"\n        Initialize the OpenAI client.\n\n        Args:\n            api_key (str): OpenAI API key\n        \"\"\"\n        pass\n\n    async def invoke_tool(self, request):\n        \"\"\"\n        Invoke an OpenAI tool.\n\n        Args:\n            request (OpenAIToolRequest): Tool request\n\n        Returns:\n            OpenAIToolResponse: Tool response\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#toolregistry","title":"ToolRegistry","text":"<p>Registry of available tools and their configurations.</p> <pre><code>class ToolRegistry:\n    def __init__(self, enabled_tools=None):\n        \"\"\"\n        Initialize the tool registry.\n\n        Args:\n            enabled_tools (List[str], optional): List of enabled tools\n        \"\"\"\n        pass\n\n    def has_tool(self, tool_id):\n        \"\"\"\n        Check if a tool is registered and enabled.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool is available\n        \"\"\"\n        pass\n\n    def get_openai_tool_type(self, tool_id):\n        \"\"\"\n        Get the OpenAI tool type for a given MCP tool ID.\n\n        Args:\n            tool_id (str): MCP tool ID\n\n        Returns:\n            str: OpenAI tool type\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#mcp-protocol-models","title":"MCP Protocol Models","text":""},{"location":"api-reference/#mcprequest","title":"MCPRequest","text":"<p>Model for MCP tool requests.</p> <pre><code>class MCPRequest(BaseModel):\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n    context: Optional[Dict[str, Any]] = Field(default=None)\n</code></pre> <p>Fields:</p> <ul> <li><code>parameters</code>: Dictionary of tool parameters</li> <li><code>context</code>: Dictionary of context information</li> </ul>"},{"location":"api-reference/#mcpresponse","title":"MCPResponse","text":"<p>Model for MCP tool responses.</p> <pre><code>class MCPResponse(BaseModel):\n    content: str\n    error: Optional[str] = None\n    context: Optional[Dict[str, Any]] = Field(default_factory=dict)\n</code></pre> <p>Fields:</p> <ul> <li><code>content</code>: Response content</li> <li><code>error</code>: Optional error message</li> <li><code>context</code>: Dictionary of context information</li> </ul>"},{"location":"api-reference/#openai-api-models","title":"OpenAI API Models","text":""},{"location":"api-reference/#openaitoolrequest","title":"OpenAIToolRequest","text":"<p>Model for OpenAI tool requests.</p> <pre><code>class OpenAIToolRequest(BaseModel):\n    tool_type: str\n    parameters: Dict[str, Any]\n    thread_id: Optional[str] = None\n    instructions: Optional[str] = None\n</code></pre> <p>Fields:</p> <ul> <li><code>tool_type</code>: OpenAI tool type</li> <li><code>parameters</code>: Dictionary of tool parameters</li> <li><code>thread_id</code>: Optional thread ID for continued conversations</li> <li><code>instructions</code>: Optional instructions for the assistant</li> </ul>"},{"location":"api-reference/#openaitoolresponse","title":"OpenAIToolResponse","text":"<p>Model for OpenAI tool responses.</p> <pre><code>class OpenAIToolResponse(BaseModel):\n    thread_id: str\n    tool_outputs: List[Any]\n</code></pre> <p>Fields:</p> <ul> <li><code>thread_id</code>: Thread ID for the conversation</li> <li><code>tool_outputs</code>: List of tool outputs</li> </ul>"},{"location":"api-reference/#built-in-tools","title":"Built-in Tools","text":""},{"location":"api-reference/#openaibuiltintools","title":"OpenAIBuiltInTools","text":"<p>Enum of built-in OpenAI tools.</p> <pre><code>class OpenAIBuiltInTools(Enum):\n    WEB_SEARCH = \"retrieval\"\n    CODE_INTERPRETER = \"code_interpreter\"\n    WEB_BROWSER = \"web_browser\"\n    FILE_SEARCH = \"file_search\"\n</code></pre>"},{"location":"api-reference/#tool-adapters","title":"Tool Adapters","text":""},{"location":"api-reference/#tooladapter","title":"ToolAdapter","text":"<p>Abstract base class for tool adapters.</p> <pre><code>class ToolAdapter(ABC):\n    @property\n    @abstractmethod\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"Translate MCP request to OpenAI parameters\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"Translate OpenAI response to MCP response\"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#http-api-endpoints","title":"HTTP API Endpoints","text":"<p>The MCP server exposes the following HTTP endpoints:</p>"},{"location":"api-reference/#tool-invocation","title":"Tool Invocation","text":"<p>Endpoint: <code>POST /v1/tools/{tool_id}/invoke</code></p> <p>Invokes a tool with the specified ID.</p> <p>Path Parameters:</p> <ul> <li><code>tool_id</code> (string): ID of the tool to invoke</li> </ul> <p>Request Body:</p> <pre><code>{\n  \"parameters\": {\n    \"param1\": \"value1\",\n    \"param2\": \"value2\"\n  },\n  \"context\": {\n    \"thread_id\": \"optional-thread-id\",\n    \"instructions\": \"optional-instructions\"\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"content\": \"Tool response content\",\n  \"error\": null,\n  \"context\": {\n    \"thread_id\": \"thread-id\"\n  }\n}\n</code></pre> <p>Status Codes:</p> <ul> <li><code>200 OK</code>: Tool executed successfully</li> <li><code>404 Not Found</code>: Tool not found</li> <li><code>400 Bad Request</code>: Invalid request</li> <li><code>500 Internal Server Error</code>: Server error</li> </ul>"},{"location":"api-reference/#translation-functions","title":"Translation Functions","text":""},{"location":"api-reference/#mcp-to-openai-translation","title":"MCP to OpenAI Translation","text":"<pre><code>def translate_request(mcp_request: MCPRequest, tool_id: str) -&gt; OpenAIToolRequest:\n    \"\"\"\n    Translate an MCP request to an OpenAI request format.\n\n    Args:\n        mcp_request: The MCP request to translate\n        tool_id: The ID of the tool to invoke\n\n    Returns:\n        An OpenAI tool request object\n    \"\"\"\n    pass\n\ndef map_tool_id_to_openai_type(tool_id: str) -&gt; str:\n    \"\"\"\n    Map MCP tool IDs to OpenAI tool types.\n\n    Args:\n        tool_id: MCP tool ID\n\n    Returns:\n        OpenAI tool type\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#openai-to-mcp-translation","title":"OpenAI to MCP Translation","text":"<pre><code>def translate_response(openai_response: OpenAIToolResponse) -&gt; MCPResponse:\n    \"\"\"\n    Translate an OpenAI response to an MCP response format.\n\n    Args:\n        openai_response: The OpenAI response to translate\n\n    Returns:\n        An MCP response object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/#mcperror","title":"MCPError","text":"<p>Base class for all MCP errors.</p> <pre><code>class MCPError(Exception):\n    def __init__(self, message, status_code=500):\n        \"\"\"\n        Initialize MCP error.\n\n        Args:\n            message (str): Error message\n            status_code (int): HTTP status code\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#toolnotfounderror","title":"ToolNotFoundError","text":"<p>Error raised when a requested tool is not found.</p> <pre><code>class ToolNotFoundError(MCPError):\n    def __init__(self, tool_id):\n        \"\"\"\n        Initialize tool not found error.\n\n        Args:\n            tool_id (str): Tool ID\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#openaierror","title":"OpenAIError","text":"<p>Error raised when there's an issue with the OpenAI API.</p> <pre><code>class OpenAIError(MCPError):\n    def __init__(self, message, status_code=500):\n        \"\"\"\n        Initialize OpenAI error.\n\n        Args:\n            message (str): Error message\n            status_code (int): HTTP status code\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#configurationerror","title":"ConfigurationError","text":"<p>Error raised when there's an issue with configuration.</p> <pre><code>class ConfigurationError(MCPError):\n    def __init__(self, message):\n        \"\"\"\n        Initialize configuration error.\n\n        Args:\n            message (str): Error message\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/#command-line-interface","title":"Command-Line Interface","text":""},{"location":"api-reference/#main-function","title":"Main Function","text":"<pre><code>def main():\n    \"\"\"Main function for CLI\"\"\"\n    pass\n</code></pre> <p>Command-Line Arguments:</p> <ul> <li><code>--host</code>: Host address to bind to (default: 127.0.0.1)</li> <li><code>--port</code>: Port to listen on (default: 8000)</li> <li><code>--api-key</code>: OpenAI API key (defaults to OPENAI_API_KEY env var)</li> <li><code>--tools</code>: List of enabled tools (defaults to all)</li> <li><code>--timeout</code>: Request timeout in seconds (default: 30)</li> <li><code>--retries</code>: Maximum number of retries for failed requests (default: 3)</li> <li><code>--log-level</code>: Logging level (default: info)</li> </ul>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#configuration-management","title":"Configuration Management","text":"<pre><code>def load_config(config_file=None):\n    \"\"\"\n    Load configuration from file.\n\n    Args:\n        config_file (str, optional): Path to configuration file\n\n    Returns:\n        dict: Configuration dictionary\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#logging-utilities","title":"Logging Utilities","text":"<pre><code>def setup_logging(level=\"info\"):\n    \"\"\"\n    Set up logging.\n\n    Args:\n        level (str): Logging level\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#security-utilities","title":"Security Utilities","text":"<pre><code>def validate_api_key(api_key):\n    \"\"\"\n    Validate OpenAI API key.\n\n    Args:\n        api_key (str): API key to validate\n\n    Returns:\n        bool: True if valid\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/#examples","title":"Examples","text":""},{"location":"api-reference/#basic-server-example","title":"Basic Server Example","text":"<pre><code>from openai_tool2mcp import MCPServer, ServerConfig\n\n# Create server with default configuration\nserver = MCPServer()\nserver.start()\n</code></pre>"},{"location":"api-reference/#custom-configuration-example","title":"Custom Configuration Example","text":"<pre><code>from openai_tool2mcp import MCPServer, ServerConfig\nfrom openai_tool2mcp.tools import OpenAIBuiltInTools\n\n# Create server with custom configuration\nconfig = ServerConfig(\n    openai_api_key=\"your-api-key\",\n    tools=[\n        OpenAIBuiltInTools.WEB_SEARCH.value,\n        OpenAIBuiltInTools.CODE_INTERPRETER.value\n    ],\n    request_timeout=60,\n    max_retries=5\n)\n\nserver = MCPServer(config)\nserver.start(host=\"127.0.0.1\", port=8888)\n</code></pre>"},{"location":"api-reference/#custom-tool-adapter-example","title":"Custom Tool Adapter Example","text":"<pre><code>from openai_tool2mcp.models.mcp import MCPRequest, MCPResponse\nfrom openai_tool2mcp.tools import ToolAdapter\n\nclass CustomToolAdapter(ToolAdapter):\n    @property\n    def tool_id(self) -&gt; str:\n        return \"custom-tool\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        return \"retrieval\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        # Custom request translation logic\n        return {\"query\": request.parameters.get(\"query\", \"\")}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        # Custom response translation logic\n        return MCPResponse(\n            content=\"Custom response\",\n            context={\"custom_context\": \"value\"}\n        )\n</code></pre> <p>This API reference provides a comprehensive overview of the openai-tool2mcp library's classes, methods, and functionalities. For more detailed examples and guides, refer to the Implementation Guide and Getting Started documentation.</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>This document provides a technical overview of how openai-tool2mcp bridges the OpenAI Assistant API with Model Context Protocol (MCP) servers.</p>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<p>openai-tool2mcp is designed as a protocol translation layer that sits between MCP-compatible clients and the OpenAI API:</p> <pre><code>sequenceDiagram\n    participant Client as \"MCP Client&lt;br&gt;(e.g., Claude App)\"\n    participant Server as \"openai-tool2mcp Server\"\n    participant OAIAPI as \"OpenAI Assistant API\"\n\n    Client-&gt;&gt;Server: MCP Tool Request\n    note over Server: Protocol Translation\n    Server-&gt;&gt;OAIAPI: OpenAI API Request\n    OAIAPI-&gt;&gt;Server: OpenAI Tool Response\n    note over Server: Response Translation\n    Server-&gt;&gt;Client: MCP Tool Response\n</code></pre>"},{"location":"architecture/#protocol-translation","title":"Protocol Translation","text":""},{"location":"architecture/#mcp-to-openai-translation","title":"MCP to OpenAI Translation","text":"<p>MCP requests follow the Model Context Protocol specification, which defines a standard for tool usage. openai-tool2mcp maps these requests to OpenAI's Assistant API format:</p> MCP Component OpenAI Equivalent Tool ID Tool type Tool parameters Tool parameters Tool context Thread context Instructions System prompt"},{"location":"architecture/#openai-to-mcp-translation","title":"OpenAI to MCP Translation","text":"<p>Responses from OpenAI tools are translated back to MCP format:</p> OpenAI Component MCP Equivalent Tool output Tool response content Error information Error messages Metadata Tool context updates"},{"location":"architecture/#architectural-components","title":"Architectural Components","text":"<p>The system consists of the following main components:</p>"},{"location":"architecture/#1-mcp-server-interface","title":"1. MCP Server Interface","text":"<ul> <li>Implements the MCP protocol specification</li> <li>Provides HTTP endpoints for tool invocation</li> <li>Manages MCP contexts and sessions</li> </ul>"},{"location":"architecture/#2-protocol-translator","title":"2. Protocol Translator","text":"<ul> <li>Converts between MCP and OpenAI formats</li> <li>Maps tool identifiers and parameters</li> <li>Handles different serialization formats</li> </ul>"},{"location":"architecture/#3-openai-client","title":"3. OpenAI Client","text":"<ul> <li>Manages connections to OpenAI API</li> <li>Handles authentication and API key management</li> <li>Implements rate limiting and error handling</li> </ul>"},{"location":"architecture/#4-tool-registry","title":"4. Tool Registry","text":"<ul> <li>Maintains mappings between OpenAI tools and MCP tools</li> <li>Provides configuration for tool-specific parameters</li> <li>Handles tool capability discovery</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<ol> <li> <p>Request Phase:</p> </li> <li> <p>MCP client sends a tool request to the server</p> </li> <li>Server validates the MCP request format</li> <li>Protocol translator converts to OpenAI format</li> <li> <p>Request is sent to OpenAI API</p> </li> <li> <p>Processing Phase:</p> </li> <li> <p>OpenAI processes the tool request</p> </li> <li>Tool executes the requested operation</li> <li> <p>Results are returned to OpenAI</p> </li> <li> <p>Response Phase:</p> </li> <li>OpenAI API returns tool output</li> <li>Protocol translator converts to MCP format</li> <li>MCP-formatted response is sent to client</li> </ol>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":"<ul> <li>API Key Management: OpenAI API keys are securely stored and never exposed to clients</li> <li>Request Validation: All incoming requests are validated before processing</li> <li>Rate Limiting: Implements rate limiting to prevent API abuse</li> <li>Error Handling: Robust error handling prevents information leakage</li> </ul>"},{"location":"architecture/#tool-specific-implementations","title":"Tool-Specific Implementations","text":""},{"location":"architecture/#web-search-tool","title":"Web Search Tool","text":"<p>The Web Search tool maps MCP search requests to OpenAI's built-in web search capability:</p> <pre><code>flowchart TD\n    A[MCP Search Request] --&gt; B{Protocol Translator}\n    B --&gt; C[OpenAI Web Search]\n    C --&gt; D[Search Results]\n    D --&gt; B\n    B --&gt; E[MCP Search Response]\n</code></pre>"},{"location":"architecture/#code-interpreter-tool","title":"Code Interpreter Tool","text":"<p>The Code Interpreter tool maps MCP code execution requests to OpenAI's code interpreter:</p> <pre><code>flowchart TD\n    A[MCP Code Execution Request] --&gt; B{Protocol Translator}\n    B --&gt; C[OpenAI Code Interpreter]\n    C --&gt; D[Execution Results]\n    D --&gt; B\n    B --&gt; E[MCP Code Execution Response]\n</code></pre>"},{"location":"architecture/#state-management","title":"State Management","text":"<p>MCP and OpenAI have different approaches to maintaining state:</p> <ul> <li>MCP: Uses explicit contexts that are passed with each request</li> <li>OpenAI: Uses threads and assistant sessions</li> </ul> <p>openai-tool2mcp handles this difference by:</p> <ol> <li>Creating OpenAI threads for each MCP session</li> <li>Persisting thread IDs and mapping them to MCP contexts</li> <li>Maintaining a session store for consistent tool state</li> </ol>"},{"location":"architecture/#extension-points","title":"Extension Points","text":"<p>The architecture is designed for extensibility:</p> <ul> <li>Tool Adapters: New tool adapters can be added to support additional OpenAI tools</li> <li>Custom Mapping Rules: Rules for mapping between protocols can be customized</li> <li>Middleware Support: Processing pipelines can be extended with custom middleware</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you quickly set up and start using openai-tool2mcp to bring OpenAI's powerful built-in tools to your MCP-compatible models.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>Python 3.9 or higher</li> <li>An OpenAI API key with access to the Assistant API</li> <li>A MCP-compatible client (like Claude App)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install openai-tool2mcp\n</code></pre>"},{"location":"getting-started/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/alohays/openai-tool2mcp.git\ncd openai-tool2mcp\npip install -e .\n</code></pre>"},{"location":"getting-started/#basic-setup","title":"Basic Setup","text":""},{"location":"getting-started/#1-set-your-openai-api-key","title":"1. Set Your OpenAI API Key","text":"<p>You can set your API key in one of two ways:</p> <p>Option 1: Environment Variable</p> <pre><code># Linux/macOS\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# Windows (Command Prompt)\nset OPENAI_API_KEY=your-api-key-here\n\n# Windows (PowerShell)\n$env:OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Option 2: Configuration File</p> <p>Create a file named <code>.env</code> in your project directory:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"getting-started/#2-start-the-mcp-server","title":"2. Start the MCP Server","text":"<p>The simplest way to start the server is using the command-line interface:</p> <pre><code># Start with all tools enabled\nopenai-tool2mcp start\n\n# Start with specific tools\nopenai-tool2mcp start --tools retrieval code_interpreter\n</code></pre> <p>The server will start on <code>http://localhost:8000</code> by default.</p>"},{"location":"getting-started/#3-connect-your-mcp-client","title":"3. Connect Your MCP Client","text":"<p>Configure your MCP-compatible client to connect to your local server:</p> <ul> <li>Server URL: <code>http://localhost:8000</code></li> </ul>"},{"location":"getting-started/#using-with-claude-app","title":"Using with Claude App","text":"<p>Claude App supports the Model Context Protocol, making it a perfect client for openai-tool2mcp.</p>"},{"location":"getting-started/#setting-up-claude-app","title":"Setting Up Claude App","text":"<ol> <li>Open Claude App</li> <li>Go to Settings &gt; API &amp; Integrations</li> <li>Add a new MCP server with the URL <code>http://localhost:8000</code></li> <li>Save your settings</li> </ol>"},{"location":"getting-started/#available-tools-in-claude","title":"Available Tools in Claude","text":"<p>Once configured, you'll see new tools available in Claude:</p> <ul> <li>Web Search: Access OpenAI's powerful web search capability</li> <li>Code Execution: Run code using OpenAI's code interpreter</li> <li>Web Browser: Browse the web using OpenAI's web browser</li> <li>File Management: Manage files using OpenAI's file tools</li> </ul>"},{"location":"getting-started/#example-usage-in-claude","title":"Example Usage in Claude","text":"<p>Here's how to use the tools in Claude:</p> <pre><code>Claude, can you search the web for the latest news about AI regulations?\n</code></pre> <p>Claude will use the OpenAI web search tool through your local MCP server to fetch the latest news.</p>"},{"location":"getting-started/#programmatic-usage","title":"Programmatic Usage","text":"<p>You can also use openai-tool2mcp programmatically in your Python applications:</p> <pre><code>from openai_tool2mcp import MCPServer, ServerConfig\nfrom openai_tool2mcp.tools import OpenAIBuiltInTools\n\n# Configure the server\nconfig = ServerConfig(\n    openai_api_key=\"your-api-key-here\",  # Optional if set in environment\n    tools=[\n        OpenAIBuiltInTools.WEB_SEARCH.value,\n        OpenAIBuiltInTools.CODE_INTERPRETER.value\n    ]\n)\n\n# Create and start the server\nserver = MCPServer(config)\nserver.start(host=\"127.0.0.1\", port=8000)\n</code></pre>"},{"location":"getting-started/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/#server-configuration-options","title":"Server Configuration Options","text":"<p>You can customize your server with these options:</p> <pre><code>config = ServerConfig(\n    openai_api_key=\"your-api-key-here\",\n    tools=[\"retrieval\", \"code_interpreter\"],  # Enable specific tools\n    request_timeout=60,                       # Timeout in seconds\n    max_retries=5                            # Max retries for failed requests\n)\n</code></pre>"},{"location":"getting-started/#command-line-options","title":"Command-Line Options","text":"<p>The CLI provides several configuration options:</p> <pre><code>openai-tool2mcp start --help\n</code></pre> <p>Available options:</p> <ul> <li><code>--host</code>: Host address to bind to (default: 127.0.0.1)</li> <li><code>--port</code>: Port to listen on (default: 8000)</li> <li><code>--api-key</code>: OpenAI API key (alternative to environment variable)</li> <li><code>--tools</code>: Space-separated list of tools to enable</li> <li><code>--timeout</code>: Request timeout in seconds</li> <li><code>--retries</code>: Maximum number of retries for failed requests</li> </ul>"},{"location":"getting-started/#docker-deployment","title":"Docker Deployment","text":"<p>You can also run openai-tool2mcp in Docker:</p> <pre><code># Build the Docker image\ndocker build -t openai-tool2mcp .\n\n# Run the container\ndocker run -p 8000:8000 -e OPENAI_API_KEY=\"your-api-key-here\" openai-tool2mcp\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/#1-server-wont-start","title":"1. Server Won't Start","text":"<p>Make sure:</p> <ul> <li>Your OpenAI API key is valid</li> <li>You have proper network permissions</li> <li>The port (default 8000) is not already in use</li> </ul>"},{"location":"getting-started/#2-tool-calls-fail","title":"2. Tool Calls Fail","text":"<p>Check:</p> <ul> <li>Your OpenAI account has access to the Assistant API</li> <li>Your API key has the necessary permissions</li> <li>You have sufficient API credits/quota</li> </ul>"},{"location":"getting-started/#3-connection-issues","title":"3. Connection Issues","text":"<p>Verify:</p> <ul> <li>The server is running (you should see log messages)</li> <li>Your client is correctly configured with the server URL</li> <li>Your network allows the connection</li> </ul>"},{"location":"getting-started/#logs","title":"Logs","text":"<p>For more detailed troubleshooting, enable debug logs:</p> <pre><code>openai-tool2mcp start --log-level debug\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have openai-tool2mcp up and running, consider:</p> <ul> <li>Reading the Architecture Overview to understand how it works</li> <li>Exploring the Implementation Guide for technical details</li> <li>Checking the API Reference for complete documentation</li> </ul> <p>For any issues or contributions, visit our GitHub repository.</p>"},{"location":"implementation/","title":"Implementation Guide","text":"<p>This guide provides detailed technical information on implementing the openai-tool2mcp bridge, focusing on code structure, key components, and protocol details.</p>"},{"location":"implementation/#project-structure","title":"Project Structure","text":"<p>The project follows a modular architecture with the following structure:</p> <pre><code>openai_tool2mcp/\n\u251c\u2500\u2500 __init__.py             # Package exports\n\u251c\u2500\u2500 server.py               # MCP server implementation\n\u251c\u2500\u2500 translator/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 mcp_to_openai.py    # MCP to OpenAI translation\n\u2502   \u2514\u2500\u2500 openai_to_mcp.py    # OpenAI to MCP translation\n\u251c\u2500\u2500 openai_client/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 client.py           # OpenAI API client\n\u2502   \u2514\u2500\u2500 assistants.py       # Assistants API wrapper\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 registry.py         # Tool registry\n\u2502   \u251c\u2500\u2500 web_search.py       # Web search implementation\n\u2502   \u251c\u2500\u2500 code_interpreter.py # Code interpreter implementation\n\u2502   \u251c\u2500\u2500 browser.py          # Web browser implementation\n\u2502   \u2514\u2500\u2500 file_manager.py     # File management implementation\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 mcp.py              # MCP protocol models\n\u2502   \u2514\u2500\u2500 openai.py           # OpenAI API models\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py           # Configuration management\n    \u251c\u2500\u2500 logging.py          # Logging utilities\n    \u2514\u2500\u2500 security.py         # Security utilities\n</code></pre>"},{"location":"implementation/#core-components","title":"Core Components","text":""},{"location":"implementation/#1-mcp-server-implementation","title":"1. MCP Server Implementation","text":"<p>The MCP server is implemented using FastAPI for high performance and compatibility with async operations:</p> <pre><code>from fastapi import FastAPI, HTTPException, Depends\nfrom .models.mcp import MCPRequest, MCPResponse\nfrom .translator import mcp_to_openai, openai_to_mcp\nfrom .openai_client import OpenAIClient\nfrom .tools import ToolRegistry\n\nclass MCPServer:\n    def __init__(self, config=None):\n        self.app = FastAPI()\n        self.config = config or ServerConfig()\n        self.openai_client = OpenAIClient(self.config.openai_api_key)\n        self.tool_registry = ToolRegistry(self.config.tools)\n\n        # Register routes\n        self.register_routes()\n\n    def register_routes(self):\n        @self.app.post(\"/v1/tools/{tool_id}/invoke\")\n        async def invoke_tool(tool_id: str, request: MCPRequest):\n            # Validate tool exists\n            if not self.tool_registry.has_tool(tool_id):\n                raise HTTPException(status_code=404, detail=f\"Tool {tool_id} not found\")\n\n            # Translate MCP request to OpenAI format\n            openai_request = mcp_to_openai.translate_request(request, tool_id)\n\n            # Call OpenAI API\n            openai_response = await self.openai_client.invoke_tool(openai_request)\n\n            # Translate OpenAI response to MCP format\n            mcp_response = openai_to_mcp.translate_response(openai_response)\n\n            return mcp_response\n\n    def start(self, host=\"127.0.0.1\", port=8000):\n        import uvicorn\n        uvicorn.run(self.app, host=host, port=port)\n</code></pre>"},{"location":"implementation/#2-protocol-translation","title":"2. Protocol Translation","text":"<p>The translation layer handles the mapping between MCP and OpenAI formats:</p>"},{"location":"implementation/#mcp-to-openai-translation","title":"MCP to OpenAI Translation","text":"<pre><code>from ..models.mcp import MCPRequest\nfrom ..models.openai import OpenAIToolRequest\n\ndef translate_request(mcp_request: MCPRequest, tool_id: str) -&gt; OpenAIToolRequest:\n    \"\"\"\n    Translate an MCP request to an OpenAI request format.\n\n    Args:\n        mcp_request: The MCP request to translate\n        tool_id: The ID of the tool to invoke\n\n    Returns:\n        An OpenAI tool request object\n    \"\"\"\n    # Extract tool parameters\n    parameters = mcp_request.parameters\n\n    # Extract context information\n    context = mcp_request.context or {}\n\n    # Determine if this is a new or existing conversation\n    thread_id = context.get(\"thread_id\")\n\n    # Create OpenAI request\n    openai_request = OpenAIToolRequest(\n        tool_type=map_tool_id_to_openai_type(tool_id),\n        parameters=parameters,\n        thread_id=thread_id,\n        instructions=context.get(\"instructions\", \"\")\n    )\n\n    return openai_request\n\ndef map_tool_id_to_openai_type(tool_id: str) -&gt; str:\n    \"\"\"Map MCP tool IDs to OpenAI tool types\"\"\"\n    mapping = {\n        \"web-search\": \"retrieval\",\n        \"code-execution\": \"code_interpreter\",\n        \"browser\": \"web_browser\",\n        \"file-io\": \"file_search\"\n    }\n    return mapping.get(tool_id, tool_id)\n</code></pre>"},{"location":"implementation/#openai-to-mcp-translation","title":"OpenAI to MCP Translation","text":"<pre><code>from ..models.mcp import MCPResponse\nfrom ..models.openai import OpenAIToolResponse\n\ndef translate_response(openai_response: OpenAIToolResponse) -&gt; MCPResponse:\n    \"\"\"\n    Translate an OpenAI response to an MCP response format.\n\n    Args:\n        openai_response: The OpenAI response to translate\n\n    Returns:\n        An MCP response object\n    \"\"\"\n    # Extract tool output\n    tool_output = openai_response.tool_outputs[0] if openai_response.tool_outputs else None\n\n    if not tool_output:\n        return MCPResponse(\n            content=\"No result\",\n            error=\"Tool returned no output\",\n            context={\"thread_id\": openai_response.thread_id}\n        )\n\n    # Create MCP response\n    mcp_response = MCPResponse(\n        content=tool_output.output,\n        error=tool_output.error if hasattr(tool_output, \"error\") else None,\n        context={\"thread_id\": openai_response.thread_id}\n    )\n\n    return mcp_response\n</code></pre>"},{"location":"implementation/#3-openai-client","title":"3. OpenAI Client","text":"<p>The OpenAI client manages interactions with the OpenAI API:</p> <pre><code>import openai\nfrom ..models.openai import OpenAIToolRequest, OpenAIToolResponse\n\nclass OpenAIClient:\n    def __init__(self, api_key):\n        self.client = openai.Client(api_key=api_key)\n\n    async def invoke_tool(self, request: OpenAIToolRequest) -&gt; OpenAIToolResponse:\n        \"\"\"\n        Invoke an OpenAI tool.\n\n        Args:\n            request: The tool request\n\n        Returns:\n            The tool response\n        \"\"\"\n        # Create or get thread\n        thread_id = request.thread_id\n        if not thread_id:\n            thread = await self.client.beta.threads.create()\n            thread_id = thread.id\n\n        # Create message with tool call\n        message = await self.client.beta.threads.messages.create(\n            thread_id=thread_id,\n            role=\"user\",\n            content=f\"Please use the {request.tool_type} tool with these parameters: {request.parameters}\",\n        )\n\n        # Create assistant with the appropriate tool\n        assistant = await self.client.beta.assistants.create(\n            name=\"Tool Executor\",\n            instructions=request.instructions or \"Execute the requested tool function.\",\n            tools=[{\"type\": request.tool_type}],\n            model=\"gpt-4-turbo\"\n        )\n\n        # Run the assistant\n        run = await self.client.beta.threads.runs.create(\n            thread_id=thread_id,\n            assistant_id=assistant.id\n        )\n\n        # Wait for completion\n        run = await self._wait_for_run(thread_id, run.id)\n\n        # Get tool outputs\n        tool_outputs = run.required_action.submit_tool_outputs.tool_calls if hasattr(run, \"required_action\") else []\n\n        # Create response\n        response = OpenAIToolResponse(\n            thread_id=thread_id,\n            tool_outputs=tool_outputs\n        )\n\n        return response\n\n    async def _wait_for_run(self, thread_id, run_id):\n        \"\"\"Wait for a run to complete\"\"\"\n        import time\n\n        while True:\n            run = await self.client.beta.threads.runs.retrieve(\n                thread_id=thread_id,\n                run_id=run_id\n            )\n\n            if run.status in [\"completed\", \"failed\", \"requires_action\"]:\n                return run\n\n            time.sleep(1)\n</code></pre>"},{"location":"implementation/#4-tool-registry","title":"4. Tool Registry","text":"<p>The tool registry manages available tools and their configurations:</p> <pre><code>from enum import Enum, auto\nfrom typing import List, Optional\n\nclass OpenAIBuiltInTools(Enum):\n    WEB_SEARCH = \"retrieval\"\n    CODE_INTERPRETER = \"code_interpreter\"\n    WEB_BROWSER = \"web_browser\"\n    FILE_SEARCH = \"file_search\"\n\nclass ToolRegistry:\n    def __init__(self, enabled_tools=None):\n        self.tools = {}\n        self.enabled_tools = enabled_tools or [t.value for t in OpenAIBuiltInTools]\n        self._register_default_tools()\n\n    def _register_default_tools(self):\n        \"\"\"Register the default tool mappings\"\"\"\n        self.tools = {\n            \"web-search\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_SEARCH.value in self.enabled_tools\n            },\n            \"code-execution\": {\n                \"openai_tool\": OpenAIBuiltInTools.CODE_INTERPRETER.value,\n                \"enabled\": OpenAIBuiltInTools.CODE_INTERPRETER.value in self.enabled_tools\n            },\n            \"browser\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_BROWSER.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_BROWSER.value in self.enabled_tools\n            },\n            \"file-io\": {\n                \"openai_tool\": OpenAIBuiltInTools.FILE_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.FILE_SEARCH.value in self.enabled_tools\n            }\n        }\n\n    def has_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"Check if a tool is registered and enabled\"\"\"\n        return tool_id in self.tools and self.tools[tool_id][\"enabled\"]\n\n    def get_openai_tool_type(self, tool_id: str) -&gt; Optional[str]:\n        \"\"\"Get the OpenAI tool type for a given MCP tool ID\"\"\"\n        if self.has_tool(tool_id):\n            return self.tools[tool_id][\"openai_tool\"]\n        return None\n</code></pre>"},{"location":"implementation/#configuration","title":"Configuration","text":"<p>Configuration is managed through a dedicated ServerConfig class:</p> <pre><code>from typing import List, Optional\nfrom .tools import OpenAIBuiltInTools\n\nclass ServerConfig:\n    def __init__(\n        self,\n        openai_api_key: Optional[str] = None,\n        tools: Optional[List[str]] = None,\n        request_timeout: int = 30,\n        max_retries: int = 3\n    ):\n        \"\"\"\n        Initialize server configuration.\n\n        Args:\n            openai_api_key: OpenAI API key (defaults to environment variable)\n            tools: List of enabled tools (defaults to all)\n            request_timeout: Timeout for API requests in seconds\n            max_retries: Maximum number of retries for failed requests\n        \"\"\"\n        import os\n\n        self.openai_api_key = openai_api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self.openai_api_key:\n            raise ValueError(\"OpenAI API key is required\")\n\n        self.tools = tools or [t.value for t in OpenAIBuiltInTools]\n        self.request_timeout = request_timeout\n        self.max_retries = max_retries\n</code></pre>"},{"location":"implementation/#mcp-protocol-implementation","title":"MCP Protocol Implementation","text":"<p>The implementation follows the MCP protocol specification:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Dict, Any, Optional\n\nclass MCPRequest(BaseModel):\n    \"\"\"Model for MCP tool request\"\"\"\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n    context: Optional[Dict[str, Any]] = Field(default=None)\n\nclass MCPResponse(BaseModel):\n    \"\"\"Model for MCP tool response\"\"\"\n    content: str\n    error: Optional[str] = None\n    context: Optional[Dict[str, Any]] = Field(default_factory=dict)\n</code></pre>"},{"location":"implementation/#running-the-server","title":"Running the Server","text":"<p>To run the server with default configuration:</p> <pre><code>from openai_tool2mcp import MCPServer, ServerConfig, OpenAIBuiltInTools\n\n# Create server with all tools enabled\nconfig = ServerConfig(\n    tools=[t.value for t in OpenAIBuiltInTools]\n)\n\n# Start server\nserver = MCPServer(config)\nserver.start(host=\"127.0.0.1\", port=8000)\n</code></pre>"},{"location":"implementation/#cli-interface","title":"CLI Interface","text":"<p>The package includes a CLI interface for easy server startup:</p> <pre><code>import argparse\nimport os\nfrom .server import MCPServer, ServerConfig\nfrom .tools import OpenAIBuiltInTools\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Start an MCP server for OpenAI tools\")\n    parser.add_argument(\"--host\", default=\"127.0.0.1\", help=\"Host to listen on\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to listen on\")\n    parser.add_argument(\"--api-key\", help=\"OpenAI API key (defaults to OPENAI_API_KEY env var)\")\n    parser.add_argument(\"--tools\", nargs=\"+\", choices=[t.value for t in OpenAIBuiltInTools],\n                        default=[t.value for t in OpenAIBuiltInTools],\n                        help=\"Enabled tools\")\n\n    args = parser.parse_args()\n\n    # Create server config\n    config = ServerConfig(\n        openai_api_key=args.api_key or os.environ.get(\"OPENAI_API_KEY\"),\n        tools=args.tools\n    )\n\n    # Start server\n    server = MCPServer(config)\n    print(f\"Starting MCP server on {args.host}:{args.port}\")\n    server.start(host=args.host, port=args.port)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"implementation/#error-handling","title":"Error Handling","text":"<p>Robust error handling is implemented across the system:</p> <pre><code>class MCPError(Exception):\n    \"\"\"Base class for all MCP errors\"\"\"\n    def __init__(self, message, status_code=500):\n        self.message = message\n        self.status_code = status_code\n        super().__init__(self.message)\n\nclass ToolNotFoundError(MCPError):\n    \"\"\"Error raised when a requested tool is not found\"\"\"\n    def __init__(self, tool_id):\n        super().__init__(f\"Tool {tool_id} not found\", 404)\n\nclass OpenAIError(MCPError):\n    \"\"\"Error raised when there's an issue with the OpenAI API\"\"\"\n    def __init__(self, message, status_code=500):\n        super().__init__(f\"OpenAI API error: {message}\", status_code)\n\nclass ConfigurationError(MCPError):\n    \"\"\"Error raised when there's an issue with configuration\"\"\"\n    def __init__(self, message):\n        super().__init__(f\"Configuration error: {message}\", 500)\n</code></pre>"},{"location":"implementation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"implementation/#custom-tool-implementation","title":"Custom Tool Implementation","text":"<p>To implement a custom tool adapter:</p> <pre><code>from ..models.mcp import MCPRequest, MCPResponse\nfrom abc import ABC, abstractmethod\n\nclass ToolAdapter(ABC):\n    \"\"\"Base class for tool adapters\"\"\"\n\n    @property\n    @abstractmethod\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"Translate MCP request to OpenAI parameters\"\"\"\n        pass\n\n    @abstractmethod\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"Translate OpenAI response to MCP response\"\"\"\n        pass\n\n# Example implementation for web search\nclass WebSearchAdapter(ToolAdapter):\n    @property\n    def tool_id(self) -&gt; str:\n        return \"web-search\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        return \"retrieval\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        # Extract search query\n        query = request.parameters.get(\"query\", \"\")\n\n        # Return OpenAI parameters\n        return {\"query\": query}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        # Extract search results\n        results = response.get(\"results\", [])\n\n        # Format results as markdown\n        content = \"# Search Results\\n\\n\"\n        for i, result in enumerate(results):\n            content += f\"## {i+1}. {result.get('title', 'No title')}\\n\"\n            content += f\"**URL**: {result.get('url', 'No URL')}\\n\"\n            content += f\"{result.get('snippet', 'No snippet')}\\n\\n\"\n\n        # Return MCP response\n        return MCPResponse(\n            content=content,\n            context={\"search_query\": query}\n        )\n</code></pre>"},{"location":"implementation/#middleware-support","title":"Middleware Support","text":"<p>Middleware can be used to add cross-cutting functionality:</p> <pre><code>from starlette.middleware.base import BaseHTTPMiddleware\nfrom fastapi import FastAPI\n\nclass LoggingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        # Log request\n        print(f\"Request: {request.method} {request.url}\")\n\n        # Call next middleware\n        response = await call_next(request)\n\n        # Log response\n        print(f\"Response: {response.status_code}\")\n\n        return response\n\ndef add_middleware(app: FastAPI):\n    \"\"\"Add middleware to FastAPI app\"\"\"\n    app.add_middleware(LoggingMiddleware)\n</code></pre> <p>This implementation guide provides a comprehensive blueprint for building the openai-tool2mcp bridge. By following this architecture and implementation details, you can create a fully functional MCP server that leverages OpenAI's powerful built-in tools.</p>"},{"location":"modules/","title":"Module Reference","text":"<p>This page provides auto-generated documentation for the openai-tool2mcp Python modules.</p>"},{"location":"modules/#core-modules","title":"Core Modules","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.server.MCPServer","title":"<code>MCPServer</code>","text":"<p>MCP server that wraps OpenAI tools</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>class MCPServer:\n    \"\"\"MCP server that wraps OpenAI tools\"\"\"\n\n    def __init__(self, config=None):\n        \"\"\"\n        Initialize the MCP server.\n\n        Args:\n            config (ServerConfig, optional): Server configuration\n        \"\"\"\n        self.app = FastAPI(\n            title=\"OpenAI Tool2MCP Server\", description=\"MCP server that wraps OpenAI built-in tools\", version=\"0.1.0\"\n        )\n        self.config = config or ServerConfig()\n\n        # Ensure we have an API key\n        if not self.config.openai_api_key:\n            raise APIKeyMissingError()\n\n        self.openai_client = OpenAIClient(\n            api_key=self.config.openai_api_key,\n            request_timeout=self.config.request_timeout,\n            max_retries=self.config.max_retries,\n        )\n        self.tool_registry = ToolRegistry(self.config.tools)\n        self.tools_map = self._build_tools_map()\n\n        # Register routes\n        self.register_routes()\n\n    def _build_tools_map(self):\n        \"\"\"Build a map of tool adapters\"\"\"\n        tools_map = {}\n\n        # Register default tool adapters\n        adapters = [WebSearchAdapter(), CodeInterpreterAdapter(), BrowserAdapter(), FileManagerAdapter()]\n\n        for adapter in adapters:\n            # Only register if the tool is enabled\n            if adapter.openai_tool_type in self.config.tools:\n                tools_map[adapter.tool_id] = adapter\n\n        return tools_map\n\n    def register_routes(self):\n        \"\"\"Register FastAPI routes for the MCP protocol\"\"\"\n\n        @self.app.get(\"/\")\n        async def root():\n            \"\"\"Root endpoint for the API\"\"\"\n            return {\n                \"name\": \"OpenAI Tool2MCP Server\",\n                \"version\": \"0.1.0\",\n                \"tools\": [\n                    {\"id\": tool_id, \"description\": adapter.description} for tool_id, adapter in self.tools_map.items()\n                ],\n            }\n\n        @self.app.get(\"/health\")\n        async def health():\n            \"\"\"Health check endpoint\"\"\"\n            return {\"status\": \"ok\"}\n\n        @self.app.get(\"/tools\")\n        async def list_tools():\n            \"\"\"List available tools\"\"\"\n            return {\n                \"tools\": [\n                    {\"id\": tool_id, \"description\": adapter.description} for tool_id, adapter in self.tools_map.items()\n                ]\n            }\n\n        @self.app.post(\"/v1/tools/{tool_id}/invoke\")\n        async def invoke_tool(tool_id: str, request: MCPRequest):\n            \"\"\"\n            Invoke a tool with the specified ID.\n\n            Args:\n                tool_id (str): ID of the tool to invoke\n                request (MCPRequest): Tool request\n\n            Returns:\n                MCPResponse: Tool response\n            \"\"\"\n            # Check if tool exists\n            if tool_id not in self.tools_map:\n                raise HTTPException(status_code=404, detail=f\"Tool {tool_id} not found\")\n\n            logger.info(f\"Invoking tool: {tool_id}\")\n\n            try:\n                # Get the tool adapter\n                adapter = self.tools_map[tool_id]\n\n                # Translate the request parameters using the adapter\n                parameters = await adapter.translate_request(request)\n\n                # Create an OpenAI tool request\n                openai_request = mcp_to_openai.translate_request(request, tool_id)\n\n                # Override the parameters with the adapter-specific ones\n                openai_request.parameters = parameters\n\n                # Call OpenAI API to execute the tool\n                openai_response = await self.openai_client.invoke_tool(openai_request)\n\n                # Translate the OpenAI response to MCP format using the adapter\n                if openai_response.tool_outputs:\n                    # Use the adapter to translate the tool-specific response\n                    mcp_response = await adapter.translate_response(openai_response.tool_outputs[0].output)\n\n                    # Add thread_id to context for state management\n                    if mcp_response.context is None:\n                        mcp_response.context = {}\n                    mcp_response.context[\"thread_id\"] = openai_response.thread_id\n                else:\n                    # Fallback to generic translation if no tool output is available\n                    mcp_response = openai_to_mcp.translate_response(openai_response)\n            except Exception as e:\n                logger.error(f\"Error invoking tool {tool_id}: {e!s}\")\n                raise HTTPException(status_code=500, detail=str(e)) from e\n            else:\n                return mcp_response\n\n    def start(self, host=\"127.0.0.1\", port=8000):\n        \"\"\"\n        Start the MCP server.\n\n        Args:\n            host (str): Host address to bind to\n            port (int): Port to listen on\n        \"\"\"\n        import uvicorn\n\n        logger.info(f\"Starting MCP server on {host}:{port}\")\n        logger.info(f\"Available tools: {', '.join(self.tools_map.keys())}\")\n\n        uvicorn.run(self.app, host=host, port=port)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ServerConfig</code> <p>Server configuration</p> <code>None</code> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def __init__(self, config=None):\n    \"\"\"\n    Initialize the MCP server.\n\n    Args:\n        config (ServerConfig, optional): Server configuration\n    \"\"\"\n    self.app = FastAPI(\n        title=\"OpenAI Tool2MCP Server\", description=\"MCP server that wraps OpenAI built-in tools\", version=\"0.1.0\"\n    )\n    self.config = config or ServerConfig()\n\n    # Ensure we have an API key\n    if not self.config.openai_api_key:\n        raise APIKeyMissingError()\n\n    self.openai_client = OpenAIClient(\n        api_key=self.config.openai_api_key,\n        request_timeout=self.config.request_timeout,\n        max_retries=self.config.max_retries,\n    )\n    self.tool_registry = ToolRegistry(self.config.tools)\n    self.tools_map = self._build_tools_map()\n\n    # Register routes\n    self.register_routes()\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer._build_tools_map","title":"<code>_build_tools_map()</code>","text":"<p>Build a map of tool adapters</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def _build_tools_map(self):\n    \"\"\"Build a map of tool adapters\"\"\"\n    tools_map = {}\n\n    # Register default tool adapters\n    adapters = [WebSearchAdapter(), CodeInterpreterAdapter(), BrowserAdapter(), FileManagerAdapter()]\n\n    for adapter in adapters:\n        # Only register if the tool is enabled\n        if adapter.openai_tool_type in self.config.tools:\n            tools_map[adapter.tool_id] = adapter\n\n    return tools_map\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer.register_routes","title":"<code>register_routes()</code>","text":"<p>Register FastAPI routes for the MCP protocol</p> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def register_routes(self):\n    \"\"\"Register FastAPI routes for the MCP protocol\"\"\"\n\n    @self.app.get(\"/\")\n    async def root():\n        \"\"\"Root endpoint for the API\"\"\"\n        return {\n            \"name\": \"OpenAI Tool2MCP Server\",\n            \"version\": \"0.1.0\",\n            \"tools\": [\n                {\"id\": tool_id, \"description\": adapter.description} for tool_id, adapter in self.tools_map.items()\n            ],\n        }\n\n    @self.app.get(\"/health\")\n    async def health():\n        \"\"\"Health check endpoint\"\"\"\n        return {\"status\": \"ok\"}\n\n    @self.app.get(\"/tools\")\n    async def list_tools():\n        \"\"\"List available tools\"\"\"\n        return {\n            \"tools\": [\n                {\"id\": tool_id, \"description\": adapter.description} for tool_id, adapter in self.tools_map.items()\n            ]\n        }\n\n    @self.app.post(\"/v1/tools/{tool_id}/invoke\")\n    async def invoke_tool(tool_id: str, request: MCPRequest):\n        \"\"\"\n        Invoke a tool with the specified ID.\n\n        Args:\n            tool_id (str): ID of the tool to invoke\n            request (MCPRequest): Tool request\n\n        Returns:\n            MCPResponse: Tool response\n        \"\"\"\n        # Check if tool exists\n        if tool_id not in self.tools_map:\n            raise HTTPException(status_code=404, detail=f\"Tool {tool_id} not found\")\n\n        logger.info(f\"Invoking tool: {tool_id}\")\n\n        try:\n            # Get the tool adapter\n            adapter = self.tools_map[tool_id]\n\n            # Translate the request parameters using the adapter\n            parameters = await adapter.translate_request(request)\n\n            # Create an OpenAI tool request\n            openai_request = mcp_to_openai.translate_request(request, tool_id)\n\n            # Override the parameters with the adapter-specific ones\n            openai_request.parameters = parameters\n\n            # Call OpenAI API to execute the tool\n            openai_response = await self.openai_client.invoke_tool(openai_request)\n\n            # Translate the OpenAI response to MCP format using the adapter\n            if openai_response.tool_outputs:\n                # Use the adapter to translate the tool-specific response\n                mcp_response = await adapter.translate_response(openai_response.tool_outputs[0].output)\n\n                # Add thread_id to context for state management\n                if mcp_response.context is None:\n                    mcp_response.context = {}\n                mcp_response.context[\"thread_id\"] = openai_response.thread_id\n            else:\n                # Fallback to generic translation if no tool output is available\n                mcp_response = openai_to_mcp.translate_response(openai_response)\n        except Exception as e:\n            logger.error(f\"Error invoking tool {tool_id}: {e!s}\")\n            raise HTTPException(status_code=500, detail=str(e)) from e\n        else:\n            return mcp_response\n</code></pre>"},{"location":"modules/#openai_tool2mcp.server.MCPServer.start","title":"<code>start(host='127.0.0.1', port=8000)</code>","text":"<p>Start the MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Host address to bind to</p> <code>'127.0.0.1'</code> <code>port</code> <code>int</code> <p>Port to listen on</p> <code>8000</code> Source code in <code>openai_tool2mcp/server.py</code> <pre><code>def start(self, host=\"127.0.0.1\", port=8000):\n    \"\"\"\n    Start the MCP server.\n\n    Args:\n        host (str): Host address to bind to\n        port (int): Port to listen on\n    \"\"\"\n    import uvicorn\n\n    logger.info(f\"Starting MCP server on {host}:{port}\")\n    logger.info(f\"Available tools: {', '.join(self.tools_map.keys())}\")\n\n    uvicorn.run(self.app, host=host, port=port)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.OpenAIBuiltInTools","title":"<code>OpenAIBuiltInTools</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for built-in OpenAI tools</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>class OpenAIBuiltInTools(Enum):\n    \"\"\"Enum for built-in OpenAI tools\"\"\"\n\n    WEB_SEARCH = \"retrieval\"\n    CODE_INTERPRETER = \"code_interpreter\"\n    WEB_BROWSER = \"web_browser\"\n    FILE_SEARCH = \"file_search\"\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry","title":"<code>ToolRegistry</code>","text":"<p>Registry for MCP tools mapped to OpenAI tools</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>class ToolRegistry:\n    \"\"\"Registry for MCP tools mapped to OpenAI tools\"\"\"\n\n    def __init__(self, enabled_tools=None):\n        \"\"\"\n        Initialize the tool registry.\n\n        Args:\n            enabled_tools (List[str], optional): List of enabled tools\n        \"\"\"\n        self.tools = {}\n        self.enabled_tools = enabled_tools or [t.value for t in OpenAIBuiltInTools]\n        self._register_default_tools()\n\n    def _register_default_tools(self):\n        \"\"\"Register the default tool mappings\"\"\"\n        self.tools = {\n            \"web-search\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_SEARCH.value in self.enabled_tools,\n                \"description\": \"Search the web for information\",\n            },\n            \"code-execution\": {\n                \"openai_tool\": OpenAIBuiltInTools.CODE_INTERPRETER.value,\n                \"enabled\": OpenAIBuiltInTools.CODE_INTERPRETER.value in self.enabled_tools,\n                \"description\": \"Execute code in a sandbox environment\",\n            },\n            \"browser\": {\n                \"openai_tool\": OpenAIBuiltInTools.WEB_BROWSER.value,\n                \"enabled\": OpenAIBuiltInTools.WEB_BROWSER.value in self.enabled_tools,\n                \"description\": \"Browse websites and access web content\",\n            },\n            \"file-io\": {\n                \"openai_tool\": OpenAIBuiltInTools.FILE_SEARCH.value,\n                \"enabled\": OpenAIBuiltInTools.FILE_SEARCH.value in self.enabled_tools,\n                \"description\": \"Search and access file content\",\n            },\n        }\n\n    def register_tool(self, tool_id: str, openai_tool: str, enabled: bool = True, description: str = \"\"):\n        \"\"\"\n        Register a new tool.\n\n        Args:\n            tool_id (str): MCP tool ID\n            openai_tool (str): OpenAI tool type\n            enabled (bool): Whether the tool is enabled\n            description (str): Tool description\n        \"\"\"\n        self.tools[tool_id] = {\"openai_tool\": openai_tool, \"enabled\": enabled, \"description\": description}\n\n    def has_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"\n        Check if a tool is registered and enabled.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool is available\n        \"\"\"\n        return tool_id in self.tools and self.tools[tool_id][\"enabled\"]\n\n    def get_openai_tool_type(self, tool_id: str) -&gt; Optional[str]:\n        \"\"\"\n        Get the OpenAI tool type for a given MCP tool ID.\n\n        Args:\n            tool_id (str): MCP tool ID\n\n        Returns:\n            str: OpenAI tool type\n        \"\"\"\n        if self.has_tool(tool_id):\n            return self.tools[tool_id][\"openai_tool\"]\n        return None\n\n    def get_enabled_tools(self) -&gt; dict[str, dict]:\n        \"\"\"\n        Get all enabled tools.\n\n        Returns:\n            dict[str, dict]: Dictionary of enabled tools\n        \"\"\"\n        return {tool_id: tool_info for tool_id, tool_info in self.tools.items() if tool_info[\"enabled\"]}\n\n    def enable_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"\n        Enable a tool.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool was enabled\n        \"\"\"\n        if tool_id in self.tools:\n            self.tools[tool_id][\"enabled\"] = True\n            return True\n        return False\n\n    def disable_tool(self, tool_id: str) -&gt; bool:\n        \"\"\"\n        Disable a tool.\n\n        Args:\n            tool_id (str): Tool ID\n\n        Returns:\n            bool: True if the tool was disabled\n        \"\"\"\n        if tool_id in self.tools:\n            self.tools[tool_id][\"enabled\"] = False\n            return True\n        return False\n\n    def register(self, tool_type: ToolType, tool):\n        \"\"\"Register a tool\"\"\"\n        self.tools[tool_type.value] = tool\n\n    def get_tool(self, tool_type: ToolType) -&gt; Optional[object]:\n        \"\"\"Get a tool by type\"\"\"\n        return self.tools.get(tool_type.value)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.__init__","title":"<code>__init__(enabled_tools=None)</code>","text":"<p>Initialize the tool registry.</p> <p>Parameters:</p> Name Type Description Default <code>enabled_tools</code> <code>List[str]</code> <p>List of enabled tools</p> <code>None</code> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def __init__(self, enabled_tools=None):\n    \"\"\"\n    Initialize the tool registry.\n\n    Args:\n        enabled_tools (List[str], optional): List of enabled tools\n    \"\"\"\n    self.tools = {}\n    self.enabled_tools = enabled_tools or [t.value for t in OpenAIBuiltInTools]\n    self._register_default_tools()\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry._register_default_tools","title":"<code>_register_default_tools()</code>","text":"<p>Register the default tool mappings</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def _register_default_tools(self):\n    \"\"\"Register the default tool mappings\"\"\"\n    self.tools = {\n        \"web-search\": {\n            \"openai_tool\": OpenAIBuiltInTools.WEB_SEARCH.value,\n            \"enabled\": OpenAIBuiltInTools.WEB_SEARCH.value in self.enabled_tools,\n            \"description\": \"Search the web for information\",\n        },\n        \"code-execution\": {\n            \"openai_tool\": OpenAIBuiltInTools.CODE_INTERPRETER.value,\n            \"enabled\": OpenAIBuiltInTools.CODE_INTERPRETER.value in self.enabled_tools,\n            \"description\": \"Execute code in a sandbox environment\",\n        },\n        \"browser\": {\n            \"openai_tool\": OpenAIBuiltInTools.WEB_BROWSER.value,\n            \"enabled\": OpenAIBuiltInTools.WEB_BROWSER.value in self.enabled_tools,\n            \"description\": \"Browse websites and access web content\",\n        },\n        \"file-io\": {\n            \"openai_tool\": OpenAIBuiltInTools.FILE_SEARCH.value,\n            \"enabled\": OpenAIBuiltInTools.FILE_SEARCH.value in self.enabled_tools,\n            \"description\": \"Search and access file content\",\n        },\n    }\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.disable_tool","title":"<code>disable_tool(tool_id)</code>","text":"<p>Disable a tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>Tool ID</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool was disabled</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def disable_tool(self, tool_id: str) -&gt; bool:\n    \"\"\"\n    Disable a tool.\n\n    Args:\n        tool_id (str): Tool ID\n\n    Returns:\n        bool: True if the tool was disabled\n    \"\"\"\n    if tool_id in self.tools:\n        self.tools[tool_id][\"enabled\"] = False\n        return True\n    return False\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.enable_tool","title":"<code>enable_tool(tool_id)</code>","text":"<p>Enable a tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>Tool ID</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool was enabled</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def enable_tool(self, tool_id: str) -&gt; bool:\n    \"\"\"\n    Enable a tool.\n\n    Args:\n        tool_id (str): Tool ID\n\n    Returns:\n        bool: True if the tool was enabled\n    \"\"\"\n    if tool_id in self.tools:\n        self.tools[tool_id][\"enabled\"] = True\n        return True\n    return False\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.get_enabled_tools","title":"<code>get_enabled_tools()</code>","text":"<p>Get all enabled tools.</p> <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: Dictionary of enabled tools</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def get_enabled_tools(self) -&gt; dict[str, dict]:\n    \"\"\"\n    Get all enabled tools.\n\n    Returns:\n        dict[str, dict]: Dictionary of enabled tools\n    \"\"\"\n    return {tool_id: tool_info for tool_id, tool_info in self.tools.items() if tool_info[\"enabled\"]}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.get_openai_tool_type","title":"<code>get_openai_tool_type(tool_id)</code>","text":"<p>Get the OpenAI tool type for a given MCP tool ID.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>MCP tool ID</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>OpenAI tool type</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def get_openai_tool_type(self, tool_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Get the OpenAI tool type for a given MCP tool ID.\n\n    Args:\n        tool_id (str): MCP tool ID\n\n    Returns:\n        str: OpenAI tool type\n    \"\"\"\n    if self.has_tool(tool_id):\n        return self.tools[tool_id][\"openai_tool\"]\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.get_tool","title":"<code>get_tool(tool_type)</code>","text":"<p>Get a tool by type</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def get_tool(self, tool_type: ToolType) -&gt; Optional[object]:\n    \"\"\"Get a tool by type\"\"\"\n    return self.tools.get(tool_type.value)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.has_tool","title":"<code>has_tool(tool_id)</code>","text":"<p>Check if a tool is registered and enabled.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>Tool ID</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool is available</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def has_tool(self, tool_id: str) -&gt; bool:\n    \"\"\"\n    Check if a tool is registered and enabled.\n\n    Args:\n        tool_id (str): Tool ID\n\n    Returns:\n        bool: True if the tool is available\n    \"\"\"\n    return tool_id in self.tools and self.tools[tool_id][\"enabled\"]\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.register","title":"<code>register(tool_type, tool)</code>","text":"<p>Register a tool</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def register(self, tool_type: ToolType, tool):\n    \"\"\"Register a tool\"\"\"\n    self.tools[tool_type.value] = tool\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolRegistry.register_tool","title":"<code>register_tool(tool_id, openai_tool, enabled=True, description='')</code>","text":"<p>Register a new tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>MCP tool ID</p> required <code>openai_tool</code> <code>str</code> <p>OpenAI tool type</p> required <code>enabled</code> <code>bool</code> <p>Whether the tool is enabled</p> <code>True</code> <code>description</code> <code>str</code> <p>Tool description</p> <code>''</code> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>def register_tool(self, tool_id: str, openai_tool: str, enabled: bool = True, description: str = \"\"):\n    \"\"\"\n    Register a new tool.\n\n    Args:\n        tool_id (str): MCP tool ID\n        openai_tool (str): OpenAI tool type\n        enabled (bool): Whether the tool is enabled\n        description (str): Tool description\n    \"\"\"\n    self.tools[tool_id] = {\"openai_tool\": openai_tool, \"enabled\": enabled, \"description\": description}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.registry.ToolType","title":"<code>ToolType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for tool types</p> Source code in <code>openai_tool2mcp/tools/registry.py</code> <pre><code>class ToolType(Enum):\n    \"\"\"Enum for tool types\"\"\"\n\n    BROWSER = \"browser\"\n    CODE = \"code\"\n    SEARCH = \"search\"\n</code></pre>"},{"location":"modules/#protocol-translation","title":"Protocol Translation","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.translator.mcp_to_openai.map_tool_id_to_openai_type","title":"<code>map_tool_id_to_openai_type(tool_id)</code>","text":"<p>Map MCP tool IDs to OpenAI tool types.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>MCP tool ID</p> required <p>Returns:</p> Type Description <code>str</code> <p>OpenAI tool type</p> Source code in <code>openai_tool2mcp/translator/mcp_to_openai.py</code> <pre><code>def map_tool_id_to_openai_type(tool_id: str) -&gt; str:\n    \"\"\"\n    Map MCP tool IDs to OpenAI tool types.\n\n    Args:\n        tool_id: MCP tool ID\n\n    Returns:\n        OpenAI tool type\n    \"\"\"\n    mapping = {\n        \"web-search\": \"retrieval\",\n        \"code-execution\": \"code_interpreter\",\n        \"browser\": \"web_browser\",\n        \"file-io\": \"file_search\",\n    }\n\n    openai_type = mapping.get(tool_id, tool_id)\n    logger.debug(f\"Mapped MCP tool ID {tool_id} to OpenAI tool type {openai_type}\")\n\n    return openai_type\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.mcp_to_openai.translate_request","title":"<code>translate_request(mcp_request, tool_id)</code>","text":"<p>Translate an MCP request to an OpenAI request format.</p> <p>Parameters:</p> Name Type Description Default <code>mcp_request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <code>tool_id</code> <code>str</code> <p>The ID of the tool to invoke</p> required <p>Returns:</p> Type Description <code>ToolRequest</code> <p>An OpenAI tool request object</p> Source code in <code>openai_tool2mcp/translator/mcp_to_openai.py</code> <pre><code>def translate_request(mcp_request: MCPRequest, tool_id: str) -&gt; ToolRequest:\n    \"\"\"\n    Translate an MCP request to an OpenAI request format.\n\n    Args:\n        mcp_request: The MCP request to translate\n        tool_id: The ID of the tool to invoke\n\n    Returns:\n        An OpenAI tool request object\n    \"\"\"\n    # Extract tool parameters and sanitize them\n    parameters = sanitize_parameters(mcp_request.parameters)\n\n    # Extract context information\n    context = mcp_request.context or {}\n\n    # Determine if this is a new or existing conversation\n    thread_id = context.get(\"thread_id\")\n\n    # Log the translation\n    logger.debug(f\"Translating MCP request for tool {tool_id} to OpenAI format\")\n\n    # Create OpenAI request\n    openai_request = ToolRequest(\n        tool_type=map_tool_id_to_openai_type(tool_id),\n        parameters=parameters,\n        thread_id=thread_id,\n        instructions=context.get(\"instructions\", \"\"),\n    )\n\n    return openai_request\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.openai_to_mcp.format_code_result","title":"<code>format_code_result(result)</code>","text":"<p>Format code execution result in a human-readable format</p> Source code in <code>openai_tool2mcp/translator/openai_to_mcp.py</code> <pre><code>def format_code_result(result: Union[dict[str, Any], str]) -&gt; str:\n    \"\"\"\n    Format code execution result in a human-readable format\n    \"\"\"\n    if isinstance(result, str):\n        return result\n\n    output = result.get(\"output\", \"\")\n    error = result.get(\"error\")\n    if error:\n        return f\"Error: {error}\\n\\nOutput:\\n{output}\"\n    return output\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.openai_to_mcp.format_search_results","title":"<code>format_search_results(results)</code>","text":"<p>Format search results in a human-readable format</p> Source code in <code>openai_tool2mcp/translator/openai_to_mcp.py</code> <pre><code>def format_search_results(results: list[dict[str, Any]]) -&gt; str:\n    \"\"\"\n    Format search results in a human-readable format\n    \"\"\"\n    if not results:\n        return \"No results found.\"\n\n    formatted_results = []\n    for result in results:\n        title = result.get(\"title\", \"Untitled\")\n        content = result.get(\"content\", \"\")\n        formatted_results.append(f\"# {title}\\n\\n{content}\")\n\n    return \"\\n\\n\".join(formatted_results)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.translator.openai_to_mcp.translate_response","title":"<code>translate_response(openai_response)</code>","text":"<p>Translate an OpenAI response to an MCP response format.</p> <p>Parameters:</p> Name Type Description Default <code>openai_response</code> <code>ToolResponse</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>An MCP response object</p> Source code in <code>openai_tool2mcp/translator/openai_to_mcp.py</code> <pre><code>def translate_response(openai_response: ToolResponse) -&gt; MCPResponse:\n    \"\"\"\n    Translate an OpenAI response to an MCP response format.\n\n    Args:\n        openai_response: The OpenAI response to translate\n\n    Returns:\n        An MCP response object\n    \"\"\"\n    # Extract tool output\n    tool_output = openai_response.tool_outputs[0] if openai_response.tool_outputs else None\n\n    # Log the translation\n    logger.debug(\"Translating OpenAI response to MCP format\")\n\n    if not tool_output:\n        # Handle case where there's no tool output\n        logger.warning(\"No tool output found in OpenAI response\")\n        return MCPResponse(\n            content=\"No result\", error=\"Tool returned no output\", context={\"thread_id\": openai_response.thread_id}\n        )\n\n    # Format the output content\n    try:\n        # Try to parse as JSON for structured output\n        output_content = tool_output.output\n        if isinstance(output_content, str):\n            try:\n                # If it's a JSON string, parse it\n                parsed_content = json.loads(output_content)\n                # Format it nicely if possible\n                if isinstance(parsed_content, dict) and \"result\" in parsed_content:\n                    content = str(parsed_content[\"result\"])\n                else:\n                    content = json.dumps(parsed_content, indent=2)\n            except json.JSONDecodeError:\n                # Not JSON, use as is\n                content = output_content\n        else:\n            # Not a string, convert to string\n            content = str(output_content)\n    except Exception as e:\n        # Fallback for any errors\n        logger.error(f\"Error formatting tool output: {e!s}\")\n        content = str(tool_output.output)\n\n    # Create MCP response\n    error = tool_output.error if hasattr(tool_output, \"error\") and tool_output.error else None\n\n    mcp_response = MCPResponse(content=content, error=error, context={\"thread_id\": openai_response.thread_id})\n\n    return mcp_response\n</code></pre>"},{"location":"modules/#api-clients","title":"API Clients","text":"<p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.openai_client.client.AssistantCreationError","title":"<code>AssistantCreationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception raised when assistant creation fails</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class AssistantCreationError(RuntimeError):\n    \"\"\"Exception raised when assistant creation fails\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Assistant creation failed\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient","title":"<code>OpenAIClient</code>","text":"<p>Client for interacting with the OpenAI API</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class OpenAIClient:\n    \"\"\"Client for interacting with the OpenAI API\"\"\"\n\n    def __init__(self, api_key: str, request_timeout: int = 30, max_retries: int = 3):\n        \"\"\"\n        Initialize the OpenAI client.\n\n        Args:\n            api_key (str): OpenAI API key\n            request_timeout (int): Request timeout in seconds\n            max_retries (int): Maximum number of retries\n        \"\"\"\n        self.client = openai.Client(api_key=api_key, timeout=request_timeout)\n        self.max_retries = max_retries\n        self.retry_delay = 1  # Assuming a default retry_delay\n\n    async def invoke_tool(self, request: ToolRequest) -&gt; ToolResponse:\n        \"\"\"\n        Invoke an OpenAI tool.\n\n        Args:\n            request (ToolRequest): Tool request\n\n        Returns:\n            ToolResponse: Tool response\n        \"\"\"\n        # Create or get thread\n        thread_id = request.thread_id\n        if not thread_id:\n            thread = await self._create_thread()\n            if thread is None:\n                raise ThreadCreationError()\n            thread_id = thread.id\n\n        # Create message with tool call\n        await self._create_message(\n            thread_id=thread_id,\n            content=f\"Please use the {request.tool_type} tool with these parameters: {request.parameters}\",\n        )\n\n        # Create assistant with the appropriate tool\n        assistant = await self._create_assistant(\n            tools=[{\"type\": request.tool_type}],\n            instructions=request.instructions or \"Execute the requested tool function.\",\n        )\n        if assistant is None:\n            raise AssistantCreationError()\n\n        # Run the assistant\n        run = await self._create_run(thread_id=thread_id, assistant_id=assistant.id)\n        if run is None:\n            raise RunCreationError()\n\n        # Wait for completion\n        run = await self._wait_for_run(thread_id, run.id)\n\n        # Get tool outputs\n        tool_outputs = []\n        if hasattr(run, \"required_action\") and hasattr(run.required_action, \"submit_tool_outputs\"):\n            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n                # Process each tool call\n                tool_outputs.append(ToolOutput(output=tool_call.function.arguments, error=None))\n\n        # Create response\n        response = ToolResponse(thread_id=thread_id, tool_outputs=tool_outputs)\n\n        return response\n\n    async def _create_thread(self) -&gt; Optional[Any]:\n        \"\"\"Create a new thread\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                thread = await asyncio.to_thread(self.client.beta.threads.create)\n            except Exception as e:\n                logger.error(f\"Error creating thread (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return thread\n        return None\n\n    async def _create_message(self, thread_id: str, content: str) -&gt; Optional[Any]:\n        \"\"\"Create a new message in a thread\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                message = await asyncio.to_thread(\n                    self.client.beta.threads.messages.create, thread_id=thread_id, role=\"user\", content=content\n                )\n            except Exception as e:\n                logger.error(f\"Error creating message (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return message\n        return None\n\n    async def _create_assistant(\n        self, tools: list[dict], instructions: str = \"\", model: str = \"gpt-4-turbo\"\n    ) -&gt; Optional[Any]:\n        \"\"\"Create a new assistant with the specified tools\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                assistant = await asyncio.to_thread(\n                    self.client.beta.assistants.create,\n                    name=\"Tool Assistant\",\n                    model=model,\n                )\n            except Exception as e:\n                logger.error(f\"Error creating assistant (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return assistant\n        return None\n\n    async def _create_run(self, thread_id: str, assistant_id: str) -&gt; Optional[Any]:\n        \"\"\"Create a new run\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                run = await asyncio.to_thread(\n                    self.client.beta.threads.runs.create, thread_id=thread_id, assistant_id=assistant_id\n                )\n            except Exception as e:\n                logger.error(f\"Error creating run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n            else:\n                return run\n        return None\n\n    async def _wait_for_run(self, thread_id: str, run_id: str) -&gt; Any:\n        \"\"\"Wait for a run to complete\"\"\"\n        max_wait_time = 60  # Maximum wait time in seconds\n        start_time = time.time()\n\n        while True:\n            if time.time() - start_time &gt; max_wait_time:\n                raise RunTimeoutError()\n\n            for attempt in range(self.max_retries):\n                try:\n                    run = await asyncio.to_thread(\n                        self.client.beta.threads.runs.retrieve, thread_id=thread_id, run_id=run_id\n                    )\n                    break\n                except Exception as e:\n                    logger.error(f\"Error retrieving run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                    if attempt == self.max_retries - 1:\n                        raise\n                    await asyncio.sleep(self.retry_delay)\n\n            if run.status in [\"completed\", \"failed\", \"requires_action\"]:\n                return run\n\n            await asyncio.sleep(1)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient.__init__","title":"<code>__init__(api_key, request_timeout=30, max_retries=3)</code>","text":"<p>Initialize the OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>request_timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries</p> <code>3</code> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>def __init__(self, api_key: str, request_timeout: int = 30, max_retries: int = 3):\n    \"\"\"\n    Initialize the OpenAI client.\n\n    Args:\n        api_key (str): OpenAI API key\n        request_timeout (int): Request timeout in seconds\n        max_retries (int): Maximum number of retries\n    \"\"\"\n    self.client = openai.Client(api_key=api_key, timeout=request_timeout)\n    self.max_retries = max_retries\n    self.retry_delay = 1  # Assuming a default retry_delay\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_assistant","title":"<code>_create_assistant(tools, instructions='', model='gpt-4-turbo')</code>  <code>async</code>","text":"<p>Create a new assistant with the specified tools</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_assistant(\n    self, tools: list[dict], instructions: str = \"\", model: str = \"gpt-4-turbo\"\n) -&gt; Optional[Any]:\n    \"\"\"Create a new assistant with the specified tools\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            assistant = await asyncio.to_thread(\n                self.client.beta.assistants.create,\n                name=\"Tool Assistant\",\n                model=model,\n            )\n        except Exception as e:\n            logger.error(f\"Error creating assistant (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return assistant\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_message","title":"<code>_create_message(thread_id, content)</code>  <code>async</code>","text":"<p>Create a new message in a thread</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_message(self, thread_id: str, content: str) -&gt; Optional[Any]:\n    \"\"\"Create a new message in a thread\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            message = await asyncio.to_thread(\n                self.client.beta.threads.messages.create, thread_id=thread_id, role=\"user\", content=content\n            )\n        except Exception as e:\n            logger.error(f\"Error creating message (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return message\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_run","title":"<code>_create_run(thread_id, assistant_id)</code>  <code>async</code>","text":"<p>Create a new run</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_run(self, thread_id: str, assistant_id: str) -&gt; Optional[Any]:\n    \"\"\"Create a new run\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            run = await asyncio.to_thread(\n                self.client.beta.threads.runs.create, thread_id=thread_id, assistant_id=assistant_id\n            )\n        except Exception as e:\n            logger.error(f\"Error creating run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return run\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._create_thread","title":"<code>_create_thread()</code>  <code>async</code>","text":"<p>Create a new thread</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _create_thread(self) -&gt; Optional[Any]:\n    \"\"\"Create a new thread\"\"\"\n    for attempt in range(self.max_retries):\n        try:\n            thread = await asyncio.to_thread(self.client.beta.threads.create)\n        except Exception as e:\n            logger.error(f\"Error creating thread (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n            if attempt == self.max_retries - 1:\n                raise\n            await asyncio.sleep(self.retry_delay)\n        else:\n            return thread\n    return None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient._wait_for_run","title":"<code>_wait_for_run(thread_id, run_id)</code>  <code>async</code>","text":"<p>Wait for a run to complete</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def _wait_for_run(self, thread_id: str, run_id: str) -&gt; Any:\n    \"\"\"Wait for a run to complete\"\"\"\n    max_wait_time = 60  # Maximum wait time in seconds\n    start_time = time.time()\n\n    while True:\n        if time.time() - start_time &gt; max_wait_time:\n            raise RunTimeoutError()\n\n        for attempt in range(self.max_retries):\n            try:\n                run = await asyncio.to_thread(\n                    self.client.beta.threads.runs.retrieve, thread_id=thread_id, run_id=run_id\n                )\n                break\n            except Exception as e:\n                logger.error(f\"Error retrieving run (attempt {attempt + 1}/{self.max_retries}): {e!s}\")\n                if attempt == self.max_retries - 1:\n                    raise\n                await asyncio.sleep(self.retry_delay)\n\n        if run.status in [\"completed\", \"failed\", \"requires_action\"]:\n            return run\n\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.OpenAIClient.invoke_tool","title":"<code>invoke_tool(request)</code>  <code>async</code>","text":"<p>Invoke an OpenAI tool.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ToolRequest</code> <p>Tool request</p> required <p>Returns:</p> Name Type Description <code>ToolResponse</code> <code>ToolResponse</code> <p>Tool response</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>async def invoke_tool(self, request: ToolRequest) -&gt; ToolResponse:\n    \"\"\"\n    Invoke an OpenAI tool.\n\n    Args:\n        request (ToolRequest): Tool request\n\n    Returns:\n        ToolResponse: Tool response\n    \"\"\"\n    # Create or get thread\n    thread_id = request.thread_id\n    if not thread_id:\n        thread = await self._create_thread()\n        if thread is None:\n            raise ThreadCreationError()\n        thread_id = thread.id\n\n    # Create message with tool call\n    await self._create_message(\n        thread_id=thread_id,\n        content=f\"Please use the {request.tool_type} tool with these parameters: {request.parameters}\",\n    )\n\n    # Create assistant with the appropriate tool\n    assistant = await self._create_assistant(\n        tools=[{\"type\": request.tool_type}],\n        instructions=request.instructions or \"Execute the requested tool function.\",\n    )\n    if assistant is None:\n        raise AssistantCreationError()\n\n    # Run the assistant\n    run = await self._create_run(thread_id=thread_id, assistant_id=assistant.id)\n    if run is None:\n        raise RunCreationError()\n\n    # Wait for completion\n    run = await self._wait_for_run(thread_id, run.id)\n\n    # Get tool outputs\n    tool_outputs = []\n    if hasattr(run, \"required_action\") and hasattr(run.required_action, \"submit_tool_outputs\"):\n        for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n            # Process each tool call\n            tool_outputs.append(ToolOutput(output=tool_call.function.arguments, error=None))\n\n    # Create response\n    response = ToolResponse(thread_id=thread_id, tool_outputs=tool_outputs)\n\n    return response\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.RunCreationError","title":"<code>RunCreationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception raised when run creation fails</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class RunCreationError(RuntimeError):\n    \"\"\"Exception raised when run creation fails\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Run creation failed\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.RunTimeoutError","title":"<code>RunTimeoutError</code>","text":"<p>               Bases: <code>TimeoutError</code></p> <p>Exception raised when run times out</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class RunTimeoutError(TimeoutError):\n    \"\"\"Exception raised when run times out\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Run timed out\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.openai_client.client.ThreadCreationError","title":"<code>ThreadCreationError</code>","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Exception raised when thread creation fails</p> Source code in <code>openai_tool2mcp/openai_client/client.py</code> <pre><code>class ThreadCreationError(RuntimeError):\n    \"\"\"Exception raised when thread creation fails\"\"\"\n\n    def __init__(self):\n        super().__init__(\"Thread creation failed\")\n</code></pre>"},{"location":"modules/#tool-implementations","title":"Tool Implementations","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter","title":"<code>WebSearchAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's web search tool</p> Source code in <code>openai_tool2mcp/tools/web_search.py</code> <pre><code>class WebSearchAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's web search tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"web-search\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"retrieval\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Search the web for real-time information\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract search query\n        query = request.parameters.get(\"query\", \"\")\n\n        logger.debug(f\"Translating web search request with query: {query}\")\n\n        # Return OpenAI parameters\n        return {\"query\": query}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract search results\n        results = response.get(\"results\", [])\n\n        logger.debug(f\"Translating web search response with {len(results)} results\")\n\n        # Format results as markdown\n        content = format_search_results(results)\n\n        # Return MCP response\n        return MCPResponse(content=content, context={\"search_count\": len(results)})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/web_search.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract search query\n    query = request.parameters.get(\"query\", \"\")\n\n    logger.debug(f\"Translating web search request with query: {query}\")\n\n    # Return OpenAI parameters\n    return {\"query\": query}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.web_search.WebSearchAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/web_search.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract search results\n    results = response.get(\"results\", [])\n\n    logger.debug(f\"Translating web search response with {len(results)} results\")\n\n    # Format results as markdown\n    content = format_search_results(results)\n\n    # Return MCP response\n    return MCPResponse(content=content, context={\"search_count\": len(results)})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter","title":"<code>CodeInterpreterAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's code interpreter tool</p> Source code in <code>openai_tool2mcp/tools/code_interpreter.py</code> <pre><code>class CodeInterpreterAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's code interpreter tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"code-execution\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"code_interpreter\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Execute code and return the result\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract code to execute\n        code = request.parameters.get(\"code\", \"\")\n        language = request.parameters.get(\"language\", \"python\")\n\n        logger.debug(f\"Translating code execution request with language: {language}\")\n\n        # Return OpenAI parameters\n        return {\"code\": code, \"language\": language}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract execution result\n        result = response.get(\"result\", {})\n\n        logger.debug(\"Translating code execution response\")\n\n        # Format result as markdown\n        content = format_code_result(result)\n\n        # Check for errors\n        error = None\n        if isinstance(result, dict) and \"error\" in result:\n            error = result[\"error\"]\n\n        # Return MCP response\n        return MCPResponse(content=content, error=error, context={\"language\": response.get(\"language\", \"python\")})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/code_interpreter.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract code to execute\n    code = request.parameters.get(\"code\", \"\")\n    language = request.parameters.get(\"language\", \"python\")\n\n    logger.debug(f\"Translating code execution request with language: {language}\")\n\n    # Return OpenAI parameters\n    return {\"code\": code, \"language\": language}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.code_interpreter.CodeInterpreterAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/code_interpreter.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract execution result\n    result = response.get(\"result\", {})\n\n    logger.debug(\"Translating code execution response\")\n\n    # Format result as markdown\n    content = format_code_result(result)\n\n    # Check for errors\n    error = None\n    if isinstance(result, dict) and \"error\" in result:\n        error = result[\"error\"]\n\n    # Return MCP response\n    return MCPResponse(content=content, error=error, context={\"language\": response.get(\"language\", \"python\")})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter","title":"<code>BrowserAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's web browser tool</p> Source code in <code>openai_tool2mcp/tools/browser.py</code> <pre><code>class BrowserAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's web browser tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"browser\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"web_browser\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Browse websites and interact with web content\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract URL and action\n        url = request.parameters.get(\"url\", \"\")\n        action = request.parameters.get(\"action\", \"browse\")\n\n        logger.debug(f\"Translating browser request for URL: {url}, action: {action}\")\n\n        # Return OpenAI parameters\n        return {\"url\": url, \"action\": action}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract content\n        content = response.get(\"content\", \"\")\n        title = response.get(\"title\", \"\")\n        url = response.get(\"url\", \"\")\n\n        logger.debug(f\"Translating browser response for URL: {url}\")\n\n        # Format content as markdown\n        formatted_content = f\"# {title}\\n\\n{content}\" if title else content\n\n        # Check for errors\n        error = response.get(\"error\")\n\n        # Return MCP response\n        return MCPResponse(content=formatted_content, error=error, context={\"url\": url, \"title\": title})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/browser.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract URL and action\n    url = request.parameters.get(\"url\", \"\")\n    action = request.parameters.get(\"action\", \"browse\")\n\n    logger.debug(f\"Translating browser request for URL: {url}, action: {action}\")\n\n    # Return OpenAI parameters\n    return {\"url\": url, \"action\": action}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.browser.BrowserAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/browser.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract content\n    content = response.get(\"content\", \"\")\n    title = response.get(\"title\", \"\")\n    url = response.get(\"url\", \"\")\n\n    logger.debug(f\"Translating browser response for URL: {url}\")\n\n    # Format content as markdown\n    formatted_content = f\"# {title}\\n\\n{content}\" if title else content\n\n    # Check for errors\n    error = response.get(\"error\")\n\n    # Return MCP response\n    return MCPResponse(content=formatted_content, error=error, context={\"url\": url, \"title\": title})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter","title":"<code>FileManagerAdapter</code>","text":"<p>               Bases: <code>ToolAdapter</code></p> <p>Adapter for OpenAI's file management tool</p> Source code in <code>openai_tool2mcp/tools/file_manager.py</code> <pre><code>class FileManagerAdapter(ToolAdapter):\n    \"\"\"Adapter for OpenAI's file management tool\"\"\"\n\n    @property\n    def tool_id(self) -&gt; str:\n        \"\"\"Get the MCP tool ID\"\"\"\n        return \"file-io\"\n\n    @property\n    def openai_tool_type(self) -&gt; str:\n        \"\"\"Get the OpenAI tool type\"\"\"\n        return \"file_search\"\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Get the tool description\"\"\"\n        return \"Search and access file content\"\n\n    async def translate_request(self, request: MCPRequest) -&gt; dict:\n        \"\"\"\n        Translate MCP request to OpenAI parameters\n\n        Args:\n            request: The MCP request to translate\n\n        Returns:\n            Dictionary of OpenAI parameters\n        \"\"\"\n        # Extract file operation parameters\n        operation = request.parameters.get(\"operation\", \"read\")\n        path = request.parameters.get(\"path\", \"\")\n        content = request.parameters.get(\"content\", \"\")\n\n        logger.debug(f\"Translating file request for operation: {operation}, path: {path}\")\n\n        # Return OpenAI parameters\n        return {\"operation\": operation, \"path\": path, \"content\": content}\n\n    async def translate_response(self, response: dict) -&gt; MCPResponse:\n        \"\"\"\n        Translate OpenAI response to MCP response\n\n        Args:\n            response: The OpenAI response to translate\n\n        Returns:\n            MCP response object\n        \"\"\"\n        # Extract result\n        path = response.get(\"path\", \"\")\n        operation = response.get(\"operation\", \"read\")\n        content = response.get(\"content\", \"\")\n\n        logger.debug(f\"Translating file response for operation: {operation}, path: {path}\")\n\n        # Format content based on operation\n        if operation == \"read\":\n            formatted_content = f\"# File: {path}\\n\\n```\\n{content}\\n```\"\n        elif operation == \"write\":\n            formatted_content = f\"File written to {path}\"\n        elif operation == \"delete\":\n            formatted_content = f\"File deleted: {path}\"\n        elif operation == \"list\":\n            formatted_content = f\"# Directory: {path}\\n\\n\"\n            for item in content.split(\"\\n\"):\n                if item.strip():\n                    formatted_content += f\"- {item.strip()}\\n\"\n        else:\n            formatted_content = f\"Operation '{operation}' completed on {path}\"\n\n        # Check for errors\n        error = response.get(\"error\")\n\n        # Return MCP response\n        return MCPResponse(content=formatted_content, error=error, context={\"path\": path, \"operation\": operation})\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get the tool description</p>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.openai_tool_type","title":"<code>openai_tool_type</code>  <code>property</code>","text":"<p>Get the OpenAI tool type</p>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.tool_id","title":"<code>tool_id</code>  <code>property</code>","text":"<p>Get the MCP tool ID</p>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.translate_request","title":"<code>translate_request(request)</code>  <code>async</code>","text":"<p>Translate MCP request to OpenAI parameters</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MCPRequest</code> <p>The MCP request to translate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of OpenAI parameters</p> Source code in <code>openai_tool2mcp/tools/file_manager.py</code> <pre><code>async def translate_request(self, request: MCPRequest) -&gt; dict:\n    \"\"\"\n    Translate MCP request to OpenAI parameters\n\n    Args:\n        request: The MCP request to translate\n\n    Returns:\n        Dictionary of OpenAI parameters\n    \"\"\"\n    # Extract file operation parameters\n    operation = request.parameters.get(\"operation\", \"read\")\n    path = request.parameters.get(\"path\", \"\")\n    content = request.parameters.get(\"content\", \"\")\n\n    logger.debug(f\"Translating file request for operation: {operation}, path: {path}\")\n\n    # Return OpenAI parameters\n    return {\"operation\": operation, \"path\": path, \"content\": content}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.tools.file_manager.FileManagerAdapter.translate_response","title":"<code>translate_response(response)</code>  <code>async</code>","text":"<p>Translate OpenAI response to MCP response</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict</code> <p>The OpenAI response to translate</p> required <p>Returns:</p> Type Description <code>MCPResponse</code> <p>MCP response object</p> Source code in <code>openai_tool2mcp/tools/file_manager.py</code> <pre><code>async def translate_response(self, response: dict) -&gt; MCPResponse:\n    \"\"\"\n    Translate OpenAI response to MCP response\n\n    Args:\n        response: The OpenAI response to translate\n\n    Returns:\n        MCP response object\n    \"\"\"\n    # Extract result\n    path = response.get(\"path\", \"\")\n    operation = response.get(\"operation\", \"read\")\n    content = response.get(\"content\", \"\")\n\n    logger.debug(f\"Translating file response for operation: {operation}, path: {path}\")\n\n    # Format content based on operation\n    if operation == \"read\":\n        formatted_content = f\"# File: {path}\\n\\n```\\n{content}\\n```\"\n    elif operation == \"write\":\n        formatted_content = f\"File written to {path}\"\n    elif operation == \"delete\":\n        formatted_content = f\"File deleted: {path}\"\n    elif operation == \"list\":\n        formatted_content = f\"# Directory: {path}\\n\\n\"\n        for item in content.split(\"\\n\"):\n            if item.strip():\n                formatted_content += f\"- {item.strip()}\\n\"\n    else:\n        formatted_content = f\"Operation '{operation}' completed on {path}\"\n\n    # Check for errors\n    error = response.get(\"error\")\n\n    # Return MCP response\n    return MCPResponse(content=formatted_content, error=error, context={\"path\": path, \"operation\": operation})\n</code></pre>"},{"location":"modules/#data-models","title":"Data Models","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.models.mcp.MCPRequest","title":"<code>MCPRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for MCP tool request</p> Source code in <code>openai_tool2mcp/models/mcp.py</code> <pre><code>class MCPRequest(BaseModel):\n    \"\"\"Model for MCP tool request\"\"\"\n\n    parameters: dict[str, Any] = Field(default_factory=dict)\n    context: Optional[dict[str, Any]] = Field(default=None)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.mcp.MCPResponse","title":"<code>MCPResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for MCP tool response</p> Source code in <code>openai_tool2mcp/models/mcp.py</code> <pre><code>class MCPResponse(BaseModel):\n    \"\"\"Model for MCP tool response\"\"\"\n\n    content: str\n    error: Optional[str] = None\n    context: Optional[dict[str, Any]] = Field(default_factory=dict)\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.openai.ToolOutput","title":"<code>ToolOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for OpenAI tool output</p> Source code in <code>openai_tool2mcp/models/openai.py</code> <pre><code>class ToolOutput(BaseModel):\n    \"\"\"Model for OpenAI tool output\"\"\"\n\n    output: str\n    error: Optional[str] = None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.openai.ToolRequest","title":"<code>ToolRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for OpenAI tool request</p> Source code in <code>openai_tool2mcp/models/openai.py</code> <pre><code>class ToolRequest(BaseModel):\n    \"\"\"Model for OpenAI tool request\"\"\"\n\n    tool_type: str\n    parameters: dict[str, Any]\n    thread_id: Optional[str] = None\n    instructions: Optional[str] = None\n</code></pre>"},{"location":"modules/#openai_tool2mcp.models.openai.ToolResponse","title":"<code>ToolResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for OpenAI tool response</p> Source code in <code>openai_tool2mcp/models/openai.py</code> <pre><code>class ToolResponse(BaseModel):\n    \"\"\"Model for OpenAI tool response\"\"\"\n\n    thread_id: str\n    tool_outputs: list[Any] = Field(default_factory=list)\n</code></pre>"},{"location":"modules/#utilities","title":"Utilities","text":"<p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"modules/#openai_tool2mcp.utils.config.APIKeyMissingError","title":"<code>APIKeyMissingError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when the API key is missing</p> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>class APIKeyMissingError(ValueError):\n    \"\"\"Exception raised when the API key is missing\"\"\"\n\n    def __init__(self):\n        super().__init__(\"No API key\")\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.config.ServerConfig","title":"<code>ServerConfig</code>","text":"<p>Configuration class for the MCP server</p> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>class ServerConfig:\n    \"\"\"Configuration class for the MCP server\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: Optional[str] = None,\n        tools: Optional[list[str]] = None,\n        request_timeout: int = 30,\n        max_retries: int = 3,\n    ):\n        \"\"\"\n        Initialize server configuration.\n\n        Args:\n            openai_api_key: OpenAI API key (defaults to environment variable)\n            tools: List of enabled tools (defaults to all)\n            request_timeout: Timeout for API requests in seconds\n            max_retries: Maximum number of retries for failed requests\n        \"\"\"\n        # Load environment variables\n        load_dotenv()\n\n        self.openai_api_key = openai_api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self.openai_api_key:\n            raise APIKeyMissingError()\n\n        self.tools = tools or []  # Will default to all tools in the ToolRegistry\n        self.request_timeout = request_timeout\n        self.max_retries = max_retries\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.config.ServerConfig.__init__","title":"<code>__init__(openai_api_key=None, tools=None, request_timeout=30, max_retries=3)</code>","text":"<p>Initialize server configuration.</p> <p>Parameters:</p> Name Type Description Default <code>openai_api_key</code> <code>Optional[str]</code> <p>OpenAI API key (defaults to environment variable)</p> <code>None</code> <code>tools</code> <code>Optional[list[str]]</code> <p>List of enabled tools (defaults to all)</p> <code>None</code> <code>request_timeout</code> <code>int</code> <p>Timeout for API requests in seconds</p> <code>30</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for failed requests</p> <code>3</code> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>def __init__(\n    self,\n    openai_api_key: Optional[str] = None,\n    tools: Optional[list[str]] = None,\n    request_timeout: int = 30,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize server configuration.\n\n    Args:\n        openai_api_key: OpenAI API key (defaults to environment variable)\n        tools: List of enabled tools (defaults to all)\n        request_timeout: Timeout for API requests in seconds\n        max_retries: Maximum number of retries for failed requests\n    \"\"\"\n    # Load environment variables\n    load_dotenv()\n\n    self.openai_api_key = openai_api_key or os.environ.get(\"OPENAI_API_KEY\")\n    if not self.openai_api_key:\n        raise APIKeyMissingError()\n\n    self.tools = tools or []  # Will default to all tools in the ToolRegistry\n    self.request_timeout = request_timeout\n    self.max_retries = max_retries\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.config.load_config","title":"<code>load_config(config_file=None)</code>","text":"<p>Load configuration from file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to configuration file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Configuration dictionary</p> Source code in <code>openai_tool2mcp/utils/config.py</code> <pre><code>def load_config(config_file=None) -&gt; dict:\n    \"\"\"\n    Load configuration from file.\n\n    Args:\n        config_file (str, optional): Path to configuration file\n\n    Returns:\n        dict: Configuration dictionary\n    \"\"\"\n    # If a specific config file is provided, load it\n    if config_file and os.path.exists(config_file):\n        # For now, just return a simple config dictionary\n        # In a real implementation, parse the file\n        return {\"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\")}\n\n    # Default configuration\n    return {\"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\")}\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.logging.setup_logging","title":"<code>setup_logging(level='info')</code>","text":"<p>Set up logging configuration.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (debug, info, warning, error, critical)</p> <code>'info'</code> Source code in <code>openai_tool2mcp/utils/logging.py</code> <pre><code>def setup_logging(level=\"info\"):\n    \"\"\"\n    Set up logging configuration.\n\n    Args:\n        level (str): Logging level (debug, info, warning, error, critical)\n    \"\"\"\n    level_map = {\n        \"debug\": logging.DEBUG,\n        \"info\": logging.INFO,\n        \"warning\": logging.WARNING,\n        \"error\": logging.ERROR,\n        \"critical\": logging.CRITICAL,\n    }\n\n    log_level = level_map.get(level.lower(), logging.INFO)\n\n    # Configure root logger\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n\n    # Create logger for this package\n    logger = logging.getLogger(\"openai_tool2mcp\")\n    logger.setLevel(log_level)\n\n    return logger\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.security.sanitize_parameters","title":"<code>sanitize_parameters(parameters)</code>","text":"<p>Sanitize parameters to prevent injection attacks.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict</code> <p>Parameters to sanitize</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Sanitized parameters</p> Source code in <code>openai_tool2mcp/utils/security.py</code> <pre><code>def sanitize_parameters(parameters: dict) -&gt; dict:\n    \"\"\"\n    Sanitize parameters to prevent injection attacks.\n\n    Args:\n        parameters (dict): Parameters to sanitize\n\n    Returns:\n        dict: Sanitized parameters\n    \"\"\"\n    sanitized = {}\n\n    for key, value in parameters.items():\n        if isinstance(value, str):\n            # Basic sanitization for strings\n            sanitized[key] = value.replace(\"&lt;script&gt;\", \"\").replace(\"&lt;/script&gt;\", \"\")\n        elif isinstance(value, (dict, list)):\n            # Recursively sanitize nested structures\n            sanitized[key] = (\n                sanitize_parameters(value)\n                if isinstance(value, dict)\n                else [sanitize_parameters(item) if isinstance(item, dict) else item for item in value]\n            )\n        else:\n            # Keep other types as is\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.security.validate_api_key","title":"<code>validate_api_key(api_key)</code>","text":"<p>Validate OpenAI API key format.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key to validate</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the key format is valid</p> Source code in <code>openai_tool2mcp/utils/security.py</code> <pre><code>def validate_api_key(api_key: str) -&gt; bool:\n    \"\"\"\n    Validate OpenAI API key format.\n\n    Args:\n        api_key (str): API key to validate\n\n    Returns:\n        bool: True if the key format is valid\n    \"\"\"\n    if not api_key:\n        return False\n\n    # Check if key follows OpenAI format\n    # This is a simple format check, not a real validation\n    return bool(api_key.startswith((\"sk-\", \"org-\")))\n</code></pre>"},{"location":"modules/#openai_tool2mcp.utils.security.validate_api_key_with_openai","title":"<code>validate_api_key_with_openai(api_key)</code>","text":"<p>Validate OpenAI API key by making a test request.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the key is valid</p> Source code in <code>openai_tool2mcp/utils/security.py</code> <pre><code>def validate_api_key_with_openai(api_key: str) -&gt; bool:\n    \"\"\"\n    Validate OpenAI API key by making a test request.\n\n    Args:\n        api_key (str): OpenAI API key\n\n    Returns:\n        bool: True if the key is valid\n    \"\"\"\n    if not api_key:\n        return False\n\n    try:\n        # Make a minimal request to check key validity\n        response = requests.get(\n            \"https://api.openai.com/v1/models\", headers={\"Authorization\": f\"Bearer {api_key}\"}, timeout=10\n        )\n\n        if response.status_code == 200:\n            logger.info(\"OpenAI API key is valid\")\n            return True\n        else:\n            logger.error(f\"OpenAI API key validation failed with status {response.status_code}\")\n            return False\n    except requests.exceptions.RequestException:\n        logger.error(\"OpenAI API key validation error: RequestException\")\n        return False\n</code></pre>"}]}